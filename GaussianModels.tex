

%TODO short intro, recall Gaussian models, remind reader that $\KK \in \{\RR, \CC\}$; say that proofs will look like they are given over $\CC$

%todo this chapter is based on...

%TODO ensure that always \lambda > 0, and \tau \in \KK^{\times}


This chapter starts our studies of ML estimation on Gaussian models and sets the stage for Chapters~\ref{ch:GaussianGroupModels} and~\ref{ch:RDAGs}. We define so-called Gaussian models via symmetrization, which in hindsight deserve a treatment on their own. The main result is the weak correspondence, Theorem~\ref{thm:WeakCorrespondence}. It views maximizing the log-likelihood as a norm minimization problem and provides a first dictionary between stability notions and ML estimation in the Gaussian case. The weak correspondence  generalizes similar statements of \cite{SiagaPaper} and we need this level of generality in Chapter~\ref{ch:RDAGs}.

The chapter is in parts based on discussions with Anna Seigal and Visu Makam and on our joint paper \cite[Appendix~A]{RDAG}.



\paragraph{Organization and Assumptions.}
In Section~\ref{sec:IntroGaussianModels}, we define Gaussian models via symmetrization and state simple properties of these. Afterwards, we define stability notions under \emph{sets} and prove the weak correspondence, Section~\ref{sec:WeakCorrespondence}.

Similarly to Section~\ref{sec:GaussianModelsMLestimation} we work in parallel over 
$\KK \in \{\RR, \CC\}$. Remember that $(\cdot)\HT$ is the Hermitian transpose, which equals the transpose $(\cdot)\T$ if $\KK = \RR$.




\section{Examples and first Properties}\label{sec:IntroGaussianModels}


\begin{defn}\label{defn:GaussianModelMA}
	For a subset $\Aset \subseteq \GL_m(\KK)$ we define
		\begin{equation}\label{eq:GaussianModelMA}
			\Mg_{\Aset} := \left\lbrace a\HT a \mid a \in \Aset \right\rbrace \subseteq \PD_m(\KK),
		\end{equation}
	the \emph{Gaussian model via symmetrization}\index{Gaussian model!via symmetrization} of $\Aset$. If $\Aset = G$ is a subgroup of $\GL_m(\KK)$ we call $\Mg_{G}$ a \emph{Gaussian group model}\index{Gaussian group model|textbf}.
	\hfill\defnSymbol
\end{defn}

The superscript $\mathtt{g}$ indicates that $\Mg_{\Aset}$ is a Gaussian model and distinguishes it from log-linear models $\Mll_A$, which are studied in Chapter~\ref{ch:LogLinearModels}. We point out that for $\KK = \RR$ the Hermitian transpose $a\HT$ is just the transpose $a\T$. Hence, Definition~\ref{defn:GaussianModelMA} matches the definition of Gaussian group models over $\RR$ respectively $\CC$ given in \cite{SiagaPaper} and its generalizations to $\Mg_{\Aset}$ in \cite{RDAG}.
It is a basic fact that \emph{any} Gaussian model is of the form $\Mg_{\Aset}$.

\begin{prop}
	Let $\Mcal \subseteq \PD_m(\KK)$ be a Gaussian model. Then there exists a subset $\Aset \subseteq \GL_m(\KK)$ with $\Mcal = \Mg_{\Aset}$.
\end{prop}

\begin{proof}
	Let $\Psi \in \Mcal$. Then the positive definite matrix $\Psi$ admits a Cholesky decomposition, denoted $\chol(\Psi)$. Recall that $\chol(\Psi) \in \GL_m(\KK)$ is the unique upper triangular matrix with \emph{positive} diagonal entries such that $\Psi = \chol(\Psi)\HT \chol(\Psi)$. %TODO possibly adjust depending on whether Cholesky decmop was already introduced
	This determines a subset $\Aset := \{ \chol(\Psi) \mid \Psi \in \Mcal\}$ of $\GL_m(\KK)$, which satisfies $\Mcal = \Mg_{\Aset}$ by construction.\footnote{Another choice for $\Aset$ is $\{ \Psi^{\nicefrac{1}{2}} \mid \Psi \in \Mcal \}$, where $\Psi^{\nicefrac{1}{2}}$ denotes the square root of $\Psi$, i.e., the unique positive definite matrix which square is $\Psi$.}
\end{proof}

\begin{remark}\label{rem:differentAparametrizations}
	We think of the set $\Aset$ as a \emph{parametrization} %TODO tell apart from usual parameters as concentration matrix!
	of the model $\Mcal = \Mg_{\Aset}$.
	A Gaussian model may admit many different parametrizations, e.g., whenever we have $\Aset \subseteq \mathcal{B} \subseteq \{ g a \mid a \in \Aset, \, g\HT g = \Id_m\}$ it holds that $\Mg_{\Aset} = \Mg_{\mathcal{B}}$. 
	\hfill\remSymbol
\end{remark}

\begin{example}[Saturated Gaussian Model] \label{ex:FullGuassianAsMgA}
	The saturated Gaussian model $\Mcal = \PD_m(\KK)$ can be parametrized by any $\Aset \subseteq \GL_m(\KK)$ that contains the group  $\Bor_m(\KK)$ of invertible upper triangular matrices.\footnote{These are not all options, e.g., $\{\chol(\Psi) \mid \Psi \in \PD_m(\KK)\}$ is strictly contained in $\Bor_m(\KK)$.} In particular, we have $\PD_m(\KK) = \Mg_{\GL_m(\KK)} = \Mg_{\Bor_m(\KK)}$. We will see corresponding statistical interpretations of these parametrizations: $\Mcal  = \Mg_{\GL_m(\KK)}$ is studied as a Gaussian group model with self-adjoint group in Example~\ref{ex:FullModelSelfAdjoint}; $\Mcal = \Mg_{\Bor_m(\KK)}$ arises as a directed Gaussian graphical model in Example~\ref{ex:FullModelAsTDAG}.
	\hfill\exSymbol
\end{example}

%mention further examples?: guess this is not necessary; we just saw that any Gaussian model is of the form \MgA

In statistics one often studies Gaussian models which are closed under positive scalars.
In this regard, the following proposition\footnote{This proposition arose from a discussion with Anna Seigal.} justifies the assumption ``$\Aset \subseteq \GL_m(\KK)$ is closed under non-zero scalar multiples'' of Theorem~\ref{thm:WeakCorrespondence}. The assumption is used there and throughout the thesis to relate ML estimation of the model $\Mg_{\Aset}$ to norm minimization and to stability notions.


\begin{prop}\label{prop:MclosedUnderPositiveScalars}
	A Gaussian model $\Mcal \subseteq \PD_m(\KK)$ is closed under positive scalar multiples if and only if there is some set $\Aset \subseteq \GL_m(\KK)$ closed under non-zero scalar multiples such that $\Mcal = \Mg_{\Aset}$.
\end{prop}

\begin{proof}
	To prove the ``if''-part, let $\Psi \in \Mcal = \Mg_\Aset$. Then there is some $a \in \Aset$ with $\Psi = a\HT a$. For $\lambda > 0$, we have $\sqrt{\lambda}a \in \Aset$ by assumption on $\Aset$ and hence
	\[\lambda \Psi = \big(\sqrt{\lambda} a \big)\HT \big( \sqrt{\lambda}a \big) \in \Mg_{\Aset} = \Mcal \]
	as claimed.
	
	Conversely, assume that $\Mcal$ is closed under positive scalar multiples. Consider the set
	\[ \Aset := \{ \tau \chol(\Psi) \mid \tau \in \KK^{\times}, \Psi \in \Mcal \}, \]
	which is closed under non-zero scalar multiples. We have $\chol(\Psi) \in \Aset$ for all $\Psi \in \Mcal$ and thus $\Mcal \subseteq \Mg_\Aset$. On the other hand, for all $\tau \in \KK^{\times}$ and all $\Psi \in \Mcal$,
	\[ \big( \tau \chol(\Psi) \big)\HT \big( \tau \chol(\Psi) \big) = |\tau|^2 \, \Psi \in \Mcal, \]
	where we used  $|\tau|^2 > 0$ and the assumption on $\Mcal$. This shows $\Mcal = \Mg_{\Aset}$.
\end{proof}

\begin{remark}
	The proof of Proposition~\ref{prop:MclosedUnderPositiveScalars} shows that the statement remains true, if we replace the assumption on $\Aset$ by ``$\Aset \subseteq \GL_m(\KK)$ closed under positive scalar multiples''. Indeed, the only necessary adjustment is to consider $\Aset = \{\lambda \chol(\Psi) \mid \lambda >0, \Psi \in \Mcal\}$ for the ``only if''-direction.
	\hfill\remSymbol
\end{remark}




%------- Section: Weak Correspondence -------

\section{The weak Correspondence}\label{sec:WeakCorrespondence}

In this section we prove the main result of this chapter -- the so-called \emph{weak correspondence}\footnote{The name \emph{weak correspondence} was coined by Anna Seigal during discussions with Gergely B\'erczi, Eloise Hamilton, Visu Makam and myself.}, Theorem~\ref{thm:WeakCorrespondence}. The weak correspondence casts maximizing the log-likelihood function as a norm minimization problem. This in turn allows to relate ML estimation to stability notions, which we introduce now.

Fix a subset $\Aset \subseteq \GL_m(\KK)$ and a tuple of samples $Y = (Y_1, \ldots, Y_n) \in (\KK^m)^n$. Remember that we view the samples $Y_i$ as column vectors, which identifies $Y$ as a matrix in $\KK^{m \times n} \cong (\KK^m)^n$. There is no Often we switch implicitly between these identifications. Given some $a \in \Aset$, we set
	\[ a \cdot Y := (aY_1, \ldots, aY_n) \in (\KK^m)^n \cong \KK^{m \times n} , \]
which is just the multiplication of the matrices $a$ and $Y$.
The dot indicates that we think of the set $\Aset$ ``acting'' via left multiplication on $\KK^{m \times n} \cong (\KK^m)^n$. In analogy to group actions we define the orbit of $Y$ and the stabilizer of $Y$ under the \emph{set} $\Aset$ as
	\begin{equation}\label{eq:defnOrbitStabilizerUnderA}
		\Aset \cdot Y := \{ a \cdot Y \mid a \in \Aset \} \quad \text{ and } \quad
		\Aset_Y := \{a \in \Aset \mid a \cdot Y = Y\},
	\end{equation}
respectively.\footnote{Since $\Aset$ is just a \emph{set} one needs to be careful: known results from the theory of group actions do not need to hold. For example, in general the orbits do not form a partition of $\KK^{m \times n}$.} Analogous to the topological stability notions for group actions, Definition~\ref{defn:StabilityGroupTopological}, we make the following definitions.

\begin{defn}[Stability Notions for Sets, {\cite[Definition~A.1]{RDAG}}] \label{defn:StabilitySets}
	\ \\
	We say the tuple of samples $Y \in (\KK^m)^n \cong \KK^{m \times n}$, under the set $\Aset$, is
	\begin{itemize}
		\item[(i)] \emph{unstable} if $0 \in \overline{\Aset \cdot Y}$;
		\item[(ii)] \emph{semistable} if $Y$ is not unstable, i.e., $0 \notin \overline{\Aset \cdot Y}$;
		\item[(iii)] \emph{polystable} if $Y \neq 0$ and the set $\Aset \cdot Y$ is Euclidean closed;
		\item[(iv)] \emph{stable} if $Y$ is polystable and $\Aset_Y$ is finite. \hfill\defnSymbol
	\end{itemize}
\end{defn}

TODO something about relation to usual stability notions, when $\Aset$ is a subgroup
%todo add remark about these notions and their relation to the usual notions, refer to part I on Invariant theory

To prove the weak correspondence we need the following lemma.

\begin{lemma}\label{lem:ForWeakCorrespondence}
	Fix $m,n > 0$ and, for $\gamma \geq 0$, consider the family of functions
	\[ f_{\gamma} \colon \RR_{>0} \to \RR, \quad x \mapsto \frac{\gamma}{n} x - m \log(x).\]
	\begin{itemize}\itemsep 3pt
		\item[(i)] If $\gamma = 0$, then $\inf_{x > 0} f_\gamma(x) = - \infty$.
		
		\item[(ii)] If $\gamma > 0$, then $f_{\gamma}$ attains a global minimum at $x_0 = \frac{m n}{\gamma}$ with function value $f_{\gamma}(\frac{m n}{\gamma}) = m (1 - \log(m n) + \log(\gamma))$. 
		
		\item[(iii)] Given $\gamma_1 > \gamma_2 > 0$, we have $f_{\gamma_1}(\frac{\alpha}{\gamma_1}) > f_{\gamma_2}(\frac{\alpha}{\gamma_2})$ at the global minima.
	\end{itemize}
\end{lemma}

\begin{proof}
	The first part follows from the properties of the logarithm.
	To prove part~(ii), one computes for $x > 0$ that $f_{\gamma}'(x) = \frac{\gamma}{n} - \frac{m}{x}$ and $f''_\gamma(x) = \frac{m}{x^2} > 0$. The latter implies that $f_{\gamma}$ is strictly convex and hence the former yields that $x_0 = \frac{m n}{\gamma}$ is the unique global minimum. 
	One directly verifies the equation for $f_{\gamma}(x_0)$, so part~(iii) follows from the strict monotonicity of the logarithm.
\end{proof}

Recall Equation~\eqref{eq:GaussianLogLikelihood}: for the model $\Mg_A$ and tuple of samples $Y \in (\KK^m)^n$ the log-likelihood function $\ell_Y$ at $\Psi = a\HT a$, where $a \in \Aset$, is given by
	\[ \ell_{Y} (\Psi) = \ell_{Y} \big( a\HT a \big) = \log \det \big( a\HT a \big) - \tr \big( a\HT a S_Y \big), \quad \text{where } S_Y = \frac{1}{n} \sum_{i=1}^n Y_i Y_i\HT.\]
A key for relating ML estimation to norm minimization is the following observation: for all $a \in \Aset$ we compute\footnote{Recall that, if not stated otherwise, we consider the norm induced by the standard inner product, so $\KK^{m \times n}$ is equipped with the Frobenius norm. Thus, the norm of $Y$ does not change under the identification $\KK^{m \times n} \cong (\KK^m)^n$. }%todo maybe mention this also in Chapter 9??
	\begin{equation}\label{eq:NormTrace}
		n \, \tr \big(a\HT a S_Y \big) = \sum_{i=1}^n \tr \big( Y\HT_i a\HT a Y_i \big) = \sum_{i=1}^n (a Y_i)\HT a Y_i = \| a \cdot Y \|^2 
	\end{equation}
where we used in the second equality that $Y\HT_i a\HT a Y_i  = (a Y_i)\HT a Y_i$ is a scalar. Hence, the log-likelihood $\ell_Y$ at $\Psi = a\HT a$ can be rewritten as
	\begin{equation}\label{eq:MgALogLikelihoodNorm}
		\ell_Y \big( a\HT a \big) = \log \det \big(a\HT a \big) - \frac{1}{n} \| a \cdot Y \|^2.
	\end{equation}
We use this equation to prove the weak correspondence. To state it, set
	\begin{align}
		\ASL &:= \{ a \in \Aset \mid \det(a) = 1\} , \label{eq:defnASL}\\
		\ASL^- &:= \{ a \in \Aset \mid \det(a) = -1\} , \label{eq:defnASL-}\\
		\ASLpm &:= \{ a \in \Aset \mid \det(a) = \pm 1\}. \label{eq:defnASLpm}
	\end{align}
The weak correspondence, Theorem~\ref{thm:WeakCorrespondence}, is based on \cite[Proposition~A.4]{RDAG} and generalizes \cite[Proposition~3.4 and Theorem~3.6]{SiagaPaper} to sub\emph{sets} $\Aset \subseteq \GL_m(\KK)$. Its key feature is that it casts maximizing the log-likelihood as a two step optimization problem, compare Equation~\eqref{eq:doubleInf}. First, one minimizes $\|b \cdot Y\|^2$ over $b \in \ASLpm$, i.e., one computes $\capac_{\ASLpm}(Y)$. Afterwards, one is left with a univariate convex optimization problem. This two step approach in combination with Lemma~\ref{lem:ForWeakCorrespondence} allows to connect ML estimation to stability notions.

\begin{theorem}[Weak Correspondence {\cite[Proposition~A.4]{RDAG}}] \label{thm:WeakCorrespondence}
	\ \\
	Let $\Aset \subseteq \GL_m(\KK)$ be closed under non-zero scalar multiples. The supremum of the log-likelihood $\ell_Y$ over $\Mg_{\Aset}$ can be computed as a double infimum:
		\begin{equation}\label{eq:doubleInf}
			\sup_{a \in \Aset} \; \ell_Y \big(a\HT a \big)
			= - \inf_{x \in \RR_{>0}} \left( \frac{x}{n} \left( \inf_{b \in \ASLpm} \|b \cdot Y\|^2 \right) - m \log(x) \right).
		\end{equation}
	The MLEs, if they exist, are the matrices $\lambda b\HT b$, where $b \in \ASLpm$ minimizes the inner infimum and $\lambda \in \RR_{>0}$ is the \emph{unique} global minimum of the outer infimum.
	Equation~\eqref{eq:doubleInf} gives a correspondence between stability under $\ASLpm$ and maximum likelihood estimation in the model $\Mg_\Aset$ given sample matrix $Y \in \KK^{m \times n}$: 
	$$ \begin{matrix} (a) & Y \text{ unstable}  & \Leftrightarrow & \text{likelihood $\ell_Y$ unbounded from above} \\ 
		(b) &  Y \text{ semistable} & \Leftrightarrow & \text{likelihood $\ell_Y$ bounded from above} \\ 
		(c) & Y \text{ polystable}  & \Rightarrow & \text{MLE exists.} \end{matrix} $$
	The whole statement holds for $\ASL$ replacing $\ASLpm$, if
		\begin{itemize}
			\item[(i)] $\KK = \CC$, or
			\item[(ii)] $\KK = \RR$ and for any $a \in \Aset$ there is an orthogonal matrix $o = o(a)$ such that $o\T a \in \Aset$ and $\det(o\T a) > 0$.
		\end{itemize}
\end{theorem}

\begin{remark}\label{rem:WeakCorrespMclosedUnderPositiveScalars}
	The weak correspondence applies exactly to those Gaussian models $\Mcal \subseteq \PD_m(\KK)$ that are closed under positive scalar multiples. Indeed, these models are exactly the ones that admit a set $\Aset \subseteq \GL_m(\KK)$ closed under scalar multiples such that $\Mcal = \Mg_{\Aset}$, see Proposition~\ref{prop:MclosedUnderPositiveScalars}.
	\hfill\remSymbol
\end{remark}

\begin{proof}[Proof of Theorem~\ref{thm:WeakCorrespondence}]
	By Equation~\eqref{eq:MgALogLikelihoodNorm}, maximizing $\ell_Y$ over $\Mg_\Aset$ is equivalent to minimizing the function
	\begin{align*}
		f \colon \Aset \to \RR, \; \qquad a \mapsto \frac{1}{n} \|a \cdot Y\|^2 - \log\det(a\HT a).
	\end{align*}
	Using the assumption on $\Aset$ we can rewrite an element $a \in \Aset$ as follows.
	For $\KK = \RR$, let $\tau := \sqrt[m]{|\det(a)|} \in \RR^{\times}$, then $b := \tau^{-1} a \in \ASLpm$ and $a = \tau b$. If $\KK = \CC$, let $\tau \in \CC^\times$ be some $m^{th}$ root of $\det(a)$, so $b := \tau^{-1} a \in \ASLpm$ and $a = \tau b$. (Actually, $b \in \ASL$ and this leads to the fact that $\ASLpm$ may always be replaced by $\ASL$ given $\KK = \CC$.) Setting $x := |\tau|^2$, we compute \emph{both} in the real and complex case that
	 \[ f(a) = \frac{|\tau|^2}{n} \| b \cdot Y \|^2 - \log\det \big( |\tau|^2 b\HT b \big)
	= \frac{x}{n} \|b \cdot Y\|^2 - m \log(x). \]
	
	Let $\gamma := \inf_{b \in \ASLpm} \| b \cdot Y\|^2$. By Lemma~\ref{lem:ForWeakCorrespondence}, the infimum of the function $\RR_{>0} \to \RR, x \mapsto \gamma n^{-1} x - m \log(x)$ increases as $\gamma \geq 0$ increases. This allows us to maximize $\ell_Y$ respectively minimize $f$ in two steps and we obtain Equation~\eqref{eq:doubleInf}.
	By Lemma~\ref{lem:ForWeakCorrespondence}, $\inf_{a \in \Aset} f(a) = - \infty$ if and only if $\gamma = \inf_{b \in \ASLpm} \| b \cdot Y\|^2 = 0$, i.e., if and only if $Y$ is unstable under $\ASLpm$. This shows parts~(a) and (b).
	
	To prove (c), assume that $Y$ is polystable under $\ASLpm$. Then $\gamma > 0$ as $Y$ is semistable and hence $x \mapsto \gamma n^{-1} x - m\log(x)$ is minimized by a unique $\lambda > 0$, by Lemma~\ref{lem:ForWeakCorrespondence}. Since $\ASLpm \cdot Y$ is closed in $\KK^{m \times n}$, we see that $\gamma$ is attained by some $b$ in the compact set $(A_{\SL}^{\pm} \cdot Y) \cap \{ Z \in \KK^{m \times n} \mid \|Z\|^2 \leq \gamma + 1 \}$. Thus, $-f \big( \sqrt{\lambda} b \big) = \sup_{\Psi \in \Mg_{\Aset}} \ell_Y(\Psi)$ and an MLE given $Y$, namely $\lambda b\HT b$, exists.
	
	Using Equation~\eqref{eq:doubleInf} we see that actually any matrix of the form $\lambda b\HT b$, where $\lambda$ and $b$ are as in the statement, is an MLE.
	Conversely, let $\hat{a} \in \Aset$ be such that $\hat{\Psi} := \hat{a}\HT \hat{a} \in \Mg_{\Aset}$ is an MLE given $Y$. Similar to the above, write $\hat{a} = \hat{\tau} \hat{b}$ with $\tau \in \KK^{\times}$ and $\hat{b} \in \ASLpm$. Then $\ell_{Y}(\hat{\Psi}) = - f(\hat{a})$ is the maximum of $\ell_Y$, equivalently,
		\[ \inf_{a \in \Aset} f(a)  = f(\hat{a}) = \frac{|\hat{\tau}|^2}{n} \| \hat{b} \cdot Y \|^2 - m \log \big( |\hat{\tau}|^2 \big). \]
	Therefore, the inner and outer infima in~\eqref{eq:doubleInf} must be attained by $|\hat{\tau}|^2$ and $\hat{b}$, respectively; otherwise we would obtain a contradiction to $\inf_{a \in \Aset} f(a)  = f(\hat{a})$ via Lemma~\ref{lem:ForWeakCorrespondence}. Altogether, $\hat{\Psi} = |\hat{\tau}|^2 \, \hat{b}\HT \hat{b}$ has the claimed form.
	
	Finally, we discuss two situations in which $\ASLpm$ can be replaced by $\ASL$. We already mentioned that we can write $a = \tau b$ with $\tau \in \CC^{\times}$ and $b \in \ASL$, if $\KK = \CC$.
	On the other hand, if condition~(ii) is satisfied, then any $a \in \Aset$ can be rewritten as $a = \tau o b$, where $\tau := \sqrt[m]{|\det(a)|}$ and $b := \tau^{-1} o\T a$. We have $b \in \Aset$, because $o\T a \in \Aset$ and $\Aset$ is closed under non-zero scalars. Furthermore, $\det(o\T a)  = \det(o)\det(a) > 0$ and $|\det(o)| = 1$ imply $\det(o\T a) = |\det(a)|$, hence $b \in \ASL$. Noting that $(ob)\HT (ob) = b\HT b$ and $\|(ob) \cdot Y\|^2 = \|b \cdot Y\|^2$ by orthogonality of $o$, we obtain \emph{both} under condition~(i) and under~(ii) Equation~\eqref{eq:doubleInf} with $\ASLpm$ replaced by $\ASL$. Moreover, the remaining parts of the proof remain valid under this replacement.
\end{proof}

\begin{remark}\label{rem:ConditionIIweakCorrespondence}
	Notice that condition~(ii) in Theorem~\ref{thm:WeakCorrespondence} is trivially satisfied if $\Aset$ only contains matrices with positive determinant (choose $o = \Id_m$), or if $n$ is odd. In the latter case, one can choose $o(a) = \mathrm{sgn}(\det(a)) \Id_m$.
	
	From an invariant theory perspective it is more natural to work with $\ASL$ instead of $\ASLpm$. In this regard, condition~(ii) seems unpleasant and artificial. However, in this generality it cannot be dropped as we shall see in the next Example~\ref{ex:EasyCounterexampleASLpm} and in Example~\ref{ex:AKRS-Example3-5}.
	Still, apart from these examples all Gaussian models studied in this thesis satisfy condition~(ii) and we will work with $\ASL$ instead of $\ASLpm$.
	\hfill\remSymbol
\end{remark}

\begin{example}\label{ex:EasyCounterexampleASLpm}
	Consider the idempotent matrix
		\[M := \begin{pmatrix}
			\nicefrac{1}{2} & 3 \\ \nicefrac{1}{4} & - \nicefrac{1}{2}
		\end{pmatrix} \]
	which is \emph{not} orthogonal and has determinant $-1$. Then
		\begin{equation}\label{eq:GroupEasyCounterexampleASLpm}
			G := \{ \tau \Id_2, \, \tau M \mid \tau \in \KK^{\times} \}
		\end{equation}
	is a subgroup of $\GL_2(\KK)$, which is closed under non-zero scalars. The Gaussian group model $\Mg_G = \{ \lambda \Id_2, \lambda M\HT M \mid \lambda > 0\}$ consists of two rays in $\PD_m(\KK)$.\footnote{The Gaussian group model is taken from \cite[Example~3.12]{SiagaPaper}, presented in Example~\ref{ex:AKRS-Example3-12}, and we use this model several times for illustration.} In the following we study the situation $n=1$ with observed sample $Y = (1, 0)\T$ and illustrate the differences between the real and complex situation. In particular, we show that violating condition~(ii) in the real case can prevent the replacement of $\GSLpm$ by $\GSL$ in Theorem~\ref{thm:WeakCorrespondence}.
	
	Let $\KK = \RR$. Since scaling a matrix of $\GL_2(\RR)$ by $\tau \in \RR^{\times}$ scales its determinant with $\tau^2$, we have $\GSL = \{ \pm \Id_2\}$ and $\GSL^{-} = \{ \pm M\}$.
	Note that there is no orthogonal matrix $o$ with $o\T M \in \GSL$, i.e., condition~(ii) in Theorem~\ref{thm:WeakCorrespondence} is violated. Indeed, otherwise we would have $o\T  = (o \T M) M \in \GSL^{-}$, but this contradicts that $\GSL^{-}$ does not contain an orthogonal matrix.
	
	We have 
		\[ \| (\pm M) \cdot Y \|^2 = \frac{1}{4} + \frac{1}{16} < 1 = \| \pm Y \|^2 \]
	and thus $\inf_{g \in \GSLpm} \| g \cdot Y \|^2 = \nicefrac{5}{16}$ is attained on $\GSL^{-} \cdot Y$, but not on $\GSL \cdot Y$. This shows that we \emph{cannot} replace $\GSLpm$ by $\GSL$ in Theorem~\ref{thm:WeakCorrespondence}.
	
	By Theorem~\ref{thm:WeakCorrespondence}, an MLE given $Y$ is of the form $\lambda b\T b$, where $b = \pm M$ and $\lambda$ is the unique global minimum of $x \mapsto (\nicefrac{5}{16}) x - 2\log(x)$. Lemma~\ref{lem:ForWeakCorrespondence}~(ii) shows $\lambda = \nicefrac{32}{5}$. Note that both choices of $b$ give the same MLE, so we conclude that
	\[\lambda M\T M = \frac{32}{5} \begin{pmatrix} \nicefrac{5}{16} & \nicefrac{11}{8} \\ \nicefrac{11}{8} & \nicefrac{37}{4} \end{pmatrix} 
	= \begin{pmatrix} 2 & \nicefrac{44}{5} \\ \nicefrac{44}{5} & \nicefrac{296}{5} \end{pmatrix}  \]
	is the unique MLE given $Y$.
	
	\medskip
	
	Next, let $\KK = \CC$. The main difference to the real case is that we now have $\GSL = \{\pm \Id_2, \pm \, \imag M\}$ and $\GSL^{-} = \{\pm M, \pm \, \imag \Id_2\}$. Therefore, $\inf_{g \in \GSLpm} \| g \cdot Y\|^2 = \nicefrac{5}{16}$ is attained \emph{both} on $\GSL \cdot Y$ and on $\GSL^{-} \cdot Y$. By Theorem~\ref{thm:WeakCorrespondence} for $\GSL$, an MLE given $Y$ is of the form $\lambda b\HT b$, where $b = \pm \imag M$ and $\lambda = \nicefrac{32}{5}$ is the unique global minimum of $x \mapsto (\nicefrac{5}{16})x - 2\log(x)$. Since $| \pm \, \imag |^2 = 1$ and $M$ has only real entries, we see that $\lambda M \HT M = \lambda M\T M$ is again the unique MLE given $Y$.
	\hfill\exSymbol
\end{example}



















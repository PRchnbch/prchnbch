
%TODO

%todo: check  "\id", "\Gcal", \Mcal, \RR, e.g., i.e., check "A", "Z" (use \Zar for Zariski closure), \T

%todo: include disjoint colours of edges and vertices in the definition?



%side effect: we will generalize the results on DAGs and TDAGs from Sections~\ref{sec:GaussianModelsMLestimation} and~\ref{sec:TDAGs}.

Graphical models play a fundamental role in statistics and have manifold applications \cite{LauritzenBook, HandbookGraphical}. 
Intuitively, the graph encodes the following statistical meaning: each vertex represents a random variable, and the edges between variables reflect their statistical dependence~\cite{verma1990causal}. In the Gaussian case we already came across two kinds of graphical models. Example~\ref{ex:UndirectedGraphicalModelIntro} defined undirected Gaussian graphical models, while Definition~\ref{defn:DAGmodel} recalled Gaussian graphical models on \emph{directed} acyclic graphs (DAGs).
Remember that DAG models are also called Gaussian Bayesian networks and they are linear structural equation models with independent errors, see \cite{drton2018algebraic} and \cite[Section~16.2]{SullivantBook}. 
DAG models have been applied to many different contexts such as cell signalling~\cite{sachs2005causal}, gene interactions~\cite{friedman2000using} and causal inference~\cite{pearl2009causality}.

In this chapter, we introduce and study Gaussian graphical models on DAGs with coloured vertices and edges. The colours impose symmetries in the model: if two vertices or two edges have the same colour, then their parameters in the model must be the same. We call such models \emph{RDAG models}, where the `R' stands for restricted, cf.~\cite{hojsgaard2008graphical}. RDAG models contain DAG models as a special case, Remark~\ref{rem:DAGmodelViaCompatible}. In that regard, many results of this chapter generalize statements on (T)DAG models from Sections~\ref{sec:GaussianModelsMLestimation} and~\ref{sec:TDAGs}.

The whole chapter is based on the preprint \cite{RDAG}, which is joint work with Visu Makam and Anna Seigal.


\paragraph{Motivation.}
We state three main motivations for studying RDAG models.

First, RDAG models are a natural analogue of so-called restricted concentration (RCON) models, which have been introduced in \cite{hojsgaard2008graphical}; compare Definition~\ref{defn:RCONmodel} below. RCON models are submodels of undirected Gaussian models (Example~\ref{ex:UndirectedGraphicalModelIntro}) and obey symmetries among the entries of the concentration matrix according to a graph colouring.
It is interesting to study possible connections between RDAG and RCON models, similar to the known connection between DAG models and undirected Gaussian graphical models in Theorem~\ref{thm:DAGCONeqChapter6}. This may allow to study RCON models through the lens of RDAG models.

Second, vertex and edge symmetries appear in various applications, such as in the study of longitudinal data~\cite{abbruzzo2016operational,vinciotti2016model}, or clustered variables~\cite{gao2015estimation,hojsgaard2008graphical}.
Therefore, it is desirable to include these symmetries in the model itself.
The coloured directed graph gives an intuitive pictorial description of these symmetry conditions.

Third, we aim to decrease the maximum likelihood (ML) thresholds (Definition~\ref{defn:MLthresholds}), since for applications it is desirable to have small ML thresholds. We comment that innovative ideas have been used to find maximum likelihood thresholds in graphical models~\cite{buhl1993existence,drton2019maximum,gross2018maximum,uhler2012geometry} and for estimating MLEs from too few samples~\cite{friedman2008sparse,wille2004sparse}. 
Removing edges from a graph can lower the threshold~\cite{uhler2012geometry, LauritzenBook}, but there is a trade-off: removing edges imposes more conditional independence among the variables. This is why, instead, we aim to decrease the maximum likelihood threshold by introducing symmetries. 

Let us illustrate these motivations in the following running example that we shall use throughout the chapter.

\begin{example}
	\label{ex:very_first}
	Consider the coloured graph \begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red] \ar[l, red] & {\color{blue}\circled{2}}
	\end{tikzcd}, with blue (circular) vertices $\{ 1, 2\}$, black (square) vertex $3$ and two red edges.
	The RDAG model is the linear structural equation model
	\[ y_1 = \lambda y_3 + \veps_1 , \qquad y_2 = \lambda y_3 + \veps_2, \qquad y_3 = \veps_3, \]
	where $\veps_1, \veps_2 \sim \Ncal(0,\omega)$ and $\veps_3 \sim \Ncal(0,\omega')$, i.e., $\omega$ is the variance of the blue vertices $1$ and $2$, and $\omega'$ is the variance of black vertex $3$. The third parameter $\lambda$ is the regression coefficient given by a red edge.
	
	Regarding our three motivations we note the following. First, Example~\ref{ex:RCONequalsColoredTDAG} will show that the above RDAG model equals its induced RCON model. Hence, one may study the latter through the former.
	Second, we use this example to model the dependence of two daughters' heights on the height of their mother, and we compute the MLE given some sample data; see Example~\ref{ex:heights}.
	Third, in Example~\ref{ex:running5} we will see that the ML threshold $\mlt_u$ for uniqueness is one.
	In contrast, if we remove the colours the resulting DAG model has uniqueness threshold two, compare Corollary~\ref{cor:MLthresholdsDAG}.
	\hfill\exSymbol
\end{example}


\paragraph{Related Models.}
To the knowledge of the authors of \cite{RDAG}, RDAG models have not been defined before in the literature. We comment on some related models.
The assumption of equal variances from~\cite{peters2014identifiability} is the special case of an RDAG model, where all vertex colours are the same.
Special colourings encode exchangeability between variables, or invariance under a group of permutations.
A graphical model is combined with group symmetries in the directed setting in~\cite{madsen2000invariant} and in the undirected setting in~\cite{andersson1998symmetry,shah2012group}. 
RDAG models also relate to the fused graphical lasso~\cite{danaher2014joint}, which penalises differences between parameters on different edges, whereas in an RDAG model the parameters on edges of the same colour must be equal.

\paragraph{Main Results.}

As a generalization of Theorem~\ref{thm:LinearIndependenceDAG} for DAG models, we characterize the existence and uniqueness of MLEs via linear algebraic properties of the sample data, see Theorem~\ref{thm:RDAGMLestimationLinDependence}. 
We give a closed-form formula for MLEs in an RDAG model, as a collection of least squares estimators, see Algorithm~\ref{algo:RDAG-MLE}. In Theorem~\ref{thm:RDAGboundsMlt} we provide upper and lower bounds on ML thresholds for RDAG models. Our results show that RDAG thresholds are less or equal to the DAG thresholds, and that high symmetry decreases the thresholds. Thus, the third motivation about decreasing ML thresholds is achieved.
Furthermore, we compare RDAG MLEs to uncoloured DAG MLEs via simulations in Section~\ref{sec:SimulationsRDAG}.
All results hold with an assumption on the graph colouring called {\em compatibility} (Definition~\ref{defn:compatibleColouring}), which allows to view RDAG models in a natural way as Gaussian models via symmetrization. It is an open problem to extend our results to the non-compatible setting, as well as to directed graphs with cycles. It is also an open problem to find the \emph{exact} ML thresholds, see Problem~\ref{prob:RDAG-thresholds}.

Regarding RCON models, the undirected analogue of RDAG models, we note the following.
Although a motivation for the graph colouring in RCON models is to lower the maximum likelihood threshold, there are relatively few graphs for which the threshold is known: colourings of the four cycle are studied in~\cite[\S 6]{uhler2012geometry},~\cite[\S 5]{sturmfels2010multivariate}, while an example with five vertices is~\cite[Example 3.2]{uhler2012geometry}.
In certain cases, RDAG models are equivalent to RCON models. We exactly determine the conditions under which this occurs in Theorem~\ref{thm:RCONequalsRDAG}. As a consequence, we obtain an entire class of RCON models where conditions for MLE existence and uniqueness can be found by appealing to our results on RDAGs.

Finally, we draw connections to stability notions and to Gaussian group models, which are studied in Chapter~\ref{ch:GaussianGroupModels}. Namely, we extend the dictionary between ML estimation and stability notions to RDAGs in Theorem~\ref{thm:RDAGstabilityVsMLE}. This requires the extended concept of stability \emph{under sets} from Definition~\ref{defn:StabilitySets}.
Furthermore, we identify RDAGs that are Gaussian group models in Proposition~\ref{prop:butterfly} and generalize a proof via Popov's Criterion from the TDAG setting (Theorem~\ref{thm:FullCorrespondenceTDAG}) to RDAGs that are Gaussian group models. We also obtain in the group situation a bijection between the stabilizer and the set of MLEs, Proposition~\ref{prop:StabilizerMLEsGroupRDAG}.

While not evident in the final presentation, the ``invariant theory perspective'' fostered the understanding and created concepts needed to obtain many of the results. For example, trying to link RDAG models in a natural way to Gaussian models via symmetrization lead to the notion of a compatible colouring, while trying to generalize a proof via Popov's Criterion resulted in the concept of augmented sample matrices (Definition~\ref{defn:MYs}).



\paragraph{Organization and Assumptions.}
Section~\ref{sec:IntroRDAG} defines RDAG models, compatible colourings and states basic properties. We compare RDAG and RCON models in Section~\ref{sec:RDAGvsRCON}. Afterwards, we characterize ML estimation for RDAG models in Section~\ref{sec:MLE-RDAG}, which enables us in Section~\ref{sec:ThresholdsRDAG} to bound the ML thresholds. Section~\ref{sec:SimulationsRDAG} presents some simulations. We end with connections to stability and to Gaussian group models in Sections~\ref{sec:RDAGsAndStability} and~\ref{sec:RDAGsGaussianGroupModels}, respectively.

In contrast to the paper \cite{RDAG} we always work in parallel over $\RR$ and~$\CC$.\footnote{\cite{RDAG} usually worked over $\RR$, but it was noted that the results extend to the complex case \cite[Remark~2.11]{RDAG}.}
Therefore, $\KK \in \{ \RR, \CC\}$ and we remind the reader that $(\cdot)\HT$ is the Hermitian transpose, which is just the transpose $(\cdot)\T$ if $\KK = \RR$.





\section{Introducing RDAG models} \label{sec:IntroRDAG}

In the following we introduce restricted DAG (short: RDAG) models and give several illustrating examples. Moreover, we define the important concept of a \emph{compatible} colouring, which is a common assumption in Chapter~\ref{ch:RDAGs}. In Lemma~\ref{lem:PropertiesCompatibleColouring} we prove important properties of a compatible colouring which we will use throughout. As a main result, we show that an RDAG model admits a natural parametrization as a Gaussian model via symmetrization if and only if the colouring is compatible, see Proposition~\ref{prop:RDAGmodelEqualsMgAGc}. This result is analogous to Lemma~\ref{lem:DAGmodelEqualsMgAG}.
We start with the definition of a coloured DAG.

\begin{defn}\label{defn:ColouredDAG}
	A \emph{coloured DAG}\index{directed acyclic graph!coloured}\index{DAG!coloured| see {directed acyclic graph, coloured} } is a tuple \gls{Gc}, where $\Gcal = (I, E)$ is a DAG on vertices $I$ and directed edges $E$, and
	\[ c \colon I \cup E \rightarrow \Col \]
	is a \emph{colouring}\index{colouring} of the vertices and edges. Vertex $i \in I$ has colour $c(i) \in \Col$, and edge $j \to i$ has colour $c(ij) \in \Col$. We sometimes denote the vertex colour $c(i)$ by $c(ii)$, with no ambiguity because a DAG cannot have loops.
	\hfill\defnSymbol
\end{defn}

In Definition~\ref{defn:DAGmodel} we introduced DAG models. Similarly, we can define sub-models of these by introducing symmetries among the parameters, which are given by a graph colouring.

\begin{defn}[{\cite[Definition~2.1]{RDAG}}] \label{defn:RDAGmodelViaLDL}
	The {\em restricted DAG (RDAG) model}\index{restricted DAG model} \gls{MGcar} on the coloured DAG $\Gc$ is
	the set of concentration matrices $\Psi = (\Id_m - \Lambda)\HT \Omega^{-1} (\Id_m - \Lambda)$,
	where $\Lambda \in \KK^{m \times m}$ satisfies
	\begin{enumerate}
		\item $\lambda_{ij} = 0$ unless $j \to i$ in $\Gcal$
		\item $\lambda_{ij} = \lambda_{kl}$ whenever edges $j \to i$ and $l \to k$ have the same colour
	\end{enumerate}
	and the diagonal matrix $\Omega \in \PD_m(\KK)$ has positive entries and satisfies
	\begin{enumerate}
		\item[3.] $\omega_{ii} = \omega_{jj}$ if vertices $i$ and $j$ have the same colour.
	\end{enumerate}
	The model $\MGcar$ is given by the linear structural equation
	$y = \Lambda y + \varepsilon$,
	where $y \in \KK^m$ and $\varepsilon \sim \Ncal(0,\Omega)$. By construction, $\MGcar \subseteq \MGar$.
	\hfill\defnSymbol
\end{defn}

\begin{example}[{\cite[Example~2.2]{RDAG}}]
	\label{ex:RDAGminus1}
	Let $\Gc$ be  \begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red] \ar[l, red] & {\color{blue}\circled{2}}
	\end{tikzcd}, the coloured DAG from Example~\ref{ex:very_first}.
	The RDAG model $\MGcar \subseteq \PD_3(\KK)$ is parametrized by matrices
	\[ 
	\Lambda = \begin{pmatrix} 0 & 0 & \lambda \\ 0 & 0 & \lambda \\ 0 & 0 & 0 \end{pmatrix}
	\qquad \text{and} \qquad 
	\Omega = \begin{pmatrix} \omega & 0 & 0 \\ 0 & \omega & 0 \\ 0 & 0 & \omega' \end{pmatrix}
	\]
	where $\lambda \in \KK$ and $\omega, \omega' \in \RR_{>0}$.
	\hfill\exSymbol
\end{example}

\begin{remark}[based on {\cite[Remark~2.3]{RDAG}}]
	Lemma~\ref{lem:DAGmodelEqualsMgAG} shows that any DAG model $\MGar$ admits a natural set $\AG$ such that $\MGar = \Mg_{\AG}$. It is desirable to view an RDAG model $\MGcar$ in a natural, analogous way as a Gaussian model via symmetrization.
	In fact, the alternative parametrization has useful consequences. First, it leads to a condition on the graph colouring, called compatibility, which is indispensable in our results of Sections~\ref{sec:MLE-RDAG} and \ref{sec:ThresholdsRDAG}. Second, it is helpful when comparing directed and undirected coloured models in Section~\ref{sec:RDAGvsRCON}. Third, it allows to generalize the connections between TDAG models and stability notions to the setting of RDAG models, see Section~\ref{sec:RDAGsAndStability} and~\ref{sec:RDAGsGaussianGroupModels}.
	\hfill\remSymbol
\end{remark}

Given a coloured DAG $\Gc$, we define the set
\begin{equation}
	\label{eq:defnAGc} %formerly known as eqn:Agc
	\AGc := \left\lbrace a \in \GL_m(\KK) \bigg| \; 
	\begin{matrix} a_{ij}=0 \text{ for } i \neq j \text{ with } j \not \to i \text{ in }  \mathcal{G} \\ 
		a_{ij} = a_{kl} \text{ whenever }  c(ij) = c(kl) \end{matrix} \right\rbrace.
\end{equation}
Note that $\AGc$ is contained in the set $\AG$ from Equation~\eqref{eq:defnAG}: their zero patterns agree and $\AGc$ has further equalities imposed by the colouring $c$.

\begin{example}[{\cite[Example~2.4]{RDAG}}]
	\label{ex:RDAG0}
	For the coloured DAG
	\begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red] \ar[l, red] & {\color{blue}\circled{2}}
	\end{tikzcd}
	we have 
	\[ \AGc = \left\lbrace \begin{pmatrix} d_1 & 0 & r \\ 0 & d_1 & r \\ 0 & 0 & d_2 \end{pmatrix} \colon d_1, d_2 \in \KK^{\times}, \; r \in \KK \right\rbrace .\]
	and hence
		\begin{equation}
		\label{eq:RDAG1AGc}
		\Mg_{\AGc} =
		\left\lbrace \begin{pmatrix} |d_1|^2 & 0 & r \overline{d_1} \\ 0 & |d_1|^2 & r \overline{d_1} \\ \overline{r} d_1 & \overline{r} d_1 & 2|r|^2 + |d_2|^2 \end{pmatrix} \, \Bigg\vert \, d_1, d_2 \in \KK^{\times}, \; r \in \KK \right\rbrace.
	\end{equation}
	is the corresponding Gaussian model via symmetrization.
	\hfill\exSymbol
\end{example}

For DAG models we always have $\MGar = \Mg_{\AG}$, compare Lemma~\ref{lem:DAGmodelEqualsMgAG}. In contrast, the models $\MGcar$ and $\Mg_{\AGc}$ do \emph{not} have to be equal. The following assumption on a colouring turns out to be necessary and sufficient for equality.

\begin{defn}[{\cite[Definition~2.5]{RDAG}}]\label{defn:compatibleColouring}
	A colouring $c$ of a directed graph is \emph{compatible}\index{colouring!compatible}, if:
	\begin{itemize}
		\item[(i)] vertex colours and edge colours are disjoint; and
		\item[(ii)] whenever edges $j \to i$ and $l \to k$ have the same colour, then the child vertices $i$ and $k$ have the same colour, i.e., $c(ij) = c(kl)$ implies $c(i) = c(k)$.
	\end{itemize}
	Note: compatibility does \emph{not} impose equality of parent colours $c(j)$ and $c(l)$.
	\hfill\defnSymbol
\end{defn}

\begin{remark}[Statistical meaning of compatibility, {\cite[Remark~2.6]{RDAG}}]
	\ \\
	In an RDAG model we do not impose equalities between $\Omega$ and $\Lambda$.
	The entry $\omega_{ii}$ is a variance, while $\lambda_{kl}$ is a regression coefficient, so setting them to be equal would be difficult to interpret.
	Hence the vertex and edge colours can always be thought of as disjoint, 
	as in compatibility condition~(i). It ensures that Equation~\eqref{eq:defnAGc} does not impose equalities between a diagonal and an off-diagonal entry.
	Compatibility condition~(ii) has the statistical interpretation that the same regression coefficient appearing in an expression for two variables implies that their error variances agree.
	This extra assumption is indispensable in many of the upcoming results and proofs.
	It is a directed analogue to the condition appearing in~\cite[Proposition~1]{hojsgaard2008graphical}.
	\hfill\remSymbol
\end{remark}

Before we relate $\MGcar$ and $\Mg_{\AGc}$, let us prove important properties of a compatible colouring.\footnote{These properties occur throughout \cite{RDAG}, but were not collected in a separate theorem environment.}
These will be frequently, often implicitly, used in the upcoming sections.
To state the results, we define for a coloured DAG $\Gc$ the set of \emph{parent relationship colours}\index{parent relationship colours} of vertex colour $s$ as 
\begin{equation}\label{eq:defnPRCs}
	\prc(s) := \{c(ij) \mid \text{there exists } j \to i \text{ in } \Gcal \text{ with } c(i) = s\}.
\end{equation}
In words, the set $\prc(s)$ contains the colours of all edges that point towards some vertex of colour $s$.

\begin{lemma}\label{lem:PropertiesCompatibleColouring}
	Let $\Gc$ be a coloured DAG with compatible colouring $c$. Then:
	\begin{itemize}
		\item[(i)] The sets of parent relationship colours partition the set of edge colour classes, i.e., we have a disjoint union $\; c(E) = \bigsqcup_{s \in c(I)} \prc(s)$. % prc(s) may be empty!!
		
		\item[(ii)] Every matrix $a \in \AGc$ is uniquely determined by the following data: an entry $a_{s,s} \in \KK^{\times}$ for each vertex colour $s \in c(I)$ and an entry $a_{s,t} \in \KK$ for the edge colour encoded by $s \in c(I)$ and $t \in \prc(s)$.\\
		Similarly, matrices $\Omega$ and $\Lambda$ as in Definition~\ref{defn:RDAGmodelViaLDL} are uniquely determined by entries $\omega_{s,s} \in \RR_{>0}$ and $\lambda_{s,t} \in \KK$, respectively.
		
		\item[(iii)] The set $T$ of diagonal matrices in $\AGc$ is an algebraic and
		%$\; T \cdot \AGc = \AGc$. In particular,
		for $t \in T$, $a \in \AGc$ it holds that $\, ta \in \AGc$.
		
		\item[(iv)] Let $U$ be the set of unipotent upper triangular matrices in $\AGc$. For any $a \in \AGc$ there exist unique $t(a) \in T$ and $u(a) \in U$ with $a = t(a) u(a)$.
	\end{itemize}
	Parts~(iii) and (iv) still hold for $\AGc_{\SL}$, with the only change that $T$ %note that this is diagonalizable matrices in AGc_SL !!
	is in general just a diagonalizable group, i.e., it does not need to be connected. %todo formulate this as part (v)
\end{lemma}

\begin{proof}
	To prove (i), let $j \to i$ be an edge in $\Gcal$. Then $c(ij) \in \prc(s_1)$, where $s_1 := c(i)$, and hence $c(E) = \bigcup_{s \in c(I)} \prc(s)$. Moreover, if $c(ij) \in \prc(s_2)$ then there is some $l \to k$ in $\Gcal$ with $c(k) = s_2$. By Definition~\ref{defn:compatibleColouring}(ii), compatibility implies $s_1 = s_2$ and therefore the sets $\prc(s)$, $s \in c(I)$ are disjoint.
	
	For (ii), note that by Definition~\ref{defn:compatibleColouring}(i) the Equation~\eqref{eq:defnAGc} never requires an equality of a diagonal with an off-diagonal entry of $a \in \AGc$. Therefore, $a$ is uniquely determined by a non-zero diagonal entry for each vertex colour and an entry for each edge colour. The edge colours are in bijection with tuples $(s,t)$ where $s \in c(I)$ and $t \in \prc(s)$, by part~(i). This finishes the argument for matrix $a$ and similarly one obtains the claim for $\Omega$ and $\Lambda$.
	
	Using part~(ii), one directly verifies that $T$ is a group which is naturally isomorphic to the algebraic torus $(\KK^{\times})^{|c(I)|}$.
	%Of course, $T \cdot \AGc \supseteq \AGc$ as $\Id_m \in T$.
	Now, let $t \in T$, $a \in \AGc$ and set $b := ta$. Since $a \in \AG$ and multiplication with an invertible diagonal matrix preserves the support, we have $b \in \AG$. It remains to check $b_{ij} = b_{kl}$ whenever $c(ij) = c(kl)$. First, note that by condition~(i) of compatibility there are no equalities between diagonal and off-diagonal entries of $b$ required. Second, for two vertices $i,k$ with $c(ii) = c(kk)$ we have $t_{ii} = t_{kk}$, $a_{ii} = a_{kk}$ and so $b_{ii} = t_{ii} a_{ii} = t_{kk} a_{kk} = b_{kk}$. Third, for edges $j \to i$ and $l \to k$ of same colour we have $a_{ij} = a_{kl}$ and condition~(ii) of compatibility implies $c(ii) = c(kk)$, so $t_{ii} = t_{kk}$. Therefore, $b_{ij} = t_{ii} a_{ij} = t_{kk} a_{kl} = b_{kl}$. This proves~(iii).
	
	To show (iv), let $a \in \AGc$ and, taking part~(ii) into account, define $t \in T$ via $t_{ss} := a_{ss}$ for $s \in c(I)$. By part~(iii), $t^{-1} \in T$ and $u := t^{-1} a \in \AGc$. By construction, we have $a = tu$ and $u_{ss} = 1$ for all vertex colours $s$, so $u \in U$. This shows existence. To prove uniqueness, let $t' \in T$ and $u' \in U$ such that $a = t' u'$. As $u'$ is unipotent we must have $(t')_{ss} = a_{ss}$ for all $s \in c(I)$, so $t = t'$. The latter implies $u' = (t')^{-1} a = t^{-1} a = u$.
	
	Finally, consider the set $\AGc_{\SL} = \AGc \cap \SL_m(\KK)$. In this situation, the set $T$ of diagonal matrices in $\AGc_{\SL}$ is a group, which is naturally isomorphic to the multiplicative group $\big\{ (t_{ss})_s \in (\KK^\times)^{|c(I)|} \mid \prod_s t_{ss}^{\alpha_s} = 1 \big\}$, where $\alpha_s$ is the number of vertices of colour $s$. The character group is $\Xfrak(T) \cong \ZZ^{|c(I)|} / \big( \ZZ \cdot (\alpha_s)_{s \in c(I)} \big)$. Hence, $T$ is connected, i.e., an algebraic torus, if and only if the greatest common divisor of all $\alpha_s$ equals one, compare ??? %todo refer to subsection on linear algebraic groups
	%todo say more precisely what this is good for.
	Now, for $t \in T$ and $a \in \AGc_{\SL}$, we have $\det(ta)=1$ and $ta \in \AGc$ by part~(iii) for $\AGc$. Thus, $ta \in \AGc_{\SL}$. Furthermore, any $a \in \AGc_{\SL}$ has a unique decomposition $a = t(a) u(a)$ in $\AGc$ by part~(iv). Since $u$ is unipotent it has determinant one, and as $\det(a)=1$ we must have $\det(t)=1$ as well. We deduce that the unique decomposition $a = t(a) u(a)$ actually lives in $\AGc_{\SL}$.
\end{proof}

A main feature of compatibility is relating the models $\MGcar$ and $\Mg_{\AGc}$.\footnote{Trying to relate these models was actually how the authors of \cite{RDAG} came up with the concept of a compatible colouring.}

\begin{prop}[{\cite[Proposition~2.7]{RDAG}}]\label{prop:RDAGmodelEqualsMgAGc}
	%formerly known as prop:compatibleColouring
	Fix a coloured DAG $\Gc$. The RDAG model $\MGcar$ is equal to $\Mg_{\AGc}$ if and only if colouring $c$ is compatible.
\end{prop}

\begin{remark}\label{rem:DAGmodelViaCompatible}
	A usual DAG model $\MGar$ on $\Gcal$ is an RDAG model with compatible colouring, as follows. Let $c$ be a colouring that assigns to each vertex and to each edge a distinct colour. By construction, vertex and edge colours are disjoint. Moreover, condition~(ii) of compatibility holds automatically as there are no two edges of same colour. Since colouring $c$ does not impose any colour symmetries, we have $\MGar = \MGcar$ and $\AG = \AGc$.\\
	In this regard, Proposition~\ref{prop:RDAGmodelEqualsMgAGc} generalizes Lemma~\ref{lem:DAGmodelEqualsMgAG}.
	\hfill\remSymbol
\end{remark}

To prove the proposition, it is instructive to think of $\MGcar$ and $\Mg_{\AGc}$ imposing zero patterns and symmetries on certain matrix decompositions.

Recall that the Cholesky decomposition of $\Psi \in \PD_m(\KK)$ is given by the \emph{unique} upper triangular matrix $a \in \KK^{m \times m}$ with \emph{real-valued,  positive} diagonal entries such that $\Psi = a\HT a$. The model $\Mg_{\AGc}$ imposes zeros and symmetries in the Cholesky decomposition, as follows.

\begin{lemma}[{\cite[Lemma~2.8]{RDAG}}]
	\label{lem:CholeskyMgAGc} %formerly known as lem:cholesky_MA
	Fix a coloured DAG $\Gc$ with compatible colouring $c$. Then $\Mg_{\AGc}$ is the set of positive definite matrices with Cholesky decomposition $a\HT a$ for some $a \in \AGc$.
\end{lemma}

\begin{proof} 
	The set $\Mg_{\AGc}$ consists of all positive definite matrices of the form
	$a\HT a$ for some $a \in \AGc$, see Equation~\eqref{eq:GaussianModelMA}, and all matrices in $\AGc$ are upper triangular by our assumption on the ordering of the vertices.
	
	It remains to show that for any $\Psi = b\HT b$, where $b \in \AGc$, its Cholesky decomposition lies in $\AGc$.	
	For $i \in [m]$, set $t_{ii} := \overline{b_{ii}} |b_{ii}|^{-1}$. This defines a diagonal matrix $t$ such that $t\HT t = \Id_m$ and $t \in \AGc$ as $b \in \AGc$.
	Thus, $a := tb \in \AGc$ using Lemma~\ref{lem:PropertiesCompatibleColouring}(iii). By construction, $a$ has positive diagonal entries $a_{ii} = |b_{ii}|$ and hence $a\HT a$ is the Cholesky decomposition of $\Psi$.
\end{proof}

%todo mentionLDL decomposition already in DAG section!
The LDL decomposition writes a positive definite matrix $\Psi \in \PD_m(\KK)$ as $LDL\HT$, where $D$ is diagonal with positive entries, and $L \in \KK^{m \times m}$ is lower triangular and unipotent (i.e., its diagonal entries are equal to one). With these properties $L$ and $D$ are uniquely determined.
The LDL decomposition is closely related to the factorization $\Psi = (\Id_m - \Lambda)\HT \Omega^{-1} (\Id_m - \Lambda)$ from~Equation~\eqref{eq:DAGmodelConcentration}: the LDL decomposition is $D = \Omega^{-1}$ and $L = (\Id_m - \Lambda)\HT$.
Hence, an RDAG model $\MGcar$ imposes zeros and symmetries in the LDL decomposition.
The LDL and Cholesky decompositions are are related by: %todo
%	\begin{align*}
%		\text{Cholesky from LDL:} \qquad & a = D^{1/2} L\HT, \\ 
%		\text{LDL from Cholesky:} \qquad & D = \mathrm{diag}(a_{11}^2,\ldots,a_{mm}^2), \quad  L\HT = D^{-1/2} a.
%	\end{align*}
	\begin{align*}
		\text{Cholesky from LDL:} \quad & a = \Omega^{-1/2} (\Id_m - \Lambda), \\ 
		\text{LDL from Cholesky:} \quad & \Omega = \diag( a_{11}^{-2},\ldots, a_{mm}^{-2}), \quad 
		\Lambda = \Id_m - \diag(a_{11}^{-1}, \ldots, a_{mm}^{-1}) a
	\end{align*}

%todo delete??
%\begin{lemma}\label{lem:AGcVsOmegaLambda} 
%	Let $\Gc$ be a coloured DAG with compatible colouring.
%	The set $\AGc$ is linked to the definition of $\MGcar$ as follows.
%	\begin{itemize}
%		\item[(i)] A diagonal matrix $\Omega \in \PD_m(\KK)$ satisfies the third condition from Definition~\ref{defn:RDAGmodelViaLDL} if and only if $\Omega \in \AGc$.
%		
%		\item[(ii)] A matrix $\Lambda \in \KK^{m \times m}$ obeys the conditions from Definition~\ref{defn:RDAGmodelViaLDL} if and only if $(\Id_m - \Lambda)$ is unipotent upper triangular and contained in $\AGc$.
%	\end{itemize}
%\end{lemma}

For DAG models, we have shown $\MGar = \Mg_{\AG}$, Lemma~\ref{lem:DAGmodelEqualsMgAG}, by comparing the support conditions in the two decompositions.
Similarly, we prove Proposition~\ref{prop:RDAGmodelEqualsMgAGc} for RDAG models by comparing zero patterns and symmetries in the LDL and Cholesky decomposition. For this, Lemma~\ref{lem:PropertiesCompatibleColouring}(iii) is the crucial property of a compatible colouring.

\begin{proof}[Proof of Proposition~\ref{prop:RDAGmodelEqualsMgAGc}.]	%CONTINUE HERE
	Let colouring $c$ be compatible. Recall, that condition~(i) of compatibility implies that Equation~\eqref{eq:defnAGc} does not impose equalities between a diagonal and an off-diagonal entry of $a \in \AGc$.
	
	First, let $\Psi = (\Id_m - \Lambda)\HT \Omega^{-1} (\Id_m - \Lambda) \in \MGcar$ as in Definition~\ref{defn:RDAGmodelViaLDL}. The colour conditions on $\Omega \in \PD_m(\KK)$ show that the diagonal matrix $t := \Omega^{-1/2}$ is in $\AGc$. Moreover, $\Id_m - \Lambda \in \AGc$ as follows. First, it is unipotent upper triangular, as $\Lambda$ is strictly upper triangular. In particular, the vertex colour conditions are fulfilled as all diagonal entries are equal to one. Second, the support and colour conditions on $\Lambda$ imply that the off-diagonal entries of $\Id_m - \Lambda$ satisfy the corresponding conditions for $\AGc$.
	By Lemma~\ref{lem:PropertiesCompatibleColouring}(iii), $a := \Omega^{-1/2} (\Id_m - \Lambda) \in \AGc$ and hence $\Psi = a\HT a \in \Mg_{\AGc}$.
	
	Conversely, let $\Psi \in \Mg_{\AGc}$. Then the Cholesky decomposition is $\Psi = a\HT a$ for $a \in \AGc$, by Lemma~\ref{lem:CholeskyMgAGc}. Since $a$ has positive diagonal entries and $a \in \AGc$, $\omega_{ii} := a_{ii}^{-2}$ defines a diagonal $\Omega \in \PD_m(\KK)$ satisfying the colour symmetries. Moreover, $u := \diag(a_{11}^{-1}, \ldots, a_{mm}^{-1}) a$ is, by construction, unipotent upper triangular and, by Lemma~\ref{lem:PropertiesCompatibleColouring}(iii), $u \in \AGc$. Therefore, $\Lambda = (\Id_m - u)$ is strictly upper triangular and satisfies the support and colour conditions from Definition~\ref{defn:RDAGmodelViaLDL}. This shows $\Psi \in \MGcar$. Altogether, a compatible colouring implies $\MGcar = \Mg_{\AGc}$.
	
	Now, assume the colouring is not compatible. We will exhibit some $\Psi \in \MGcar$, in terms of $\Omega$ and $\Lambda$, such that $\Psi \notin \Mg_{\AGc}$. For this, let $\Psi = a\HT a$ be the Cholesky decomposition, i.e.,
	\begin{equation}
		\label{eq:entries_a}
		a_{ij} = \begin{cases} \omega_{ii}^{-1/2} & \text{if } i = j \\ 
			-\omega_{ii}^{-1/2} \lambda_{ij} 
			& \text{if } i \neq j .
		\end{cases} 
	\end{equation}
	If $\Psi = b\HT b$ for some $b \in \AGc$ then, similar to the proof of Lemma~\ref{lem:CholeskyMgAGc}, there is some diagonal matrix $t$ with $tb = a$ and $|t_{ii}| = 1$ for all $i \in [m]$.\footnote{However, we cannot deduce $a \in \AGc$, because compatibility is needed for Lemma~\ref{lem:PropertiesCompatibleColouring}(iii).} In particular, $|b_{ij}| = |a_{ij}|$ for all $i,j \in [m]$.
	
	First, if Definition~\ref{defn:compatibleColouring}(i) does not hold, then there is a vertex $k \in [m]$ and an edge $j \to i$ with $c(kk) = c(ij)$. The RDAG model imposes no relation between $\omega_{kk}$ and $\lambda_{ij}$, so let $\Psi$ be given by some $\Omega$ and $\Lambda$ with $\omega_{kk} = 1$ and $\lambda_{ij} = 0$. Then $|a_{kk}| = 1$ and $|a_{ij}| = 0$, by~\eqref{eq:entries_a}. Hence, $\Psi \notin \Mg_{\AGc}$ as otherwise $|b_{kk}| = |a_{kk}| = 1 \neq 0 = |a_{ij}| = |b_{ij}|$ violates the colour conditions for $\AGc$.
	
	Second, if Definition~\ref{defn:compatibleColouring}(ii) does not hold, then there exist edges $j \to i$ and $l \to k$ with $c(ij) = c(kl)$ but $c(i) \neq c(k)$. We choose $\Psi$ given by some $\Omega$ and $\Lambda$ with $\omega_{ii} = 1$, $\omega_{kk} = \frac{1}{4}$ and $\lambda_{ij} = \lambda_{kl} = 1$. Then $|a_{ij}| = 1$ and $|a_{kl}| = 2$, by \eqref{eq:entries_a}. Again, we must have $\Psi \notin \Mg_{\AGc}$ as otherwise $|b_{ij}| = |a_{ij}| = 1 \neq 2 = |a_{kl}| = |b_{kl}|$ would violate the colour conditions for $\AGc$.
\end{proof}

\begin{example}[{\cite[Example~2.10]{RDAG}}]
	\label{ex:RDAG1}
	We return to the coloured DAG
	\begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red] \ar[l, red] & {\color{blue}\circled{2}}
	\end{tikzcd}
	from Examples~\ref{ex:RDAGminus1} and~\ref{ex:RDAG0}.
	The colouring is compatible, because the sets of vertex and edge colours are disjoint, and the children of both red edges have the same colour. Hence, Proposition~\ref{prop:RDAGmodelEqualsMgAGc} shows that $\Mg_{\AGc}$ from Equation~\eqref{eq:RDAG1AGc} is equal to $\MGcar$.
	\hfill\exSymbol
\end{example}




\section{Comparison of RDAG and RCON models} \label{sec:RDAGvsRCON}

In this section we compare RDAG models to their undirected analogue: restricted concentration (RCON) models which were introduced in \cite{hojsgaard2008graphical}. Similar to RDAG models, RCON models are sub-models of undirected Gaussian graphical (CON) models, see Example~\ref{ex:UndirectedGraphicalModelIntro}, and impose symmetries on concentration matrices according to a graph colouring. In Theorem~\ref{thm:RCONequalsRDAG} we precisely characterize when an RDAG model equals its induced RCON model. To prove this theorem, we need the similar statement for DAG models and CON models, Theorem~\ref{thm:DAGCONeqChapter6}. It is well-known in the literature, see \cite[Theorem~3.1]{andersson1997markov} or \cite[Theorem~5.6]{frydenberg1990chain}. Still, it is instructive to start with a proof of Theorem~\ref{thm:DAGCONeqChapter6}, since the presented method generalizes to give a proof of Theorem~\ref{thm:RCONequalsRDAG}.

\medskip

Given a DAG $\Gcal$, remember that $\Gcal^u$ denotes the corresponding undirected graph, which is obtained by forgetting the direction of each edge in $\Gcal$.
For convenience, we restate Theorem~\ref{thm:DAGCONeqChapter6}.

\begin{theorem}[Theorem~\ref{thm:DAGCONeqChapter6} restated] 
	\label{thm:DAGCONeq}
	Let $\Gcal$ be a DAG. The DAG model $\MGar$ is equal to the undirected Gaussian graphical model $\Mud_{\Gcal^u}$ on $\Gcal^u$ if and only if $\Gcal$ has no unshielded colliders.
\end{theorem}

We prove Theorem~\ref{thm:DAGCONeq} via two propositions. Note that these propositions and their proofs only appear in the \emph{first} arXiv version of \cite{RDAG}, e.g., the following is Proposition~3.8 in the first arXiv version.

\begin{prop}\label{prop:DAGinCON}
	Let $\Gcal$ be a DAG. Then $\MGar \subseteq \Mud_{\Gcal^u}$ if and only if $\Gcal$ has no unshielded colliders.
\end{prop} 

\begin{proof}
	The DAG model $\MGar$ equals $\Mg_{\AG}$, by Lemma~\ref{lem:DAGmodelEqualsMgAG}. Assume $\Gcal$ has an unshielded collider \begin{tikzcd}[cramped, sep=small] i \ar[r] & k & j \ar[l] \end{tikzcd}. In particular, $\Gcal^u$ has no edge between $i$ and $j$, so $\Psi_{ij} = \Psi_{ji} = 0$ for all $\Psi \in \Mud_{\Gcal^u}$. Let $a \in \AG$ be given by $a_{ki} = a_{kj} = 1$, $a_{ll}  = 1$ for all $l \in [m]$ and all other entries zero. Then $(a\HT a)_{ij} = \overline{a_{ki}} a_{kj} = 1 \neq 0$ and hence $a\HT a \notin \Mud_{\Gcal^u}$.
	
	Conversely, if $\Mg_{\AG} = \MGar \nsubseteq \Mud_{\Gcal^u}$ then there is $a \in \AG$ with $a\HT a \notin \Mud_{\Gcal^u}$. Thus, $a\HT a$ violates the off-diagonal zero pattern of $\Mud_{\Gcal^u}$, i.e., there is a pair of indices $i \neq j$ such that there is no edge between $i$ and $j$ in $\Gcal^u$ but $ (a\HT a)_{ij} \neq 0$. Since $(a\HT a)_{ij} = \sum_{k=1}^m \overline{a_{ki}} a_{kj}$, some product $\overline{a_{ki}} a_{kj}$ must be non-zero, i.e., there must exist edges \begin{tikzcd}[cramped, sep=small] i \ar[r] & k & j \ar[l] \end{tikzcd} in $\Gcal$. This is an unshielded collider, because $\Gcal^u$ (and hence $\Gcal$) has no edge between $i$ and $j$.
\end{proof} 

The following is Proposition~3.9 in the \emph{first} arXiv version of \cite{RDAG}. The proof method is to run an algorithm for computing the Cholesky decomposition and checking the support conditions for $\AG$ on the fly.

\begin{prop}\label{prop:CONinDAG}
	If a DAG $\Gcal$ has no unshielded colliders, then $\Mud_{\Gcal^u} \subseteq \MGar$.
\end{prop}

\begin{proof}
	Given a concentration matrix $\Psi \in \Mud_{\Gcal^u}$, we show $\Psi \in \Mg_{\AG}$ by proving that its Cholesky decomposition is $\Psi = a\HT a$ with $a \in \AG$. %todo stress uniqueness of Cholesky decomposition
	%use notation \chol for this
	We induct over the number of vertices of $\Gcal$. If $\Gcal$ has one vertex, the Cholesky decomposition of the positive number $\Psi$ is the positive number $\sqrt{\Psi}$, which lies in $\AG = \KK^\times$.
	
	Assume the statement holds for all DAGs with $m$ vertices and let $\Gcal$ be a DAG with $m+1$ vertices. Our ordering of vertices ensures that vertex $m+1$ has no parents. Let $\tilde{\Gcal}$ be obtained from $\Gcal$ by removing vertex $m+1$ and its edges.
	The induction hypothesis applies to $\tilde{\Gcal}$.
	Hence, setting $\tilde{\Psi}$ to be the top left $m \times m$ block of $\Psi$,
	the Cholesky decomposition $\tilde{\Psi} = \tilde{a}\HT \tilde{a}$ satisfies $\tilde{a} \in \Aset(\tilde{\Gcal})$. We add another row and column to $\tilde{a}$ to construct the Cholesky decomposition $a$ of $\Psi$.
	Since $a$ is upper triangular, we set $a_{m+1,1} = \cdots = a_{m+1,m} = 0$. We are left to determine the last column of $a$ so that $\Psi = a\HT a$.
	%todo write down a general inductive formula, which can be referenced in later proofs
	We require
	\[\Psi_{1,m+1} = \sum_{k=1}^{m+1} \overline{a_{k,1}} a_{k,m+1} = \overline{a_{1,1}} a_{1,m+1} = a_{1,1} a_{1,m+1}, \]
	where we used $a_{k,1} = \tilde{a}_{k,1} = 0$ for $k \geq 2$ and $a_{1,1} = \tilde{a}_{1,1} \in \RR_{>0}$ by property of the Cholesky decomposition.
	Since $\Psi_{1,m+1}$ and $a_{1,1} > 0$ are already given, we set $a_{1,m+1} := \Psi_{1,m+1} / a_{1,1}$. This does not break the conditions of $\AG$: if $(m+1) \not\to 1$ in $\Gcal$, then $\Gcal^u$ has no edge between $m+1$ and $1$. Therefore, $\Psi_{1,m+1} = 0$ and so $a_{1,m+1} = 0$.
	Next, we require (using $a_{2,2} > 0$)
	\[\Psi_{2,m+1} = \sum_{k=1}^{m+1} \overline{a_{k,2}} a_{k,m+1} = \overline{a_{1,2}} a_{1,m+1} + a_{2,2} a_{2,m+1}. \]
	We set $a_{2,m+1} := (\Psi_{2,m+1} - \overline{a_{1,2}} a_{1,m+1}) / a_{2,2}$. This does not break the support conditions of $\AG$, as follows. If there is no edge from $m+1$ to $2$ in $\Gcal$, then $\Gcal^u$ has no edge between $m+1$ and $2$, so $\Psi_{2,m+1} = 0$. Moreover, $\overline{a_{1,2}} a_{1,m+1} = 0$, since otherwise $2 \to 1 \leftarrow m+1$ would be an unshielded collider. Hence $a_{2,m+1} = 0$ if there is no edge from $m+1$ to $2$ in~$\Gcal$. Repeating this process determines $a_{i,m+1}$ for $i = 3,4,\ldots,m$ inductively, ensuring $a_{i,m+1} = 0$ whenever there is no edge from $m+1$ to $i$ in $\Gcal$.
	It remains to choose a positive real $a_{m+1,m+1}$ such that $\Psi_{m+1,m+1} = \sum_{k \in [m+1]} \overline{a_{k,m+1}} a_{k,m+1}$. We set
	\[a_{m+1,m+1} := \Big( \Psi_{m+1,m+1} - \sum_{k=1}^m |a_{k,m+1}|^2 \Big)^{1/2}. \]
	The expression under the square root is a positive real number, see~\cite[Lecture 23]{trefethen1997numerical}. By construction, $a \in \AG$ is the Cholesky decomposition of $\Psi$.
\end{proof}

Combining Propositions~\ref{prop:DAGinCON} and \ref{prop:CONinDAG} proves Theorem~\ref{thm:DAGCONeq}.

\begin{proof}[Proof of Theorem~\ref{thm:DAGCONeq}]
	An unshielded collider in $\Gcal$ implies $\MGar \nsubseteq \Mud_{\Gcal^u}$, by Proposition~\ref{prop:DAGinCON}, and hence prevents equality of the models. The absence of unshielded colliders implies $\MGar \subseteq \Mud_{\Gcal^u}$ (Proposition~\ref{prop:DAGinCON}) and $\Mud_{\Gcal^u} \subseteq \MGar$ (Proposition~\ref{prop:CONinDAG}).
\end{proof}



Next, we define RCON models as in \cite{hojsgaard2008graphical}.
For this, a coloured undirected graph is a tuple $(\Gcal,c)$, where $\Gcal = (I, E)$ is an undirected graph and the map
	\[ c: I \cup E \rightarrow \Col \]
assigns a colour to each vertex and to each edge.

\begin{defn}[{see~\cite[\S3]{hojsgaard2008graphical}}]
	\label{defn:RCONmodel} %formerly known as def:rcon
	The {\em RCON model}\index{RCON model} $\Mud_{(\Gcal,c)}$ on the coloured undirected graph $(\Gcal, c)$ consists of concentration matrices $\Psi \in \PD_m(\KK)$ with
	\begin{itemize}
		\item[(i)] $\Psi_{ij} = \Psi_{ji} = 0$ whenever \begin{tikzcd}[cramped, sep=small]
			i \ar[r, no head] & j
		\end{tikzcd} is \emph{not} an edge in $\Gcal$
		\item[(ii)] $\Psi_{ii} = \Psi_{jj}$ whenever $c(i) = c(j)$,
		\item[(iii)] $\Psi_{ij} = \Psi_{kl}$ whenever $i<j$ and $k<l$ such that
		$c(\begin{tikzcd}[cramped, sep=small] i \ar[r, no head] & j	\end{tikzcd}) =
		c(\begin{tikzcd}[cramped, sep=small] k \ar[r, no head] & l	\end{tikzcd})$.\\
		Note that this implies $\Psi_{ji} = \overline{\Psi_{ij}} = \overline{\Psi_{kl}} = \Psi_{lk}$ since $\Psi\HT = \Psi$.
	\end{itemize}
	By part~(i), $\Mud_{(\Gcal,c)}$ is a sub-model of the model $\Mud_{\Gcal}$ from Example~\ref{ex:UndirectedGraphicalModelIntro}.
	\hfill\defnSymbol
\end{defn}

Let $(\Gcal, c)$ be a coloured DAG. Similarly to the construction of $\Gcal^u$, we obtain a coloured undirected graph $(\Gcal^u, c)$ by forgetting the edge directions in $\Gcal$. All vertex and edge colours are inherited. We call $\Mud_{(\Gcal^u, c)}$ the RCON model induced\index{RCON model!induced} by the RDAG model $\MGcar$.
Let us compare RDAG models and their induced RCON models in two examples.

\begin{example}[RDAG $=$ RCON, {\cite[Example~3.2]{RDAG}}] \label{ex:RCONequalsColoredTDAG}
	We revisit our running example  \begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red] \ar[l, red] & {\color{blue}\circled{2}}
	\end{tikzcd}. 
	The corresponding RCON model has coloured undirected graph \begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red, no head] \ar[l, red, no head] & {\color{blue}\circled{2}}
	\end{tikzcd}, with blue (circular) vertices $1$ and $2$, black (square) vertex $3$, and red edges. 
	By Definition~\ref{defn:RCONmodel}, the RCON model is the set of positive definite matrices of the form
	\begin{align*}
		\Psi = \begin{pmatrix} \delta_1 & 0 & \varrho \\ 0 & \delta_1 & \varrho \\ \overline{\varrho} & \overline{\varrho} & \delta_2 \end{pmatrix},
		\quad \text{where } \varrho \in \KK \text{ and } \delta_1, \delta_2 \in \RR_{>0}.
	\end{align*}
	Since the colouring is compatible, the RDAG model $\MGcar$ is equal to $\Mg_{\AGc}$ from Equation~\eqref{eq:RDAG1AGc}. 
	Any matrix in $\Mg_{\AGc}$ satisfies the equalities for the RCON model, so $\Mg_{\AGc} \subseteq \Mud_{(\Gcal^u, c)}$. Conversely, given positive-definite $\Psi \in \Mud_{(\Gcal^u, c)}$,
	\begin{align*}
		\det(\Psi) = \delta_1^2 \big( \delta_2 - 2 |\varrho|^2 \delta_1^{-1} \big) > 0 \quad \text{ and hence} \quad
		\delta_2 - 2 |\varrho|^2 \delta_1^{-1}  > 0.
	\end{align*}
	Setting $d_1 := \sqrt{\delta_1} \in \RR_{>0}$, $d_2 := \sqrt{\delta_2 - 2 |\varrho|^2 \delta_1^{-1}} \in \RR_{>0}$ and $r := \varrho / d_1 \in \KK$ shows that $\Psi$ is of the form in Equation~\eqref{eq:RDAG1AGc}, i.e., $\Psi \in \Mg_{\AGc}$.
	\hfill\exSymbol
\end{example}


\begin{example}[RDAG $\neq$ RCON, {\cite[Example~3.2]{RDAG}}]
	Consider the RDAG model on
	\begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & {\color{blue}\circled{2}} \ar[l, red]
	\end{tikzcd},
	the graph with two blue (circular) vertices and a red edge. The colouring is compatible, so by Proposition~\ref{prop:RDAGmodelEqualsMgAGc} the RDAG model is $\Mg_{\AGc}$, where
		\[ \AGc = \left\{ \begin{pmatrix} d & r \\ 0 & d \end{pmatrix} \, \bigg\vert \, d \in \KK^{\times} , \, r \in \KK\right\} . \]
	The induced RCON model is given by
	\begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & {\color{blue}\circled{2}} \ar[l, no head, red]
	\end{tikzcd}
	and consists of all $\Psi \in \PD_2(\KK)$ with $\Psi_{11} = \Psi_{22}$ and $\Psi_{12} = \Psi_{21}$, by Definition~\ref{defn:RCONmodel}.
	Neither model is contained in the other: the RCON model contains
	\begin{align*}
		\Psi' := \begin{pmatrix} 4 & 2 \\ 2 & 4 \end{pmatrix} = 
		\begin{pmatrix} 2 & 0 \\ 1 & \sqrt{3} \end{pmatrix} \begin{pmatrix} 2 & 1 \\ 0 & \sqrt{3} \end{pmatrix},
	\end{align*}
	but the diagonal entries $2$ and $\sqrt{3}$ in the Cholesky decomposition do not satisfy the condition $a_{11} = a_{22}$ for $a \in \AGc$. Therefore, $\Psi' \notin \Mg_{\AGc}$ by Lemma~\ref{lem:CholeskyMgAGc}. Conversely, the matrix
	\begin{align*}
		\Psi'' := \begin{pmatrix} 1 & 0 \\ 2 & 1 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ 0 & 1 \end{pmatrix}
		= \begin{pmatrix} 1 & 2 \\ 2 & 5 \end{pmatrix}
	\end{align*}
	is in the RDAG model, but not the RCON model, since $\Psi_{11}^{''} \neq \Psi_{22}^{''}$.
	\hfill\exSymbol
\end{example}

To characterize when an RDAG model is equal to its corresponding RCON model, we give two constructions of coloured graphs, one that is built from a vertex of a coloured DAG $(\Gcal,c)$ and the other from an edge.

Fix a vertex $i \in I$. Recall that $\ch(i)$ is the set of children of $i$.
Consider the subgraph on vertex set $\{ i \} \cup {\ch}(i)$ with edges $i \to k$ for each $k \in {\ch}(i)$, and colours inherited from $(\Gcal, c)$. We denote the coloured subgraph by $\Gcal_i$.

Now, fix an edge $(j \to i)$ in $\Gcal$. Consider the set $\{ i \} \cup \left( {\rm ch}(i)\cap{\rm ch}(j) \right)$ of vertices with vertex colours inherited from $(\Gcal,c)$. For each $k \in {\rm ch}(i)\cap{\rm ch}(j)$, we introduce two edges $i \to k$, one with colour $c(ki)$ and the other with colour $c(kj)$. We denote this coloured multi-digraph by $\Gcal_{(j \to i)}$.

%todo note that if i and j do not have common children, then 

\begin{example}[{\cite[Example~3.3]{RDAG}}]
	\label{ex:half_jumbled} 
	Consider the coloured DAG\footnote{\label{note1}with three vertex colours (blue/circular, black/square, and purple/triangular) and four edge colours (red/solid, green/squiggly, orange/dashed, and brown/dotted)}
	\begin{center}
		\begin{tikzcd}[column sep = small, row sep = small,decoration={snake,amplitude=0.8pt}]
			& & & {\color{Fuchsia}\triangled{5}} \ar[dl, orange, dashed] \ar[dll, OliveGreen, bend right = 7,decorate]\ar[dlll, red, bend right = 20] \ar[dd, Maroon,thick,dotted] \\
			\squared{1} & \squared{2} & \squared{3} & \\
			& & & {\color{blue}\circled{4}} \ar[ul, OliveGreen,decorate] \ar[ull, red, bend left = 10]\ar[ulll, orange, bend left = 20, dashed] & 
		\end{tikzcd}
	\end{center}
	The vertex construction at vertex $5$ and edge construction at edge
	$5 \to 4$ are:
	\begin{center} 
		$\Gcal_5 =$\begin{tikzcd}[column sep = small, row sep = small,decoration={snake,amplitude=0.8pt}]
			& & & {\color{Fuchsia}\triangled{5}} \ar[dl, orange,dashed] \ar[dll, OliveGreen, bend right = 10,decorate]\ar[dlll, red, bend right = 20] \ar[d, Maroon,dotted] \\
			\squared{1} & \squared{2} & \squared{3} & {\color{blue}\circled{4}} & 
		\end{tikzcd}
		\qquad
		$\Gcal_{(5 \to 4)} =$
		\begin{tikzcd}[column sep = scriptsize, row sep = small,decoration={snake,amplitude=0.8pt}]
			\squared{1} & \squared{2} & \squared{3} & {\color{blue}\circled{4}} \ar[l, orange, bend right = 20,dashed] \ar[ll, OliveGreen, bend right = 40,decorate] \ar[lll, red, bend right = 50] \ar[l, OliveGreen, bend left = 30,decorate] \ar[ll, red, bend left = 40]\ar[lll, orange, bend left = 50,dashed]
		\end{tikzcd}
	\end{center}
\hfill\exSymbol
\end{example} 

Two coloured graphs $(\Gcal,c)$ and $(\Gcal',c')$ are \emph{isomorphic} if the coloured graphs are the same up to relabelling vertices. We denote an isomorphism by $\Gcal \simeq \Gcal'$ when the colouring is clear. Now, we formulate the main theorem of this section.

\begin{theorem}[{\cite[Theorem~3.4]{RDAG}}] \label{thm:RCONequalsRDAG}
	Let $(\Gcal,c)$ be a coloured DAG where colouring $c$ is compatible. The 
	RDAG model $\MGcar$ on $(\Gcal,c)$
	is equal to the RCON model $\Mud_{(\Gcal^u,c)}$ on $(\Gcal^u,c)$ if and only if:
	\begin{itemize}
		\item[(a)] $\Gcal$ has no unshielded colliders; 
		\item[(b)] $\Gcal_i \simeq \Gcal_j$ for every pair of vertices $i,j$ of the same colour; and
		\item[(c)] $\Gcal_{(j \to i)} \simeq \Gcal_{(l \to k)}$ for every pair of edges $j \to i$ and $l \to k$ in $\Gcal$ of same colour.
	\end{itemize}
\end{theorem}

Before we prove the theorem, we illustrate it in  two examples.

\begin{example}[{\cite[Example~3.5]{RDAG}}]
	Our running example  \begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red] \ar[l, red] & {\color{blue}\circled{2}}
	\end{tikzcd}
	satisfies the conditions of Theorem~\ref{thm:RCONequalsRDAG}: it has no unshielded colliders and the graphs $\Gcal_1$ and $\Gcal_2$ both consist of a single blue vertex. Moreover, $\Gcal_{(3\to 1)}$ and $\Gcal_{(3 \to 2)}$ only consist of a blue vertex as $1$ and $3$ (respectively $2$ and $3$) do not have common children.
	The RDAG and RCON models are therefore equal, as we saw in Example~\ref{ex:RCONequalsColoredTDAG}.
	\hfill\exSymbol
\end{example}

\begin{example}[{\cite[Example~3.6]{RDAG}}]
	The coloured DAG $\Gc$ given by
	\begin{center}
		\begin{tikzcd}[column sep = small, row sep = small,decoration={snake,amplitude=0.8pt}]
			& & & {\color{Fuchsia}\triangled{9}} \ar[dl, orange,dashed] \ar[dll, OliveGreen, bend right = 10,decorate]\ar[dlll, red, bend right = 20] \ar[dd, Maroon,thick,dotted] & {\color{Fuchsia}\tiny\triangled{10}} \ar[dr, orange,dashed] \ar[drr, OliveGreen, bend left = 10,decorate]\ar[drrr, red, bend left = 20] \ar[dd, Maroon,thick,dotted] & & &\\
			\squared{1} & \squared{2} & \squared{3} & & & \squared{4} & \squared{5} & \squared{6} \\
			& & & {\color{blue}\circled{7}} \ar[ul, OliveGreen,decorate] \ar[ull, red, bend left = 10]\ar[ulll, orange, bend left = 20,dashed] & {\color{blue}\circled{8}} \ar[ur, red] \ar[urr, orange, bend right = 10,dashed]\ar[urrr, OliveGreen, bend right = 20,decorate] & & &
		\end{tikzcd}
	\end{center}
	 also satisfies the conditions of Theorem~\ref{thm:RCONequalsRDAG}:
	\begin{itemize}
		\item[(a)] It has no unshielded colliders.
		\item[(b)] For the black (square) vertices, the graphs $\Gcal_i$ consist of one black vertex. For the blue (circular) vertices, the $\Gcal_i$ are isomorphic to
		\begin{center}
			\begin{tikzcd}[column sep = small, row sep = small,decoration={snake,amplitude=0.8pt}]
				\squared{1} & \squared{2} & \squared{3} & {\color{blue}\circled{4}} \ar[l, orange, bend right = 20,dashed] \ar[ll, OliveGreen, bend right = 30,decorate] \ar[lll, red, bend right = 40]
			\end{tikzcd}
		\end{center}
		The purple (triangular) vertices have $\Gcal_i$ isomorphic to $\Gcal_5$ from Example~\ref{ex:half_jumbled}.
		\item[(c)] All edges $j \to i$ have $\ch(j) \cap \ch(i) = \emptyset$, except for the two brown edges. For these, $\Gcal_{(10 \to 8)}$ and $\Gcal_{(9 \to 7)}$ are both isomorphic to $\Gcal_{(5 \to 4)}$ from Example~\ref{ex:half_jumbled}.
	\end{itemize} 
	Hence, the RDAG model $\MGcar$ equals the induced RCON model $\Mud_{(\Gcal^u, c)}$.
	Note that the two connected components of $\Gc$ are not isomorphic as coloured directed graphs. We will see why this is not required for the proof of Theorem~\ref{thm:RCONequalsRDAG}, i.e., why we can collapse vertices $i$ and $j$ in the definition of $\Gcal_{(j \to i)}$.
	\hfill\exSymbol
\end{example}

Finally, we prove Theorem~\ref{thm:RCONequalsRDAG} in a similar way as Theorem~\ref{thm:DAGCONeq}.

\begin{prop}[{\cite[Proposition~3.8]{RDAG}}] \label{prop:RDAGinRCON}
	Let $(\Gcal,c)$ be a coloured DAG with compatible colouring $c$. Then $\MGcar \subseteq \Mud_{(\Gcal^u, c)}$ if and only if conditions~(a), (b) and (c) of Theorem~\ref{thm:RCONequalsRDAG} hold.
%	\begin{itemize}
%		\item[(a)] $\Gcal$ has no unshielded colliders; 
%		\item[(b)] $\Gcal_i \simeq \Gcal_j$ for every pair of vertices $i,j$ of the same colour; and
%		\item[(c)] $\Gcal_{(j \to i)} \simeq \Gcal_{(l \to k)}$ for every pair of edges $j \to i$ and $l \to k$ of the same colour. \end{itemize} 
\end{prop}


\begin{proof} %todo use Peters suggestion: split def'n of \AGc into three conditions I, II and III (I is support condition, II is vertex colour condition, III is edge colour condition)
	We have $\MGcar = \Mg_{\AGc}$ since the colouring is compatible, see Proposition~\ref{prop:RDAGmodelEqualsMgAGc}. Let $a \in \AGc$. If we speak of a \emph{general} $a$, then we think of it having indeterminate entries, one for each vertex colour and one for each edge colour.
	
	If $\MGcar \subseteq \Mud_{(\Gcal^u, c)}$, then we must have $(a\HT a)_{ij} = 0$ whenever $a_{ij} = a_{ji} = 0$.  This holds if and only if there are no unshielded colliders in $\Gcal$, by Proposition~\ref{prop:DAGinCON}. Moreover, certain equalities must hold on $a\HT a$. We have vertex colour condition $(a\HT a)_{ii} = (a\HT a)_{jj}$ whenever $c(i) = c(j)$ and edge colour condition $(a\HT a)_{ij} = (a\HT a)_{kl}$ whenever $i<j$, $k<l$ and $c(ij) = c(kl)$. These give the identities
	\begin{align}
		\label{eqn:vertex_condition} |a_{ii}|^2 + \sum_{k \in {\rm ch}(i)} |a_{ki}|^2 & = |a_{jj}|^2 + \sum_{l \in {\rm ch}(j)} |a_{lj}|^2  & &\text{whenever} \quad c(i) = c(j) \\
		\label{eqn:edge_condition} \overline{a_{ii}} a_{ij} + \sum_{p \neq i,j}^m \overline{a_{pi}} a_{pj} & =  \overline{a_{kk}} a_{kl} + \sum_{q \neq k,l}^m \overline{a_{qk}} a_{ql} & &\text{whenever} \quad c(ij) \! = \! c(kl). 
	\end{align} 
	We show that~\eqref{eqn:vertex_condition} is equivalent to (b) and that~\eqref{eqn:edge_condition} is equivalent to (c).
	
	Given vertices $i$ and $j$ with $c(i) = c(j)$, we have $|a_{ii}|^2 = |a_{jj}|^2$ as $a \in \AGc$.
	If $\Gcal_i \simeq \Gcal_j$ then \eqref{eqn:vertex_condition} holds for all $a \in \AGc$, by definition of $\Gcal_i$ and $\Gcal_j$. Conversely, assume \eqref{eqn:vertex_condition} holds for all $a \in \AGc$. The equation over $\KK = \CC$ implies the equation over $\KK = \RR$, so it suffices to assume the latter. Over $\RR$, \eqref{eqn:vertex_condition} is a polynomial identity that is assumed to hold for all $a \in \AGc$.
	But the sums in~\eqref{eqn:vertex_condition} are equal for general $a \in \AGc$ only if $\vert \ch(i) \vert = \vert \ch(j) \vert$ and the edge colours in $\Gcal_i$ and $\Gcal_j$ agree (counted with multiplicity). By compatibility, the corresponding child vertex colours in $\Gcal_i$ and $\Gcal_j$ also agree, hence we have $\Gcal_i \simeq \Gcal_j$.
	
	Next, let $j \to i$ and $l \to k$ be edges in $\Gcal$ of same colour. In particular, $i<j$ and $k<l$. The compatibility of the colouring gives $a_{ii} = a_{kk}$, hence $a_{ii} a_{ij} = a_{kk} a_{kl}$.
	Moreover, in Equation~\eqref{eqn:edge_condition} no terms $a_{ji} a_{jj}$ and $a_{lk} a_{ll}$ appear, since $i \not\to j$ and $k \not\to l$ in $\Gcal$ by acyclicity. In particular, it does not matter whether $c(j) = c(l)$ holds or not.
	
	Now, if $\Gcal_{(j \to i)} \simeq \Gcal_{(l \to k)}$, then \eqref{eqn:edge_condition} holds for all $a \in \AGc$, by definition of $\Gcal_{(j \to i)}$ and $\Gcal_{(l \to k)}$.
	Conversely, assume \eqref{eqn:edge_condition} holds for all $a \in \AGc$. Again, it suffices to assume $\KK=\RR$. Then \eqref{eqn:edge_condition} is a polynomial identity.
	A summand in~\eqref{eqn:edge_condition} vanishes unless $p \in \ch(i) \cap \ch(j)$ or $q \in \ch(k) \cap \ch(l)$. The sums are equal for general $a \in \AGc$ only if $\vert \ch(i) \cap \ch(j) \vert = \vert \ch(k) \cap \ch(l) \vert$ and the graphs $\Gcal_{(j \to i)}$ and $\Gcal_{(l \to k)}$ are isomorphic on their edge colours. By compatibility, the corresponding child vertex colours must also agree and hence $\Gcal_{(j \to i)} \simeq \Gcal_{(l \to k)}$.
\end{proof}


\begin{prop}[{\cite[Proposition~3.9]{RDAG}}] \label{prop:RCONinRDAG}
	Let $\Gc$ be a coloured DAG with compatible colouring $c$ such that conditions~(a), (b) and (c) of Theorem~\ref{thm:RCONequalsRDAG} hold. Then $\Mud_{(\Gcal^u, c)} \subseteq \MGcar$.
%	It holds that $\Mud_{(\Gcal^u, c)} \subseteq \MGcar$, if
%	$(\Gcal,c)$ is a coloured DAG with compatible colouring $c$ such that
%	\begin{itemize}
%		\item[(a)] $\Gcal$ has no unshielded colliders; 
%		\item[(b)] $\Gcal_i \simeq \Gcal_j$ for every pair of vertices $i,j$ of the same colour; and
%		\item[(c)] $\Gcal_{(j \to i)} \simeq \Gcal_{(l \to k)}$ for every pair of edges $j \to i$ and $l \to k$ of the same colour.
%	\end{itemize}
\end{prop}

\begin{proof} 
	We have $\MGcar = \Mg_{\AGc}$ as colouring $c$ is compatible, see Proposition~\ref{prop:RDAGmodelEqualsMgAGc}.
	Given some $\Psi \in \Mud_{(\Gcal^u,c)}$, we show its Cholesky decomposition $\Psi = a\HT a$ satisfies $a \in \AGc$.
	Since $\Gcal$ has no unshielded colliders, the Cholesky decomposition satisfies $a \in \AG$ by Proposition~\ref{prop:CONinDAG}. In the proof of Proposition~\ref{prop:CONinDAG} we constructed %todo adjust formulation
	$a$ inductively: for any vertex $l$ and any edge $i \leftarrow j$ we have
	\begin{align} %todo move this to uncoloured case, and use it alreagy there? (Peters suggestion)
		a_{l,l} &= \Big( \Psi_{l,l} \, - \sum_{p \in {\ch}(l)} |a_{p,l}|^2 \Big)^{1/2} \label{eq:RCONinRDAGvertex} \\
		a_{i,j} &= \Big( \Psi_{i,j} \, - \sum_{p \in {\ch}(i) \cap {\ch}(j)} \overline{a_{p,i}} a_{p,j} \Big) a_{i,i}^{-1}. \label{eq:RCONinRDAGedge}
	\end{align}
	
	We show that $a$ satisfies the symmetries of the colouring.
	We prove this inductively over the top left $k \times k$ blocks of $a$.
	If $k=1$ there are no symmetries to check.
	We assume that the top left $k \times k$ submatrix of $a$ satisfies the symmetries. For the induction step, we compare $a_{1,k+1},a_{2,k+1},\ldots,a_{k+1,k+1}$ with each other and with $a_{i,j}$, where $i,j \in [k]$.
	
	If there is an edge $(k+1) \to 1$ with same colour as $j \to i$ for $i,j \in [k]$, we
	need to show that $a_{1,k+1} = a_{i,j}$.
	First, $a_{11} = a_{ii}$ by compatibility. Second, $\Psi_{i,j} = \Psi_{1,k+1}$ since $i<j$, $1 < k+1$ and $\Psi \in \Mud_{(\Gcal^u, c)}$, compare Definition~\ref{defn:RDAGmodelViaLDL}(iii). Third, all $a_{p,q}$ for $p,q \in [k]$ respect the symmetries by induction hypothesis. Since $\Gcal_{(j \to i)} \simeq \Gcal_{(k+1 \to 1)}$, the expressions~\eqref{eq:RCONinRDAGedge} for $a_{i,j}$ and $a_{1,k+1}$ are equal. %todo more explanation? (Peters suggestion)
	
	Proceeding inductively, we show analogously that all entries $a_{2,k+1},\ldots,a_{k,k+1}$ respect the symmetries of colouring~$c$. Indeed, for $a_{i',k+1}$ with $i' \in \{2,\ldots,k\}$ the above argument still applies, even if we need to compare to $a_{i,k+1}$ where $i < i'$. This is due to the fact that \eqref{eq:RCONinRDAGedge} for $a_{i',k+1}$ and for $a_{i,k+1}$ only involves entries of~$a$, which have already been proven to respect the symmetries among each other, namely, $a_{p,q}$ with $p,q \in [k]$ and $a_{1,k+1}, \ldots, a_{i'-1,k+1}$.
	
	Finally, if vertex $k+1$ has same colour as vertex $l \in [k]$, we show $a_{k+1,k+1} = a_{l,l}$. We have $\Gcal_l \simeq \Gcal_{k+1}$ by assumption~(b) and $\Psi_{l,l} = \Psi_{k+1,k+1}$, since $\Psi$ is in the RCON model. Furthermore, we have shown that all $a_{p,q}$, where $p \in [k]$ and $q \in [k+1]$, obey colouring $c$. Altogether, we conclude $a_{l,l} = a_{k+1,k+1}$ using \eqref{eq:RCONinRDAGvertex}.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{thm:RCONequalsRDAG}]
	If any of conditions (a), (b), and (c) do not hold, then $\MGcar \nsubseteq \Mud_{(\Gcal^u, c)}$, by Proposition~\ref{prop:RDAGinRCON}, and hence the models cannot be equal. If conditions (a), (b) and (c) hold, we have $\MGcar \subseteq \Mud_{(\Gcal^u, c)}$ (by Proposition~\ref{prop:RDAGinRCON}) and $\Mud_{(\Gcal^u, c)} \subseteq \MGcar$ (by Proposition~\ref{prop:RCONinRDAG}).
\end{proof}


\section{MLE: existence, uniqueness and an algorithm} \label{sec:MLE-RDAG}


In this section we characterize existence and uniqueness of MLEs in an RDAG model via linear dependence conditions on certain augmented sample matrices, see Theorem~\ref{thm:RDAGMLestimationLinDependence}. This generalizes the characterization of ML estimation in usual DAG models from Theorem~\ref{thm:LinearIndependenceDAG}. Furthermore, the proof of Theorem~\ref{thm:RDAGMLestimationLinDependence} directly gives an algorithm to compute an MLE, if existent, in an RDAG model. Finally we present illustrative examples. %todo illustrative examples included, or skipped?

\medskip

First, we define the augmented sample matrices given a coloured DAG $\Gc$ and sample matrix $Y \in \KK^{m \times n}$.
Let $\alpha_s$ be the number of vertices of colour $s \in c(I)$. 
Recall the set of \emph{parent relationship colours} of vertex colour $s$ from Equation~\eqref{eq:defnPRCs}:
\begin{align*}
	\prc(s) = \lbrace c(ij) \mid \text{there exists } j \to i \text{ in } \mathcal{G} \text{ with } c(i) = s \rbrace, \qquad \beta_s := \vert \prc(s) \vert.
\end{align*}

\begin{defn}[{\cite[Definition~4.1]{RDAG}}] \label{defn:MYs} %formerly known as def:MYs
	The {\em augmented sample matrix} of sample matrix $Y \in \KK^{m \times n}$ and vertex colour $s$, denoted~$M_{Y,s}$, has size $(\beta_s +1) \times \alpha_s n$. We construct it row by row: let $M_{Y,s}^{(i)}$ denote the $i^{th}$ row of $M_{Y,s}$, where we index from $0$ to $\beta_s$.
	Each row consists of $\alpha_s$ blocks, each a row vector of length $n$.
	Let $i_1 < i_2 < \ldots < i_{\alpha_s}$ be the vertices of colour $s$.
	Then the top row of $M_{Y,s}$ is 
		\[ M^{(0)}_{Y,s} := \begin{pmatrix} Y^{(i_1)} & Y^{(i_2)} & \ldots & Y^{(i_{\alpha_s})} \end{pmatrix} \in \KK^{1 \times (\alpha_s n)},\]
	where $Y^{(i)}$ is the $i^{th}$ row of sample matrix $Y$.
	The other rows of $M_{Y,s}$ are indexed by the parent relationship colours $t \in \prc(s)$:
		%\[ M^{(t)}_{Y,s} := \left( \sum_{\substack{1 \leftarrow j \\ c(1j) = t}} Y^{(j)} \quad \sum_{\substack{2 \leftarrow j \\ c(2j) = t}} Y^{(j)}  \quad \cdots \quad \sum_{\substack{\alpha_s \leftarrow j \\ c(\alpha_s j) = t}} Y^{(j)}  \right) .\]
		\[ M^{(t)}_{Y,s} := \left( \sum_{\substack{i_1 \leftarrow j \\ c(i_1 j) = t}} Y^{(j)} \quad \sum_{\substack{i_2 \leftarrow j \\ c(i_2 j) = t}} Y^{(j)}  \quad \cdots \quad \sum_{\substack{i_{\alpha_s} \leftarrow j \\ c(i_{\alpha_s} j) = t}} Y^{(j)}  \right) .\]
	For $k \in [\alpha_s]$, the sum at the $k^{th}$ block of $M^{(t)}_{Y,s}$ is zero if there are no $j \to i_k$ in $\Gcal$ of colour $t$.
	Note that we frequently use the following abuse of notation: $t$ is viewed as an edge colour like in $c(i_1 j) = t$, but also as its corresponding number $t \in [\beta_s]$ like in $M^{(t)}_{Y,s}$.
	\hfill\defnSymbol
\end{defn}


\begin{example}[{\cite[Example~4.2]{RDAG}}]
	For running example \begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red] \ar[l, red] & {\color{blue}\circled{2}}
	\end{tikzcd},
	\begin{equation}\label{eqn:running_ex_MYs} M_{Y, {\color{blue}\circ} } = \begin{pmatrix} Y^{(1)} & Y^{(2)} \\ Y^{(3)} & Y^{(3)} \end{pmatrix} 
		\begin{matrix} {\color{blue}\circ} \\ {\color{red}\to}  \end{matrix} \in \KK^{2 \times 2n} 
		\qquad \text{ and } \qquad
		M_{Y, {\square} } = \begin{pmatrix} Y^{(3)}  \end{pmatrix} \in \KK^{1 \times n} 
	\end{equation}
	are the two augmented sample matrices, one for each vertex colour.
	\hfill\exSymbol
\end{example}

\begin{example}[{\cite[Example~4.3]{RDAG}}] \label{ex:augmentedSampleMatrix}
	The coloured DAG
	\begin{center}
		{\small 
		\begin{tikzcd}[column sep = small,decoration={snake,amplitude=1pt}]
			& & {\color{blue}\circled{1}} & &\\
			\squared{3} \ar[rru, red, bend left = 30] \ar[rrd, orange, dashed, bend right = 30] & \squared{4} \ar[ru, Maroon, dotted] \ar[rd, OliveGreen, decorate] & \squared{5} \ar[u, OliveGreen, decorate] \ar[d, orange, dashed] & \squared{6} \ar[lu, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] \ar[ld, orange, dashed] & \squared{7} \ar[llu, Maroon, dotted, bend right = 30] \\
			& & {\color{blue}\circled{2}} & &
		\end{tikzcd}
		\, has \,
		$M_{Y,{\color{blue}\circ}} =         \begin{pmatrix}
			Y^{(1)} & Y^{(2)} \\ Y^{(3)} & 0 \\ 0 & Y^{(3)} + Y^{(5)} + Y^{(6)} \\ Y^{(5)} & Y^{(4)} \\ Y^{(6)} & 0 \\ Y^{(4)} + Y^{(7)} & 0
		\end{pmatrix}
		\begin{matrix}
			%comment: "\ " is a protected blank space, this avoids an error by tikzcd (arrows need an endpoint)
			{\color{blue}\circ} \\ \begin{tikzcd}[cramped, sep = small] \ar[r, red] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, orange, dashed] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, OliveGreen, decorate, decoration={snake,amplitude=1.2pt}] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, Maroon, dotted] & \  \end{tikzcd}
		\end{matrix}$
	}
	\end{center}
	as augmented sample matrix for vertex colour blue.
	\hfill\exSymbol
\end{example}

The following two remarks are implicitly contained in \cite{RDAG}.

\begin{remark}[$M_{Y,s}$ recovers $Y^{(i) \cup \pa(i)}$ for usual DAG models] \label{rem:MYsDAGmodel}
	In Remark~\ref{rem:DAGmodelViaCompatible} we have seen that any DAG model $\MGar$ is an RDAG model $\MGcar$, where colouring $c$ assigns each vertex and each edge its own distinct colour. Thus, setting $s := c(i)$ for vertex $i \in [m]$, we have $\alpha_s = 1$ and $M_{Y,s}^{(0)} = Y^{(i)}$. Moreover, as each edge has its own colour, any parent of $i$ can be uniquely identified with its parent relationship colour. Therefore, $ |\pa(i)| = \beta_s := |\prc(s)|$ and $M_{Y,s}^{(t)} = Y^{(j)}$, where $j \to i$ in $\Gcal$ and $c(ij) = t$.
	Altogether, $M_{Y,s} = Y^{(i) \cup \pa(i)}$ for a vertex $i$ of $\Gcal$.
	\hfill\remSymbol
\end{remark}

\begin{remark}\label{rem:MYsAndActionOfAGc}
	Let $\Gc$ be a coloured DAG with compatible colouring. Left-multiplication of $a \in \AG$ on $Y \in \KK^{m \times n}$ is given by
		\[ (a \cdot Y)^{(i)} = a_{{ii}} Y^{(i)} + \sum_{j \in \pa(i)} a_{ij} Y^{(j)} \]
	for all vertices $i \in [m]$. The augmented sample matrices are constructed such that the latter generalizes to $\AGc$. Let $a \in \AGc$ with vertex colour entries $a_{ss} \in \KK^{\times}$ and edge colour entries $a_{st} \in \KK$, where $s \in c(I)$ and $t \in \prc(s)$, compare Lemma~\ref{lem:PropertiesCompatibleColouring}(ii). Then $a \cdot Y$ is determined by
		\begin{equation}\label{eq:MYsLeftMultiplication}
			M^{(0)}_{a \cdot Y, s} = a_{ss} M^{(0)}_{Y,s} + \sum_{t \in \prc(s)} a_{st} M^{(t)}_{Y,s}
		\end{equation}
	for all vertex colours $s \in c(I)$.
	\hfill\remSymbol
\end{remark}

Now, we formulate the main theorem of this section. By Remark~\ref{rem:MYsDAGmodel}, it generalizes Theorem~\ref{thm:LinearIndependenceDAG} for DAG models to RDAG models.

\begin{theorem}[{\cite[Theorem~4.4]{RDAG}}] \label{thm:RDAGMLestimationLinDependence}
	Consider the RDAG model $\MGcar$ on $\Gc$ where colouring $c$ is compatible, 
	and fix sample matrix $Y \in \KK^{m \times n}$.
	The following possibilities characterize maximum likelihood estimation given $Y$:
	\[ \begin{matrix} \text{(a)} & \ell_Y \text{ unbounded from above} & \Leftrightarrow & \exists \, s \in c(I) \colon & M_{Y,s}^{(0)} \in \Span \big\lbrace M_{Y,s}^{(t)} : t \in [\beta_s] \big\rbrace \\[3pt]
		\text{(b)} & \text{MLE exists} & \Leftrightarrow & \forall \, s \in c(I) \colon &  M_{Y,s}^{(0)} \notin \Span  \big\lbrace M_{Y,s}^{(t)} : t \in [\beta_s] \big\rbrace \\[3pt]
		\text{(c)} & \text{MLE exists uniquely} &  \Leftrightarrow &  \forall \, s \in c(I) \colon &  M_{Y,s} \text{ has full row rank} . \\ \end{matrix} \]
\end{theorem}

\begin{example}[{\cite[Example~4.5]{RDAG}}]
	\label{ex:running5}
	For running example \begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red] \ar[l, red] & {\color{blue}\circled{2}}
	\end{tikzcd}, Theorem~\ref{thm:RDAGMLestimationLinDependence} says that the MLE exists uniquely if $Y^{(3)} \neq 0$ and $\begin{pmatrix} Y^{(1)} & Y^{(2)} \end{pmatrix}$ is not parallel to $\begin{pmatrix} Y^{(3)} & Y^{(3)} \end{pmatrix}$.
	This holds almost surely as soon as we have one sample, i.e., here $\mlt_u =1$, as we mentioned in Example~\ref{ex:very_first}.
	\hfill\exSymbol
\end{example}

\begin{example}[{\cite[Example~4.6]{RDAG}}]
	Returning to Example~\ref{ex:augmentedSampleMatrix}, the MLE given $Y$ exists provided $M_{Y, {\square} } = \begin{pmatrix} Y^{(3)} & \cdots & Y^{(7)} \end{pmatrix} \neq 0$, and $\begin{pmatrix} Y^{(1)} & Y^{(2)} \end{pmatrix}$ is not in the linear hull of the other rows of $M_{Y,{\color{blue}\circ}}$. The MLE is unique if and only if $M_{Y,{\color{blue}\circ}}$ is full row rank, since this also implies $M_{Y, {\square} } \neq 0$.
	\hfill\exSymbol
\end{example}

The proof of Theorem~\ref{thm:RDAGMLestimationLinDependence} is analogous to the proof for uncoloured models in Theorem~\ref{thm:LinearIndependenceDAG}. In particular, we use again Lemma~\ref{lem:MinimumOfMinusLogLikelihoodRDAG}. The following proof also gives Algorithm~\ref{algo:RDAG-MLE} for computing an MLE, and a description of all MLEs, see Corollary~\ref{cor:RDAG-MLEs}.

\begin{proof}[Proof of Theorem~\ref{thm:RDAGMLestimationLinDependence}]
	By Proposition~\ref{prop:RDAGmodelEqualsMgAGc}, we have $\MGcar = \Mg_{\AGc}$ as colouring $c$ is compatible. In particular, for $\Psi = (\Id_m - \Lambda)\HT \Omega^{-1} (\Id_m - \Lambda) \in \MGcar$, the matrix $a = \Omega^{-1/2} (\Id_m - \Lambda)$ giving the Cholesky decomposition $\Psi = a\HT a$ is in $\Mg_{\AGc}$, compare Lemma~\ref{lem:CholeskyMgAGc}. As usual, let $\alpha_s := \vert c^{-1}(s) \vert$ and $\beta_s := \vert \prc(s) \vert$.
	By Lemma~\ref{lem:PropertiesCompatibleColouring}(ii), we can write the entries of the matrices $a$, $\Omega$ and $\Lambda$ as $a_{ss}$ and $a_{st}$, $\omega_{ss}$ and $\lambda_{st}$, where $s \in c(I)$ and $t \in [\beta_s]$. Using Equation~\eqref{eq:MYsLeftMultiplication} with $a_{ss} = \omega_{ss}^{-1/2}$ and $a_{st} = - \omega_{ss}^{-1/2} \lambda_{st}$, and that $\det(\Id_m -\Lambda) = 1$, we compute
	\begin{align*}
		%\begin{split} 
			%\label{eqn:finding_MLE} 
			- \ell_Y(\Psi) &= - \log \det(\Psi) + \tr(\Psi S_Y)
			\overset{\eqref{eq:NormTrace}}{=} \log \det(\Omega) + \frac{1}{n} \|a \cdot Y\|^2 \\
			&= \log \bigg(\prod_{s \in c(I)} \omega_{ss}^{\alpha_s} \bigg) + \frac{1}{n} \sum_{s \in c(I)} \Big\| \omega_{ss}^{-1/2} \Big( M_{Y,s}^{(0)} - \sum_{t \in [\beta_s]} \lambda_{s,t} M_{Y,s}^{(t)} \Big) \Big\|^2 \\
			&= \sum_{s \in c(I)} \alpha_s \log(\omega_{ss}) + \frac{1}{n \omega_{ss}} \Big\| M_{Y,s}^{(0)} - \sum_{t \in [\beta_s]} \lambda_{s,t} M_{Y,s}^{(t)} \Big\|^2.
		%\end{split}
	\end{align*}
	An MLE is a minimizer of the above expression. Each parameter occurs in exactly one of the summands over $s \in c(I)$, because the $\prc(s)$ partition the edge colours by compatibility, see Lemma~\ref{lem:PropertiesCompatibleColouring}(i). We therefore minimize each summand separately, so fix $s \in c(I)$. We can first determine $\hat{\lambda}_{s,t}$, $t \in [\beta_s]$ that minimize
	\begin{equation}\label{eq:RDAGminUnipotentPart}
		\Big\| M_{Y,s}^{(0)} - \sum_{t \in [\beta_s]} \lambda_{s,t} M_{Y,s}^{(t)} \Big\|^2 ,
	\end{equation}
	by Lemma~\ref{lem:MinimumOfMinusLogLikelihoodRDAG}(iii).
	Such $\hat{\lambda}_{s,t}$ always exist: they are coefficients in the orthogonal projection $P_{Y,s}$ of $M_{Y,s}^{(0)}$ onto $\mathrm{span} \big\lbrace M_{Y,s}^{(t)} : t \in [\beta_s] \big\rbrace$, i.e.,
	\[ P_{Y,s} = \sum_{t \in [\beta_s]} \hat{\lambda}_{s,t} M_{Y,s}^{(t)}. \]
	Furthermore, $\hat{\lambda}_{s,t}$, $t \in [\beta_s]$ are unique if and only if the vectors $M_{Y,s}^{(t)}$, $t \in [\beta_s]$ are linearly independent. Denote the minimum value of \eqref{eq:RDAGminUnipotentPart} by $\zeta_s$. We will apply Lemma~\ref{lem:MinimumOfMinusLogLikelihoodRDAG} several times with $\gamma_s := \zeta_s/n$.
	
	If $M_{Y,s}^{(0)} \in \mathrm{span} \big\lbrace M_{Y,s}^{(t)} : t \in [\beta_s] \big\rbrace$ for some $s \in c(i)$, then $\zeta_s = 0$ and the summand $\alpha_s \log(\omega_{ss}) + \zeta_s/ (n \omega_{ss})$ is not bounded from below for $\omega_{ss} > 0$, by Lemma~\ref{lem:MinimumOfMinusLogLikelihoodRDAG}(i). Hence, setting $\omega_{s',s'} = 1$ and $\lambda_{s',t'} = 0$ for all $s' \in c(I)\setminus \{s\}$ and all $t' \in [\beta_{s'}]$ shows that $\ell_Y$ is not bounded from above. This proves ``$\Leftarrow$'' of (a).
	
	If $M_{Y,s}^{(0)} \notin \mathrm{span} \big\lbrace M_{Y,s}^{(t)} : t \in [\beta_s] \big\rbrace$, equivalently $\zeta_s > 0$, then the summand $\alpha_s \log(\omega_{ss}) + \zeta_s/ (n\omega_{ss})$ has unique minimiser $\hat{\omega}_{ss} = \zeta_s /(n \alpha_s)$, by Lemma~\ref{lem:MinimumOfMinusLogLikelihoodRDAG}(ii). Hence, an MLE exists if $\zeta_s > 0$ for all $s \in c(I)$, which proves ``$\Leftarrow$'' in (b). As the right-hand sides of (a) and (b) are opposites and since MLE existence implies $\ell_{Y}$ is bounded from above, we have proved (a) and (b).
	
	Since the $\hat{\omega}_{ss}$ are uniquely determined (if they exist), an MLE is unique if and only if all $\hat{\lambda}_{s,t}$ are unique. The latter is equivalent to: for all $s \in c(I)$ the vectors $M_{Y,s}^{(t)}$, $t \in [\beta_s]$ are linearly independent. In combination with the condition for MLE existence from (b) we deduce (c).
\end{proof}

The above proof of Theorem~\ref{thm:RDAGMLestimationLinDependence} gives Algorithm~\ref{algo:RDAG-MLE} and its correctness for finding a MLE in an RDAG model with compatible colouring. The MLE is given in a closed-form formula, as a collection of least squares estimators. It is returned in terms of the matrices $\Lambda$ and $\Omega$.

\begin{algorithm}[h]
	\label{algo:RDAG-MLE} 
	\SetAlgoLined
	\Input{A coloured DAG $\Gc$ with compatible colouring $c$,\\a sample matrix $Y \in \KK^{m \times n}$.}
	\Output{An MLE given $Y$ in the RDAG model $\MGcar$, if one exists.\\Otherwise, returns ``MLE does not exist''.}
	\BlankLine
	\For{$s \in c(I)$}{
		$\alpha_s:= |c^{-1}(s)|$\; 
		$\beta_s:= |\prc(s)|$\;
		construct matrix $M_{Y,s} \in \KK^{(\beta_s + 1) \times \alpha_s n}$\; 
		$P_{Y,s}:=$ orthogonal projection of $M_{Y,s}^{(0)}$ onto $\Span \big\{ M_{Y,s}^{(t)} : t \in [\beta_s] \big\}$\;
		\eIf{$P_{Y,s} = M_{Y,s}^{(0)}$}{
			\Return{ MLE does not exist}\;
		}{
			coefficients $\lambda_{s,t}$ are such that $P_{Y,s} = \sum_{t \in \prc(s)} \lambda_{s,t} M_{Y,s}^{(t)}$\;
			$\omega_{s,s} := (\alpha_s n)^{-1} \big\| P_{Y,s} - M_{Y,s}^{(0)} \big\|^2$\;
		}
	}
	\Return{MLE for $\Lambda$ and $\Omega$}
	\caption{ {\cite[Algorithm~1]{RDAG}}\\MLE computation for an RDAG model with compatible colouring}
\end{algorithm}

The proof of Theorem~\ref{thm:RDAGMLestimationLinDependence} also gives a description of the set of MLEs.

\begin{cor}[{\cite[Corollary~4.8]{RDAG}}] \label{cor:RDAG-MLEs} %formerly cor:the_MLEs
	Consider the RDAG model on $\Gc$ where colouring $c$ is compatible, 
	with sample matrix $Y \in \KK^{m \times n}$. If $(\Lambda, \Omega)$ and $(\Lambda',\Omega')$ are two MLEs, then $\Omega = \Omega'$ and 
		\[ \forall \, s \in c(i) \colon \quad \sum_{t \in \prc(s)} (\lambda_{s,t} - \lambda'_{s,t}) M_{Y,s}^{(t)} = 0 . \]
\end{cor}

%Illustrative Examples
We end this section with two illustrative examples of RDAG models and the theory presented herein. First, we apply our running example to model the effect of a mother's height on her two daughters' heights.

\begin{example}[{\cite[Example~4.12]{RDAG}}] \label{ex:heights}
	Let $\KK = \RR$. The RDAG model on the coloured DAG \begin{tikzcd}[cramped, sep = small]
		{\color{blue}\circled{1}} & \squared{3} \ar[r, red] \ar[l, red] & {\color{blue}\circled{2}}
	\end{tikzcd} is parametrized by $\lambda \in \RR$, $\omega, \omega' \in \RR_{>0}$ and given by the linear structural equations
		\[ y_1 = \lambda y_3 + \veps_1 , \quad y_2 = \lambda y_3 + \veps_2, \quad y_3 = \veps_3, \quad \text{where } \, \veps_1, \veps_2 \sim \Ncal(0,\omega), \veps_3 \sim \Ncal(0,\omega'). \]
	Let	variable $y_3$ be the height (in cm) of a woman and let variables $y_1$ and $y_2$ be, respectively, the heights of her younger and older daughter. Vertices $1$ and $2$ both being blue indicates that, conditional on the mother's height, the variance of the daughter's heights is the same. Both edges being red encodes that the dependence of a daughter's height on the mother's height is the same for both daughters.
	
	We saw in Example~\ref{ex:running5} that the MLE exists almost surely given one sample.
	We use Algorithm~\ref{algo:RDAG-MLE} to find the MLE,
	given one sample where the the younger daughter's height is 159.75cm, the older daughter's height is 161.56, the mother's height is 155.32, and the population mean height is 163.83cm.\footnote{We point out the difference to Remark~{rem:MeanZeroVsGeneralMean}.
	The latter discusses that the ML threshold increases by one, if the mean is \emph{unknown} and also part of an MLE. However, here we assume the population mean to be known and use it . Hence, we only need to find the concentration matrix.}
	Mean-centring the data gives \[ \begin{pmatrix} Y^{(1)} & Y^{(2)} & Y^{(3)} \end{pmatrix} = \begin{pmatrix} 
		-4.08 & -2.27 & -8.51 \end{pmatrix} 
	.\]
	The only black vertex is $3$, and it has no parents, hence $\omega' = \| Y^{(3)} \|^2 = 72.42$. 
	The orthogonal projection of  $\begin{pmatrix} Y^{(1)} & Y^{(2)} \end{pmatrix}$ onto the line spanned by $ \begin{pmatrix} Y^{(3)} & Y^{(3)} \end{pmatrix}$ has coefficient $\lambda = 0.37$ and residual $\omega = \big[ (-3.175+4.08)^2 + (-3.175+2.27)^2 \big]/2 = 0.82$.
	As we would expect, the regression coefficient $\lambda$ is positive and the variance of the daughters' heights conditional on the mother's height is lower than the variance of the mother's height.
	\hfill\exSymbol
\end{example}

Now, we consider multiple measurements taken in each generation.

\begin{example}[{\cite[Example~4.13]{RDAG}}] \label{ex:dog}
	We consider measurements of the snout length and head length of dogs.
	These are the first two of the 
	seven morphometric parameters in the study of clinical measurements of dog breeds
	in~\cite{momozawa2020genome}. We compare two RDAG models:
	\begin{center}
		\begin{tikzcd}[column sep = small,decoration={snake,amplitude=1pt}]
			\squared{1} & {\color{Fuchsia}\triangled{5}}\ar[r,red] \ar[l,red] & \squared{3} \\ 
			{\color{blue}\circled{2}} & {\color{gray}\pentagoned{6}}\ar[r,OliveGreen,decorate]\ar[l,OliveGreen,decorate] & {\color{blue}\circled{4}} 
		\end{tikzcd}
		\qquad \text{vs.}
		\qquad
		\begin{tikzcd}[column sep = small,decoration={snake,amplitude=1pt}]
			\squared{1} & {\color{Fuchsia}\triangled{5}}\ar[r,red] \ar[l,red]\ar[rd,dotted,Maroon]\ar[ld,dotted,Maroon] & \squared{3} \\ 
			{\color{blue}\circled{2}} & {\color{gray}\pentagoned{6}}\ar[r,OliveGreen,decorate]\ar[l,OliveGreen,decorate]\ar[ru,dashed,orange]\ar[lu,dashed,orange] & {\color{blue}\circled{4}} 
		\end{tikzcd}
	\end{center}
	The black/square vertices 1 and 3 are the snout lengths of the two offspring. Blue/circular vertices 2 and 4 are their head lengths. The purple/triangular vertex 5 is the snout length of the parent and grey/pentagonal vertex 6 is the head length of the parent. The edges encode the dependence of the offsprings' traits on those of the parents.
	
	Maximum likelihood estimation in the left hand model is two copies of Example~\ref{ex:heights}, one on the three odd variables, and one on the three even variables. Thus, given one sample a unique MLE exists almost surely. For the right hand model, Theorem~\ref{thm:RDAGMLestimationLinDependence} says that an MLE exists provided $Y^{(5)} \neq 0$, $Y^{(6)} \neq 0$  and neither $\begin{pmatrix} Y^{(1)} & Y^{(3)} \end{pmatrix}$ nor $\begin{pmatrix} Y^{(2)} & Y^{(4)} \end{pmatrix}$ are in $\mathrm{span} \left\lbrace \begin{pmatrix} Y^{(5)} & Y^{(5)} \end{pmatrix}, \begin{pmatrix} Y^{(6)} & Y^{(6)} \end{pmatrix} \right\rbrace$.
	Hence an MLE exists almost surely with one sample. Moreover, the augmented sample matrices $M_{Y,{\color{blue}\circ}}$ and $M_{Y, {\square} }$ have full row rank almost surely provided $n \geq 2$, hence the MLE exists uniquely with two samples, by Theorem~\ref{thm:RDAGMLestimationLinDependence}.
	\hfill\exSymbol
\end{example} 



\section{Bounds on ML thresholds} \label{sec:ThresholdsRDAG}


In the previous section we gave a characterization of existence and unique existence of an MLE based on linear independence conditions, Theorem~\ref{thm:RDAGMLestimationLinDependence}. Here we use this theorem to give bounds on ML thresholds for RDAG models. These bounds hold whenever the colouring is compatible and there are no edges between vertices of the same colour.

We point out that, similarly to the DAG case, $\mlt_b(\MGcar) = \mlt_e(\MGcar)$ holds by Theorem~\ref{thm:RDAGMLestimationLinDependence}. However, in distinction to DAG models, we can have $\mlt_e(\MGcar) < \mlt_u(\MGcar)$ for an RDAG model $\MGcar$. In fact, Example~\ref{ex:ArbitraryLargeGap} gives a family of RDAG models for which the gap becomes arbitrarily large.

The section is organized as follows. We start with some definitions. Then we prove two lemmata and two propositions to deduce the main result, Theorem~\ref{thm:RDAGboundsMlt}. Afterwards, we discuss examples and end with a randomized method to compute existence and uniqueness threshold.

\medskip

\begin{defn}[{\cite[Definition~5.1]{RDAG}}]	\label{def:rs} 
	Let $M_Y$ be a matrix whose entries are linear combinations of the entries of a matrix $Y \in \KK^{m \times n}$. 
	The \emph{generic rank}\index{generic rank} of $M_Y$ is its rank for generic $Y$.
	\hfill\defnSymbol
\end{defn}

We often study the generic rank of $M_Y$ by considering it as a symbolic matrix whose entries are linear forms in the $mn$ indeterminates $Y_{ij}$.

\begin{example}[{\cite[Example~5.2]{RDAG}}] \label{ex:ArbitraryLargeGap1}
	The coloured DAG\footnote{with two vertex colours (blue/circular and black/square) and five edge colours (red/solid, orange/dashed, green/squiggly, purple/zigzag, and brown/dotted)} 
	\begin{center}
		\begin{tikzcd}[column sep = small,decoration={snake,amplitude=1pt}]
			& & {\color{blue}\circled{1}} & &\\
			\squared{3} \ar[rrd, red, bend right = 30] \ar[rru, red, bend left = 30] & \squared{4} \ar[rd, orange,dashed] \ar[ru, orange,dashed] & \squared{5} \ar[d, OliveGreen,decorate] \ar[u, OliveGreen,decorate] & \squared{6} \ar[ld, Fuchsia,decorate,decoration={zigzag,amplitude=3pt}] \ar[lu, Fuchsia,decorate,decoration={zigzag,amplitude=3pt}] & \squared{7} \ar[llu, Maroon, bend right = 30,dotted] \ar[lld, Maroon, bend left = 30,dotted] \\
			& & {\color{blue}\circled{2}} & &
		\end{tikzcd}
		\qquad \text{has} \qquad 
		$   M_{Y,{\color{blue}\circ}} = \begin{pmatrix}
			Y^{(1)} & Y^{(2)} \\ Y^{(3)} & Y^{(3)} \\
			Y^{(4)} & Y^{(4)} \\ Y^{(5)} & Y^{(5)} \\
			Y^{(6)} & Y^{(6)} \\
			Y^{(7)} & Y^{(7)}
		\end{pmatrix}
		\begin{matrix}
			%comment: "\ " is a protected blank space, this avoids an error by tikzcd (arrows need an endpoint)
			{\color{blue}\circ} \\ \begin{tikzcd}[cramped, sep = small] \ar[r, red] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, orange, dashed] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, OliveGreen, decorate, decoration={snake,amplitude=1.2pt}] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, Maroon, dotted] & \  \end{tikzcd}
		\end{matrix}$
	\end{center}
	When $n=1$, the matrix $M_{Y,{\color{blue}\circ}}$ has generic rank two. Removing its top row gives a $5 \times 2$ matrix of generic rank one.
	\hfill\exSymbol
\end{example}

Let $\Gc$ be a coloured DAG. For $s \in c(I)$, let $\alpha_s$ be the number of vertices of colour $s$ and $\beta_s$ the number of parent relationship colours of $s$. 

\begin{defn} \label{defn:M'YsEtc}
	Fix a vertex colour $s \in c(I)$. For sample matrix $Y \in \KK^{m \times n}$ let $M_{Y,s} \in \KK^{(\beta_s + 1) \times \alpha_s n}$ be as in Definition~\ref{defn:MYs}. We define the following.
		\begin{enumerate} \itemsep 0pt
			\item \label{defn:M'Ys}
			$M'_{Y,s} \in \KK^{\beta_s \times \alpha_s n}$ is the submatrix of $M_{Y,s}$ obtained from removing the top row $M_{Y,s}^{(0)}$. In other words, the rows of $M'_{Y,s}$ are $M_{Y,s}^{(1)}, M_{Y,s}^{(2)}, \ldots, M_{Y,s}^{(\beta_s)}$.
			
			\item \label{defn:r-s} $r_s$ is the generic rank of $M'_{Y,s}$ when $n = 1$.
			
			\item $\mlt_e \big(\MGcar, s\big)$ is the smallest $n$ such that $M_{Y,s}^{(0)} \notin \big\lbrace M_{Y,s}^{(t)} \mid t \in \prc(s) \big\rbrace$ holds for almost all $Y \in \KK^{m \times n}$.
			
			\item $\mlt_u \big(\MGcar, s\big)$ is the smallest $n$ such that $M_{Y,s}$ has full row rank $\beta_s + 1$ for almost all $Y \in \KK^{m \times n}$.\hfill\defnSymbol
		\end{enumerate}
\end{defn}

In the following we prove bounds on the ML thresholds of an RDAG model $\MGcar$. We proceed as follows.
By Theorem~\ref{thm:RDAGMLestimationLinDependence}, it suffices to give bounds on $\mlt_e \big(\MGcar, s\big)$ and $\mlt_u \big(\MGcar, s\big)$ for all $s \in c(I)$. Such bounds are given in Propositions~\ref{prop:boundMltsExistence} and~\ref{prop:boundMltsUniqueness} in terms of $\alpha_s$, $\beta_s$ and $r_s$.
To obtain these bounds, we show the following two lemmata which study the generic rank of $M'_{Y,s}$ as the sample size $n$ grows.

\begin{lemma}[{\cite[Lemma~5.8]{RDAG}}] \label{lem:HelpMLTsUniqueness}
	Consider the RDAG model on $\Gc$ where colouring $c$ is compatible, and fix a vertex colour~$s$. For $n \geq \beta_s$ and generic $Y \in \KK^{m \times n}$ the row vectors  $M_{Y,s}^{(1)}, \ldots, M_{Y,s}^{(\beta_s)}$ are linearly independent.
\end{lemma}

%todo illustrative example? to showcase combinatorial idea

\begin{proof}
	We think of the $mn$ entries of $Y$ as indeterminates and construct an invertible $\beta_s \times \beta_s$ submatrix of $M'_{Y,s} \in \KK^{\beta_s \times \alpha_s n}$.
	
	Let $i_1 < i_2 < \ldots < i_{\alpha_s}$ be the vertices of colour $s$. The matrix $M_{Y,s}'$ has $\alpha_s$ many blocks of size $\beta_s \times n$. For each parent relationship colour $p_t$, $t \in [\beta_s]$ there is some vertex $i_k = i_k(t)$ (where $k \in [\alpha_s]$) such that there is an edge of colour $p_t$ pointing towards vertex $i_k = i_k(t)$. That is, the $k^{th}$ block of $M'_{Y,s}$ has non-zero entries in the $t^{th}$ row. Let $C_t \in \KK^{\beta_s \times 1}$ be the $t^{th}$ column of that block, which exists as $n \geq \beta_s$. By construction, the $t^{th}$ entry of $C_t$ is non-zero. We show that the matrix $C = \begin{pmatrix} C_{1} & C_2 & \ldots & C_{\beta_s}\end{pmatrix}$, is invertible.
	
	An entry of $C$ is either a sum of variables or it is zero. By construction, column $C_t$ only contains (sums of) elements of the $t^{th}$ column of $Y$. The same variable $Y_{j,t}$ cannot occur in two different entries of $C_t$, because there is at most one edge from $j$ to vertex $i_k(t)$. Altogether, the entries of $C$ are (possible empty) sums of variables and each variable occurs in at most one entry of $C$. Thus, and since the determinant is an alternating sum over products of permutations, it is enough to show that one product is non-zero.
	By construction,  $C_{11} C_{22} \cdots C_{\beta_s \beta_s} \neq 0$. Thus, $M_{Y,s}'$ has generic rank $\beta_s$ for $n \geq \beta_s$.
\end{proof}

The next lemma and its proof are contained (in condensed form) in \cite{RDAG} in the proof of Proposition~5.9.

\begin{lemma}\label{lem:M'YsStepsToFullRowRank}
	Let $\Gc$ be a coloured DAG with compatible colouring $c$. For generic $Y \in \KK^{m \times n}$ the rank of $M'_{Y,s}$ is at least $\, \min \{ r_s + n -1 , \beta_s \}$.
\end{lemma}

\begin{proof}
	Note that by construction of $M_{Y,s}$ (respectively $M'_{Y,s}$)
		\[ \mathcal{X}^{ \{1,n\} } := \big\{ M'_{Y,s} \mid Y \in \KK^{m \times n} \big\} \]
	is a linear subspace of $\KK^{\beta_s \times \alpha_s n}$. The generic rank of $M'_{Y,s} \in \KK^{\beta_s \times \alpha_s n}$, denoted $r_s(n)$, is given by $r_s(n) = \max \{ \rk(X) \mid X \in \mathcal{X}^{ \{1,n\} } \}$. Note that $r_s = r_s(1)$, by Definition~\ref{defn:M'YsEtc}.
	The space $\mathcal{X}^{ \{1,n\} }$ is the so-called $(1,n)$ blow up\footnote{This is not related to the blow up construction from Algebraic Geometry, e.g., to resolve singularities.} of $\mathcal{X} := \mathcal{X}^{ \{1,1\} }$. In view of the generic matrix $M'_{Y,s} \in \KK^{\beta_s \times \alpha_s}$ the $(1,n)$ matrix blow up means that the scalar variables $Y^{(i)}$ are replaced by generic row vectors of length $n$, to give a $\beta_s \times \alpha_s n$ matrix.
	As suggested by the notation, this setting fits \cite[Section~2]{derksen2017polynomial}. By \cite[Lemma~2.7 parts~(1) and (3)]{derksen2017polynomial}, we have for all $n$ that
		\begin{equation}\label{eq:rsnIncreasingConcave}
			r_s(n) \leq r_s(n+1) \quad \text{ and } \quad r_s(n + 1) \geq \frac{1}{2} \big( r_s(n) + r_s(n+2) \big),
		\end{equation}
	i.e., $r_s(n)$ is weakly increasing and weakly concave.
	Moreover, the maximum rank among the $r_s(n )$ is $\beta_s$, which occurs for $n \geq \beta_s$ by Lemma~\ref{lem:HelpMLTsUniqueness}.
	Now, let $n$ be such that $r_s(n) < \beta_s$ and $r_s(n) = r_s(n+1)$. Then, by the left inequality in \eqref{eq:rsnIncreasingConcave}, there exists some integer $2 \leq k \leq \beta_s - n$ with
	\[ r_s(n) = r_s(n+1) = \cdots = r_s(n+k-1) < r_s(n+k) , \]
	but this contradicts $ r_s(n + k - 1) \geq \frac{1}{2} \big( r_s(n + k - 2) + r_s(n+k) \big)$, the right inequality of \eqref{eq:rsnIncreasingConcave}. Therefore, $r_s(n) < \beta_s$ implies $r_s(n) +1 \leq r_s(n+1)$. We conclude $ r_s(n) \geq \min ( r_s + n-1, \beta_s)$ for all $n$.
\end{proof}

Equipped with the previous lemmata we prove bounds on $\mlt_e \big(\MGcar, s \big)$ and $\mlt_u \big(\MGcar, s \big)$.

\begin{prop}[{\cite[Proposition~5.10]{RDAG}}] \label{prop:boundMltsUniqueness}
	Let $\Gc$ be a coloured DAG that has no edges between any vertices of colour $s$, and $c$ is compatible. Then
	\[\left\lfloor \frac{\beta_s}{\alpha_s} \right\rfloor + 1 \leq \mlt_u \big(\MGcar, s \big) \leq \beta_s + 2 - r_s. \]
	Moreover, if $r_s \neq \beta_s + 1 - (\beta_s / \alpha_s)$ then $\mlt_u \big( \MGcar, s \big) \leq \beta_s + 1 - r_s$.
\end{prop}

\begin{proof}
	To prove the lower bound, we observe that if $\alpha_s n \leq \beta_s$, then the $\beta_s + 1$ rows of $M_{Y,s}$ will be linearly dependent. Hence, we need at least $n > \beta_s / \alpha_s$ many samples for $M_{Y,s}$ to have generically full row rank.
	
	For the upper bound, let $M'_{Y,s}$ and $r_s$ be as in Definition~\ref{defn:M'YsEtc}. By Lemma~\ref{lem:M'YsStepsToFullRowRank}, for $n$ samples we have $\rk(M_{Y,s}') \geq \min(r_s + n - 1, \beta_s)$ generically.
	Thus, for $n = \beta_s + 1 - r_s$ the matrix $M'_{Y,s}$ generically has full row rank $\beta_s$.
	It remains to consider the top row of $M_{Y,s}$. We must have $\beta_s \leq \alpha_s n$, otherwise the $\beta_s \times \alpha_s n$ matrix $M'_{Y,s}$ could not have full row rank. We look separately at the possible cases: $\beta_s < n \alpha_s$ and $\beta_s = n \alpha_s$.
	If $\beta_s < n \alpha_s$, the row vector $M_{Y,s}^{(0)} \in \KK^{1 \times \alpha_s n}$ is generically not in the span of the $\beta_s$ rows of $M'_{Y,s}$, because there are no edges between vertices of colour $s$. Thus, $M_{Y,s}$ generically has full row rank $\beta_s + 1$, and $\mlt_u(\MGcar, s) \leq n = \beta_s + 1 - r_s$. If $\beta_s = n \alpha_s$, equivalently if $r_s = \beta_s + 1 - (\beta_s / \alpha_s)$, an additional sample ensures $\rk(M_{Y,s}) = \beta_s + 1$ generically.
\end{proof}


\begin{prop}[{\cite[Proposition~5.9]{RDAG}}] \label{prop:boundMltsExistence}
	Let $\Gc$ be a coloured DAG that has no edges between any vertices of colour $s$, and $c$ is compatible. 
	If $\alpha_s = 1$, then $\mlt_e \big(\MGcar, s \big) = \mlt_u \big(\MGcar, s \big) = \beta_s + 1$, while
	if $\alpha_s \geq 2$ we have
		\[ \left\lfloor \frac{r_s-1}{\alpha_s-1} \right\rfloor + 1  \leq \mlt_e \big(\MGcar, s\big) \leq \left\lfloor \frac{\beta_s}{\alpha_s} \right\rfloor + 1 .\]
\end{prop}

\begin{proof}
	If $\alpha_s = 1$, then $M_{Y,s}^{(0)} = Y^{(i)}$ where $i$ is the unique vertex of colour $s$. Moreover, each row $M_{Y,s}^{(t)}$, $t \in [\beta_s]$ is non-zero and a sum of certain $Y^{(j)}$, $j \in \pa(i)$. Note that $Y^{(i)}$ only appears in $M_{Y,s}^{(0)}$ and each parent row of $Y^{(i)}$ appears in exactly one row $M_{Y,s}^{(t)}$, $t \in [\beta_s]$. Similarly to the uncoloured case, the $M_{Y,s}^{(t)}$, $t \in [\beta_s]$ span $\KK^{1 \times n}$ generically if $n \leq \beta_s$; and $M_{Y,s}$ has generically full row rank if $n \geq \beta_s +1$. Altogether, $\mlt_e \big(\MGcar, s \big) = \mlt_u \big(\MGcar, s \big) = \beta_s + 1$ if $\alpha_s = 1$.
	
	It remains to consider $\alpha_s \geq 2$.
	To prove the upper bound, we show that if $n > \beta_s / \alpha_s$, then the top row of $M_{Y,s}$ is generically not in the span of the other rows. Since there are no edges between two vertices of colour $s$, the $n \alpha_s$ entries of the top row $M_{Y,s}^{(0)}$ are all independent, from each other and from the entries of the other rows. If $\beta_s < \alpha_s n$, the other $\beta_s$ rows do not span $\KK^{n \alpha_s}$, so a generic choice of top row will not lie in their span. 
	
	For the lower bound, the generic rank of $M'_{Y,s}$ at least $ \min \{ r_s + n -1 , \beta_s \}$, by Lemma~\ref{lem:M'YsStepsToFullRowRank}. Thus, the top row $M^{(0)}_{Y,s}$ is in the span of the other rows whenever 
	$\min(r_s + n -1, \beta_s) \geq n \alpha_s$. The latter holds in particular, if $\alpha_s n \leq r_s + n - 1 \leq \beta_s$ holds, i.e.,
		\[n \leq \min \left( \left\lfloor\frac{r_s-1}{\alpha_s-1}\right\rfloor , \beta_s + 1 - r_s \right) .	\]
	Hence, we need at least one more sample to guarantee that $M^{(0)}_{Y,s}$ is not in the row span of $M'_{Y,s}$, i.e.,
		\[\mlt_e \big( \MGcar, s \big) \geq \min \left( \left\lfloor\frac{r_s-1}{\alpha_s-1}\right\rfloor + 1 , \beta_s + 2 - r_s \right).\]
	The minimum must be attained by the former expression, because
		\[ \mlt_e \big( \MGcar, s \big) \leq \mlt_u \big( \MGcar, s \big) \leq \beta_s + 2 - r_s \]
	holds by Proposition~\ref{prop:boundMltsUniqueness}.
\end{proof}


As a consequence of Theorem~\ref{thm:RDAGMLestimationLinDependence} parts~(b) and (c) we have
	\[ \mlt_e \big(\MGcar \big) = \max_{s \in c(I)} \: \mlt_e \big( \MGcar, s \big)
		, \quad
		\mlt_u \big(\MGcar \big) = \max_{s \in c(I)} \: \mlt_u \big( \MGcar, s \big). \]
Taking the maximum of the lower and upper bounds in Propositions~\ref{prop:boundMltsExistence} and~\ref{prop:boundMltsUniqueness}, over all vertex colours, gives the main theorem of this section.

\begin{theorem}[{\cite[Theorem~5.3]{RDAG}}] \label{thm:RDAGboundsMlt} %todo first the alpha_s part, then the assumption  on ``no edges between...''
	Consider the RDAG model $\MGcar$ on $\Gc$ where colouring $c$ is compatible, and $\Gc$ has no edges between vertices of the same colour. If $\;\max_{s \in c(I)} \alpha_s = 1$, then
		\[\mlt_e \big(\MGcar\big) = \mlt_u \big(\MGcar\big) = \max_{s \in c(I)} \beta_s + 1 . \]
	Otherwise, we have the following bounds on ML thresholds:
	\begin{align}
		\label{eq:BoundsMltExistence}
		\max_{s \in c(I)} \left\lfloor \frac{r_s-1}{\alpha_s-1} \right\rfloor + 1  &\leq \mlt_e \left( \MGcar \right) \leq \max_{s \in c(I)}  \left\lfloor \frac{\beta_s}{\alpha_s} \right\rfloor + 1 , \\[5pt]
		\max_{s \in c(I)} \left\lfloor \frac{\beta_s}{\alpha_s} \right\rfloor + 1 &\leq \mlt_u \left( \MGcar \right) \leq \max_{s \in c(I)} \left( \beta_s + 2 - r_s \right). \label{eq:BoundsMltUniqueness}
	\end{align}
\end{theorem}

It remains an open problem to turn the bounds in Theorem~\ref{thm:RDAGboundsMlt} into formulae.
\begin{problem}[{\cite[Problem~5.4]{RDAG}}] \label{prob:RDAG-thresholds} %formerly prob:1
	Determine the maximum likelihood thresholds of an RDAG model $\MGcar$, as formulae involving properties of the DAG $\Gcal$ and its colouring $c$.
\end{problem}


\begin{remark} We point out the following regarding Theorem~\ref{thm:RDAGboundsMlt}.
	\begin{itemize}
		\item[(i)] Recall from Remark~\ref{rem:DAGmodelViaCompatible} that any DAG model $\MGar$ is an RDAG model $\MGcar$ with compatible colouring. In this situation, $\alpha_s = 1$ for all $s \in c(I)$ and $\beta_s = |\pa(i)|$, where $i$ is the unique vertex with $c(i) = s$. Therefore, Theorem~\ref{thm:RDAGboundsMlt} contains Corollary~\ref{cor:MLthresholdsDAG} as a special case.
		
		\item[(ii)] The upper bounds for existence threshold and uniqueness threshold are both at most $\max_s \beta_s +1$.\footnote{If $\Gcal$ does not have any edges, i.e., $\beta_s = r_s = 0$ for all $s \in c(I)$, then the uniqueness threshold trivially equals one as then each row vector $M_{Y,s} \in \KK^{1 \times \alpha_s n}$ has generic rank one.} Hence, the RDAG thresholds are always at least as small as the DAG threshold, by part(i).
		
		\item[(iii)] \label{rmk:no_edges_same_colour}
		Theorem~\ref{thm:RDAGboundsMlt} applies to all RDAG models with compatible colouring that are equal to its induced RCON model, because such models never have edges between vertices of the same colour, as follows.
		Take the minimal vertex $i$ such that $i \leftarrow j$ in~$\Gcal$ and $c(i) = c(j)$. Then no children of $i$ have colour $c(i)$, therefore $\Gcal_i \neq \Gcal_j$, a contradiction to Theorem~\ref{thm:RCONequalsRDAG}(b).\footnote{This is \cite[Remark~5.6]{RDAG}.}
		\hfill\remSymbol
	\end{itemize}
\end{remark}

We illustrate the threshold bounds in some examples. The first example shows that the existence threshold and uniqueness threshold for an RDAG model can have arbitrarily large distance.

\begin{example}[{\cite[Example~5.5]{RDAG}}] \label{ex:ArbitraryLargeGap}
	We find the existence and uniqueness threshold for the RDAG model on the coloured DAG $\Gc$ from Example~\ref{ex:ArbitraryLargeGap1}. 
	Since the black (square) vertices have no parents, the matrix $M_{Y, \square}$ has full rank as soon as $n \geq 1$.
	Therefore, the thresholds are determined by vertex colour blue. The generic rank of $M'_{Y,{\color{blue}\circ}}$ is one when $n=1$, i.e., $r_{\color{blue}\circ} = 1$. Using $\alpha_{\color{blue}\circ} = 2$ and $\beta_{\color{blue}\circ} = 5$, Theorem~\ref{thm:RDAGboundsMlt} and Proposition~\ref{prop:boundMltsUniqueness} give bounds
	\begin{align*}
		\frac{r_{\color{blue}\circ}-1}{\alpha_{\color{blue}\circ}-1} + 1 = 1 \leq \mlt_e(\MGcar)
		\quad \text{ and } \quad
		\mlt_u(\MGcar) \leq \beta_{\color{blue}\circ} + 1 - r_{\color{blue}\circ} = 5.
	\end{align*}
	In fact, both bounds are attained as follows. For all $n \geq 1$, the row $M_{Y,{\color{blue}\circ}}^{(0)} = (Y^{(1)}, Y^{(2)})$ is almost surely not contained in the span of the other rows of $M_{Y,{\color{blue}\circ}}$, hence $\mlt_e(\MGcar) = 1$. Moreover, we need $n \geq 5$ samples for generic linear independence of the rows $(Y^{(3)}, Y^{(3)}), \ldots, (Y^{(7)}, Y^{(7)})$. Thus, $\mlt_u(\MGcar) = 5$.
	
	This example extends to an arbitrary number of vertices, i.e., to the coloured DAG with $k+2$ vertices, $2$ blue/circular and $k$ black/square, and $2k$ edges of $k$ colours (arranged as in the $k=5$ case above).
	Repeating the above argument gives $\mlt_e(\MGcar) = 1$ and $\mlt_u(\MGcar) = k$.
	\hfill\exSymbol
\end{example}

We modify the edges from Examples~\ref{ex:ArbitraryLargeGap1} and~\ref{ex:ArbitraryLargeGap} to see how the thresholds change.

\begin{example}[{\cite[Example~5.7]{RDAG}}] \label{ex:RDAGsThresholds}
	Consider the following coloured DAGs, both with compatible colouring
	\begin{center}
		\begin{tikzcd}[column sep = small, decoration={snake,amplitude=1pt}]
			& & {\color{blue}\circled{1}} & &\\
			\squared{3} \ar[rru, red, bend left = 30] & \squared{4} \ar[rd, Maroon, dotted] \ar[ru, orange, dashed] & \squared{5} \ar[u, OliveGreen, decorate] & \squared{6} \ar[lu, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] & \squared{7} \ar[llu, Maroon, dotted, bend right = 30] \\
			& & {\color{blue}\circled{2}} & &
		\end{tikzcd}
		\qquad \qquad
		\begin{tikzcd}[column sep = small, decoration={snake,amplitude=1pt}]
			& & {\color{blue}\circled{1}} & &\\
			\squared{3} \ar[rru, red, bend left = 30] \ar[rrd, orange, dashed, bend right = 30] & \squared{4} \ar[ru, Maroon, dotted] \ar[rd, OliveGreen, decorate] & \squared{5} \ar[u, OliveGreen, decorate] \ar[d, orange, dashed] & \squared{6} \ar[lu, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] \ar[ld, orange, dashed] & \squared{7} \ar[llu, Maroon, dotted, bend right = 30] \\
			& & {\color{blue}\circled{2}} & &
		\end{tikzcd}
	\end{center}
	Since the black vertices do not have parents, the thresholds are determined by the blue vertices.
	Given sample matrix $Y \in \KK^{7 \times n}$, we respectively obtain 
	\begin{equation*}
		M_{Y,{\color{blue}\circ}} = 
		\begin{pmatrix}
			Y^{(1)} & Y^{(2)} \\ Y^{(3)} & 0 \\ Y^{(4)} & 0 \\ Y^{(5)} & 0 \\ Y^{(6)} & 0 \\ Y^{(7)} & Y^{(4)}
		\end{pmatrix}
		\begin{matrix}
			%comment: "\ " is a protected blank space, this avoids an error by tikzcd (arrows need an endpoint)
			{\color{blue}\circ} \\ \begin{tikzcd}[cramped, sep = small] \ar[r, red] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, orange, dashed] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, OliveGreen, decorate, decoration={snake,amplitude=1.2pt}] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, Maroon, dotted] & \  \end{tikzcd}
		\end{matrix}
		\qquad 
		M_{Y,{\color{blue}\circ}} = 
		\begin{pmatrix}
			Y^{(1)} & Y^{(2)} \\ Y^{(3)} & 0 \\ 0 & Y^{(3)} + Y^{(5)} + Y^{(6)} \\ Y^{(5)} & Y^{(4)} \\ Y^{(6)} & 0 \\ Y^{(4)} + Y^{(7)} & 0
		\end{pmatrix}
		\begin{matrix}
			%comment: "\ " is a protected blank space, this avoids an error by tikzcd (arrows need an endpoint)
			{\color{blue}\circ} \\ \begin{tikzcd}[cramped, sep = small] \ar[r, red] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, orange, dashed] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, OliveGreen, decorate, decoration={snake,amplitude=1.2pt}] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] & \  \end{tikzcd} \\ \begin{tikzcd}[cramped, sep=small] \ar[r, Maroon, dotted] & \  \end{tikzcd}
		\end{matrix}
	\end{equation*}
	In both cases we have $\alpha_{\color{blue}\circ} = 2$, $\beta_{\color{blue}\circ} = 5$, and $r_{\color{blue}\circ} = 2$. Thus, Theorem~\ref{thm:RDAGboundsMlt} gives
	\begin{align*}
		2 = \left\lfloor \frac{r_{\color{blue}\circ}-1}{\alpha_{\color{blue}\circ}-1} \right\rfloor + 1 \leq &\mlt_e \leq \left\lfloor \frac{\beta_{\color{blue}\circ}}{\alpha_{\color{blue}\circ}} \right\rfloor + 1 = 3
		\\
		3 = \left\lfloor \frac{\beta_{\color{blue}\circ}}{\alpha_{\color{blue}\circ}} \right\rfloor + 1 \leq &\mlt_u \leq \beta_{\color{blue}\circ} + 2 - r_{\color{blue}\circ} = 5.
	\end{align*}
	Actually, Proposition~\ref{prop:boundMltsUniqueness} yields $\mlt_u \leq 4$, since $r_{\color{blue}\circ} \neq \beta_{\color{blue}\circ} + 1 - (\beta_{\color{blue}\circ} / \alpha_{\color{blue}\circ})$.
	
	%todo stress what the goal is, i.e., that we want to determine exact ML thresholds
	First, we study the left-hand RDAG. When $n=2$ the row $Y^{(2)} \in \KK^{1 \times 2}$ is generically not in the span of $Y^{(4)}$, hence $M_{Y,{\color{blue}\circ}}^{(0)} = (Y^{(1)}, Y^{(2)})$ is not in the linear span of the other five rows of $M_{Y,{\color{blue}\circ}}$, so $\mlt_e = 2$. For $n \geq 2$, the submatrix
	\begin{align*}
		\begin{pmatrix}
			Y^{(2)} \\ Y^{(4)}
		\end{pmatrix} \in \KK^{2 \times n}
	\end{align*}
	has generic rank two. Therefore, $M_{Y,{\color{blue}\circ}} \in \KK^{6 \times 2n}$ has generic rank at most five if $n = 3$. However, $n=4$ suffices for $M_{Y,{\color{blue}\circ}}$ to have full row rank $6$ generically. We conclude $\mlt_u = 4$.
	
	Next, we study the right-hand RDAG. For $n=2$, $M_{Y,{\color{blue}\circ}}^{(0)} = (Y^{(1)}, Y^{(2)})$ is generically contained in the linear span of the other rows of $M_{Y,{\color{blue}\circ}}$. Together with $\mlt_e \leq 3$ we conclude that $\mlt_e = 3$. For uniqueness, when $n = 3$ the submatrices
	\begin{align*}
		\begin{pmatrix}
			Y^{(3)} \\ Y^{(6)} \\ Y^{(4)} + Y^{(7)}
		\end{pmatrix},
		\quad
		\begin{pmatrix}
			Y^{(2)} \\ Y^{(3)} + Y^{(5)} + Y^{(6)} \\ Y^{(4)}
		\end{pmatrix} \in \KK^{3 \times 3}
	\end{align*}
	of $M_{Y,{\color{blue}\circ}}$ generically have rank three, and the zero pattern ensures that $M_{Y,{\color{blue}\circ}}$ has full row rank six generically. Combining this with $3 \leq \mlt_u$ gives $\mlt_u = 3$.
	\hfill\exSymbol
\end{example}

We end this section with the following proposition.

%todo mention that this is poly-time
\begin{prop}[{\cite[Proposition~5.11]{RDAG}}]  \label{prop:RDAGcomputingMlt}
	For an RDAG model $\MGcar$, where colouring $c$ is compatible, there is a randomized algorithm for computing the thresholds $\mlt_e \big(\MGcar \big)$ and $\mlt_u \big( \MGcar \big)$.
\end{prop}

\begin{proof}
	It suffices to give a randomized algorithm to compute $\mlt_e(\MGcar,s)$ and $\mlt_u(\MGcar, s)$ for fixed vertex colour $s$.
	The rank of a symbolic matrix can be computed (efficiently) by a randomized algorithm, see e.g., \cite{lovasz1979onDeterminants, schwartz1980fast}. Hence, thinking of the entries of $Y \in \KK^{m \times n}$ as indeterminates, we can compute for any $n \geq 1$ the rank of the symbolic $(\beta_s + 1) \times \alpha_s n$ matrix $M_{Y,s}$ as well as the rank of the symbolic $\beta_s \times \alpha_s n$ matrix $M'_{Y,s}$. We obtain $\mlt_e(\MGcar,s)$ as the smallest $n$ such that $\rk (M_{Y,s}) > \rk(M'_{Y,s})$ and $\mlt_u(\MGcar, s)$ as the smallest $n$ such that $\rk (M_{Y,s}) = \beta_s + 1$. The algorithm terminates by the upper bound of $\beta_s + 1$ for both $\mlt_e(\MGcar, s)$ and $\mlt_u(\MGcar, s)$.
\end{proof}



\section{Simulations}\label{sec:SimulationsRDAG}


This section is \cite[Section~6]{RDAG}. The simulations, their Python implementation and the analysis were completely done by Anna Seigal.

%todo slight reformulations?
In the previous section, we gave upper and lower bounds for the maximum likelihood thresholds for RDAG models, see Theorem~\ref{thm:RDAGboundsMlt}. The bounds quantify how the graph colouring serves to decrease the number of samples needed for existence and uniqueness of the MLE. In this section, we 
assume that the number of samples is above the maximum likelihood threshold. We
explore via simulations the distance of an MLE to the true model parameters.
We compare the RDAG model estimate 
from Algorithm~\ref{algo:RDAG-MLE} to the usual (uncoloured) DAG model MLE.


The details of our simulations are as follows.
We used the NetworkX Python package~\cite{hagberg2008exploring} to build an RDAG model via the following steps.
We first build a DAG by generating an undirected graph according to an Erdős–Rényi model that includes each edge with fixed probability, and then directing the edges so that $j \to i$ implies $j > i$. We assign edge colours randomly, after fixing the total number of possible edge colours. We choose the unique vertex colouring with the largest number of vertex colours that satisfies the compatibility assumption from Definition~\ref{defn:compatibleColouring}. We sample edge weights $\lambda_{st}$ from a uniform distribution on $[-1, -0.25] \cup [0.25, 1]$ and we sample noise variances $\omega_{ss}$ uniformly from $[0,1]$. Our code is available at \url{https://github.com/seigal/rdag}.

%todo
TODO add explanations about the diagram type "violin plot"


The RDAG MLE is generally closer to the true model parameters than the DAG MLE, see Figure~\ref{fig:1}. As we would expect, both estimates get closer to the true parameters as the number of samples from the distribution increases. At a high number of samples, the difference between the RDAG MLE and the DAG MLE is smaller than at a low number of samples.

\begin{figure}[ht]
	\centering
	\includegraphics[width=8cm]{picture1.png}
	\caption{\cite[Figure~1]{RDAG} We generated RDAGs on $10$ vertices, with each edge present with probability $0.5$ and $5$ edge colours. We sampled from the distribution $n \in \{ 5, 10, 100, 1000, 10000 \}$ times. For each $n$ we generated $50$ random graphs and computed the RDAG MLE and the DAG MLE, comparing them to the true parameter values on a log scale. The results are displayed in a violin plot, with blue for the RDAG MLE and orange for the DAG MLE.}
	\label{fig:1}
\end{figure}

Next we examined how the RDAG MLE was affected by the number of edge colours, see Figure~\ref{fig:2}. The RDAG MLE is closest to the true parameters when the number of edge colours is small; i.e., when there are fewer parameters to learn. As the number of edge colours increases, the difference between the RDAG MLE and the DAG MLE decreases. Note that the DAG model is the setting where each vertex and edge has its own colour.

\begin{figure}[ht]
	\centering
	\includegraphics[width=8cm]{picture2b.png}
	\caption{\cite[Figure~2]{RDAG} We generated RDAGs on $10$ vertices, each edge present with probability $0.5$ and number of edge colours in $\{ 2, 5, 10, 50, 100 \}$. We sampled from the distribution $100$ times and compared the MLE to the true parameter values on a log scale. The DAG MLE is shown in orange for comparison.}
	\label{fig:2}
\end{figure}

Finally, we looked at how the RDAG MLE and DAG MLE are affected by the edge density of the graph, see Figure~\ref{fig:3}. The RDAG MLEs get closer to the true parameter values as the edge density increases: more edges have the same weight, so more samples contribute to estimating each edge weight. By comparison, the DAG MLEs get further from the true parameters as the edge density increases, because there are more parameters to learn.

\begin{figure}[ht]
	\centering
	\includegraphics[width=8cm]{picture3.png}
	\caption{\cite[Figure~3]{RDAG} We generated RDAGs on $10$ vertices, each edge present with probability in $\{ 0.1, 0.3, 0.5, 0.7, 0.9\}$ and $5$ edge colours. For each edge probability we generated $50$ random graphs, sampled from each one $100$ times, and compared the RDAG and DAG MLEs. As above, blue is the RDAG MLE and orange is the DAG MLE.}
	\label{fig:3}
\end{figure}


%end copy paste from RDAG paper



\section{Connections to Stability Notions}\label{sec:RDAGsAndStability}


This section characterizes ML estimation for RDAG models via stability notions under \emph{sets} $\Aset \subseteq \GL_{m}(\KK)$, see Definition~\ref{defn:StabilitySets}. We proceed similar to the study of TDAG models in Section~\ref{sec:TDAGs}. It is remarkable that the full correspondence extends to RDAG models $\MGcar$ with compatible colouring $c$, Theorem~\ref{thm:RDAGstabilityVsMLE}. We prove this by showing that the linear independence conditions from Theorem~\ref{thm:RDAGMLestimationLinDependence} are equivalent to stability notions for the sample matrix, see Theorem~\ref{thm:RDAGStabilityVsLinDependence}. Furthermore, we study the set of MLEs in Proposition~\ref{prop:RDAGsecond-bijection}, which offers an alternative characterization to Corollary~\ref{cor:RDAG-MLEs}.
We start with the weak correspondence for RDAG models.


\begin{remark}[Weak Correspondence for RDAG models, {\cite[Remark~A.5]{RDAG}}]
	\label{rem:RDAGweakCorrespondence} %formerly {rem:A-SL-forRDAG}
	Let $\MGcar$ be an RDAG model with compatible colouring $c$, so $\MGcar = \Mg_{\AGc}$ by Proposition~\ref{prop:RDAGmodelEqualsMgAGc}. The set $\AGc \subseteq \GL_m(\KK)$ is closed under non-zero scalar multiples. Therefore, the weak correspondence, Theorem~\ref{thm:WeakCorrespondence}, holds for the RDAG model $\Mg_{\AGc}$. Moreover, we can always apply the weak correspondence using $\AGc_{\SL}$ (instead of $\AGc_{\SL}^{\pm}$). Indeed, if $\KK = \RR$ and $\alpha_s$ is even for all $s \in c(I)$, then $\AGc$ only contains matrices of positive determinant, so $\AGc_{\SL} = \AGc_{\SL}^{\pm}$. On the other hand, if $\KK = \RR$ and $\alpha_s$ is odd for some vertex colour $s$, then $\AGc$ contains the diagonal, orthogonal matrix $t$ defined by $t_{s,s} := -1$ and $t_{s',s'} := 1$ for $s' \in c(I) \backslash \{s\}$. We have $ta \in \AGc$ for all $a \in \AGc$, by Lemma~\ref{lem:PropertiesCompatibleColouring}(iii). Therefore, condition~(ii) in Theorem~\ref{thm:WeakCorrespondence} is satisfied: choose $o(a) = \Id_m$ if $\det(a) > 0$ and otherwise choose $o(a) = t$.
	\hfill\remSymbol
\end{remark}

Next, we link the linear independence conditions from Theorem~\ref{thm:RDAGMLestimationLinDependence} to stability notions under $\AGc_{\SL}$. First, we prove a condition for polystability.

\begin{lemma}[{\cite[Lemma~A.6]{RDAG}}] \label{lem:RDAGSemistablePolystable}
	Consider the RDAG model on $\Gc$ where colouring $c$ is compatible,
	and set $\Aset := \AGc_{\SL}$. 
	Let $Y \in \KK^{m \times n}$ be such that $M_{Y,s}^{(0)} \notin \Span \big\lbrace M_{Y,s}^{(1)},\ldots, M_{Y,s}^{(\beta_s)} \big\rbrace$ for all $s \in c(I)$. Then $Y$ is polystable under $\Aset$ and $\Aset \cdot Y$ is Zariski closed.
\end{lemma}

\begin{proof}
	Note that the assumption on $Y$ implies that $Y \neq 0$.
	To study the orbit $\Aset \cdot Y$, let $T$ be the set of diagonal matrices in $\Aset$ and $U$ the set of unipotent matrices in $\Aset$. We have $\Aset = T \cdot U$ and actually any $a \in \Aset$ admits a unique decomposition $a = t(a)u(a)$, where $t(a) \in T$ and $u(a) \in U$, compare Lemma~\ref{lem:PropertiesCompatibleColouring}(iv).
	For $s \in c(I)$, recall the construction of $M_{Y,s} \in \KK^{(\beta_s + 1)\times \alpha_s n}$ from Definition~\ref{defn:MYs}. Setting $V_s := \KK^{1 \times \alpha_s n}$ we can identify $\KK^{m \times n} \cong \bigoplus_s V_s$ such that the rows of vertex colour $s$ belong to $V_s$.
	By Equation~\eqref{eq:MYsLeftMultiplication}, the set $U \cdot Y$ is $H := \prod_s H_s$ with
	\begin{align*}
		H_s = \left\lbrace M_{Y,s}^{(0)} + a_{s,1} M_{Y,s}^{(1)} + \ldots + a_{s,\beta_s} M_{Y,s}^{(\beta_s)} \mid a_{s,t} \in \KK \right\rbrace.
	\end{align*}
	The affine space $H_s$ equals $M_{Y,s}^{(0)} + X_s$, where $X_s := \mathrm{span} \big\lbrace M_{Y,s}^{(1)},\ldots, M_{Y,s}^{(\beta_s)} \big\rbrace$. Since $M_{Y,s}^{(0)} \notin X_s$ for all $s \in c(I)$,  we have a direct sum $K_s := \big( \KK M_{Y,s}^{(0)} \big) \oplus X_s \subseteq V_s$ and $H_s$ has at least codimension one in $V_s$.
	Since $T$ acts on each $V_s$ with the non-zero scalar for vertex colour $s$, we have
	\begin{align*}
		\Aset \cdot Y = T \cdot (U \cdot Y) = T \cdot H = T \cdot \prod_s H_s
		\subseteq \bigoplus_s K_s \subseteq \bigoplus_s V_s.
	\end{align*}
	It suffices to show that $\Aset \cdot Y$ is Zariski-closed in $\bigoplus_s K_s$. Each $H_s$ is an affine subspace of $K_s$ with codimension one, by definition of $K_s$. Therefore, there exists a linear form $p_s \in K_s^*$ such that
		\[ H_s = \mathbb{V}_{K_s}(p_s - 1) , \]
	where $\mathbb{V}(\cdot)$ denotes the vanishing locus.
	
	We finish the proof by showing that $\Aset \cdot Y = \mathbb{V} \big( \prod_s p_s^{\alpha_s} - 1 \big)$ in $\bigoplus_{s} K_s$.
	First, given $W = (W_s)_s \in \Aset \cdot Y = T \cdot H$ we can write $W = t \cdot Z$ with $t \in T$ and $Z = (Z_s)_s \in H$. Then
	\[ \Big(\prod_s p_s^{\alpha_s} \Big)(W) = \prod_s p_s(W_s)^{\alpha_s}
	= \prod_s \big(t_{ss} p_s(Z_s) \big)^{\alpha_s} = \prod_s (t_{ss})^{\alpha_s} = 1 \]
	by the choice of $p_s \in K_s^*$ and since $\det(t) = \prod_s t_{ss}^{\alpha_s} = 1$. On the other hand, suppose $W = (W_s)_s \in \mathbb{V} \big( \prod_s p_s^{\alpha_s} - 1 \big) \subseteq \bigoplus_{s} K_s$. Set $t_{ss} := p_s(W_s)$, then we have $\prod_s t_{ss}^{\alpha_s} = 1$, so the $t_{ss}$ define some $t \in T$.
	Moreover, $t^{-1}_{ss} W_s \in H_s$ by definition of $t_{ss}$, so $W' := (t^{-1}_{ss} W_s)_s \in H$. Hence, $W = t \cdot W'$ is contained in $T \cdot H = \Aset \cdot Y$.
\end{proof}

The upcoming theorem links stability notions under $\AGc_{\SL}$ to linear independence conditions. It generalizes Theorem~\ref{thm:StabilityLinearIndepTDAG} for TDAG models, compare Remark~\ref{rem:MYsDAGmodel}.

\begin{theorem}[{\cite[Proposition~A.7]{RDAG}}] \label{thm:RDAGStabilityVsLinDependence}
	Consider an RDAG model on $\Gc$ with compatible colouring $c$ and sample matrix $Y \in \KK^{m \times n}$.
	Stability under $\AGc_{\SL}$ relates to linear independence conditions on the matrices $M_{Y,s}$:
	\[ \begin{matrix} (a) &  Y \text{ unstable}   & \Leftrightarrow & \exists \, s \in c(I) \colon & M_{Y,s}^{(0)} \in \Span \big\lbrace M_{Y,s}^{(i)} : i \in [\beta_s] \big\rbrace  \\ 
		(b) & Y \text{ polystable} & \Leftrightarrow & \forall \, s \in c(I) \colon  &  M_{Y,s}^{(0)} \notin \Span  \big\lbrace M_{Y,s}^{(i)} : i \in [\beta_s] \big\rbrace \\
		(c) & Y \text{ stable} &  \Leftrightarrow & \forall \, s \in c(I) \colon  &  M_{Y,s} \text{ has full row rank} . \end{matrix} \]
	In particular, $Y$ is semistable if and only if it is polystable.
\end{theorem} 

\begin{proof}
	The RDAG model $\MGcar$ equals $\Mg_{\AGc}$ by compatibility.
	Recall that the weak correspondence, Theorem~\ref{thm:WeakCorrespondence}, holds for $\Mg_{\AGc}$ using $\Aset := \AGc_{\SL}$, compare Remark~\ref{rem:RDAGweakCorrespondence}. Therefore, part~(a) and the forwards direction of (b) follows in combination with Theorem~\ref{thm:RDAGMLestimationLinDependence}, while Lemma~\ref{lem:RDAGSemistablePolystable} gives the backwards direction of (b).
	
	For part (c), it suffices to see that a polystable $Y$ has a trivial stabilizing set $\Aset_Y$ if and only if for all $s \in c(I)$ the rows $M_{Y,s}^{(1)},\ldots, M_{Y,s}^{(\beta_s)}$ are linearly independent. So let $Y$ be polystable. By Equation~\eqref{eq:MYsLeftMultiplication}, a matrix $a \in \Aset$ satisfies $aY = Y$ if and only if for all $s \in c(I)$
	\begin{equation}\label{eq:stableRDAG}
		a_{s,s} M_{Y,s}^{(0)} + \sum_{t \in [\beta_s]} a_{s,t} M_{Y,s}^{(t)} = M_{Y,s}^{(0)} \, .
	\end{equation}
	We have $M_{Y,s}^{(0)} \notin \Span  \big\lbrace M_{Y,s}^{(i)} : i \in [\beta_s] \big\rbrace$ for all $s \in c(I)$, by part~(b) and $Y$ being polystable. Therefore, Equation~\eqref{eq:stableRDAG} implies $a_{s,s} = 1$ and $\sum_{t \in [\beta_s]} a_{s,t} M_{Y,s}^{(t)} = 0$.
	
	If $M_{Y,s}^{(1)},\ldots, M_{Y,s}^{(\beta_s)}$ are linearly independent, then \eqref{eq:stableRDAG} has exactly one solution, namely $a_{s,s} = 1$ and $a_{s,t}=0$ for all $t \in [\beta_s]$. Thus, if $M_{Y,s}^{(1)},\ldots, M_{Y,s}^{(\beta_s)}$ are linearly independent for all $s \in c(I)$, then $\Aset_Y = \{\Id_m \}$.
	On the other hand, if for some $s \in c(I)$ the rows $M_{Y,s}^{(1)},\ldots, M_{Y,s}^{(\beta_s)}$ are linearly dependent, then $\sum_{t \in [\beta_s]} a_{s,t} M_{Y,s}^{(t)} = 0$ has infinitely many solutions. Distinct solutions give distinct unipotent matrices $u \in \Aset$ by setting $u_{s,t} := a_{s,t}$ for $t \in \prc(s)$, and $u_{s',t'} := 0$ for $s' \in c(I) \backslash \{s\}$, $t' \in \prc(s')$. By \eqref{eq:MYsLeftMultiplication}, such a unipotent matrix $u \in \Aset$ satisfies $uY = Y$, since the sets $\prc(s)$ are disjoint, so the $u_{s,t}$ do not affect any rows of $Y$ with a different vertex colour. In conclusion, $\Aset_Y$ is infinite if $M_{Y,s}^{(1)},\ldots, M_{Y,s}^{(\beta_s)}$ are linearly dependent for some $s \in c(I)$.
\end{proof}


Combining Theorem~\ref{thm:RDAGStabilityVsLinDependence} with Theorem~\ref{thm:RDAGMLestimationLinDependence} directly yields the following.

\begin{theorem}[Full Correspondence for RDAG models, {\small\cite[Theorem~A.2]{RDAG}}] \label{thm:RDAGstabilityVsMLE}
	Consider the RDAG model on $\Gc$ with compatible colouring $c$ and sample matrix $Y \in \KK^{m \times n}$. Then stability under $\AGc_{\SL}$ relates to ML estimation:
	\[ \begin{matrix}
		\text{(a)} & Y \text{ unstable} &  \Leftrightarrow &  \ell_Y \text{ unbounded from above} \\ 
		\text{(b)} & Y \text{ semistable} & \Leftrightarrow & \ell_Y \text{ bounded from above} \\
		\text{(c)} & Y \text{ polystable} & \Leftrightarrow & \text{MLE exists} \\
		\text{(d)} & Y \text{ stable}  & \Leftrightarrow & \text{MLE exists uniquely.}
	\end{matrix} \]
\end{theorem}

Theorem~\ref{thm:RDAGstabilityVsMLE} applies to any DAG model, see Remark~\ref{rem:DAGmodelViaCompatible}. Therefore, Theorem~\ref{thm:RDAGstabilityVsMLE} generalizes Theorem~\ref{thm:FullCorrespondenceTDAG}. First, it extends from \emph{transitive} DAGs to all DAGs and, second, it extends from uncoloured DAG models to RDAG models.

Now, we link the stabilizing set $\Aset_Y$ to the set of MLEs given $Y$. Recall that  in the case of Gaussian group models given by a self-adjoint group $G$ such a connection is made in Proposition~\ref{prop:MLEsTransitiveStabilizerAction}. For its proof Kempf-Ness, Theorem~\ref{thm:KempfNessAKRS}(b), was crucial.
To be able to adapt the proof method, the next lemma serves as a substitute of the Kempf-Ness theorem.

\begin{lemma}[{\cite[Lemma~A.8]{RDAG}}] \label{lem:twoStepRDAG}
	Consider the RDAG model on $\Gc$ where colouring $c$ is compatible. For $\Aset := \AGc_{\SL}$ let $T$ and $U$ be the set of diagonal respectively unipotent matrices in $\Aset$. If $Y \in \KK^{m \times n}$ is polystable under $\Aset$, then the following hold:
	\begin{itemize}
		\item[(a)] $U \cdot Y$ contains a unique element $\widetilde{Y}$ of minimal norm.
		\item[(b)] For $t \in T$ and $u \in U$, $\|tu \cdot Y\| \geq \|t \cdot \widetilde{Y} \|$ with equality if and only if $u \cdot Y = \widetilde{Y}$.
		\item[(c)]  Let $a, \widetilde{a} \in \Aset$ be such that $a \cdot Y$ and $\widetilde{a} \cdot Y$ are of minimal norm in $\Aset \cdot Y$. Then there is some $t \in T$ such that $t\HT t = \Id_m$ and $\;ta \cdot Y = \widetilde{a} \cdot Y$.
	\end{itemize}
\end{lemma}

\begin{proof}
	We often use Lemma~\ref{lem:PropertiesCompatibleColouring} in this proof without explicitly referencing it.
	Since the $\prc(s)$, $s \in c(I)$ partition the edge colours, when minimizing
		\[ \|uY\|^2 = \sum_{s \in c(I)} \Big\| M_{uY,s}^{(0)} \Big\|^2 \overset{\eqref{eq:MYsLeftMultiplication}}{=}
		\sum_{s \in c(I)} \Big\| M_{Y,s}^{(0)} + \sum_{t \in [\beta_s]} u_{s,t} M_{Y,s}^{(t)} \Big\|^2 \]
	over $u \in U$ we can minimize each summand separately. For each $s \in c(I)$, the affine space $M_{Y,s}^{(0)} + \Span \big\lbrace M_{Y,s}^{(t)} \colon t \in [\beta_s] \big\rbrace$ has a unique element of minimal norm, call it $M_s$. Hence, $U \cdot Y$ has a unique element of minimal norm $\widetilde{Y}$, determined by $M_{\widetilde{Y},s}^{(0)} = M_s$ for all $s \in c(I)$.\footnote{Note that there may be several $u \in U$ with $uY = \widetilde{Y}$, i.e., the uniqueness only refers to $\widetilde{Y}$.}
	This shows part~(a).
	
	To prove part~(b), we use (the proof of) part~(a) to obtain
	\begin{equation}\label{eq:twoStepRDAG}
		\big\| M_{tuY,s}^{(0)} \big\|^2 = \big\| t_{ss} \, M_{uY,s}^{(0)} \big\|^2
		= |t_{ss}|^2 \, \big\| M_{uY,s}^{(0)} \big\|^2 \geq |t_{ss}|^2 \, \big\| M_{\widetilde{Y},s}^{(0)} \big\|^2 = \big\| M_{t \widetilde{Y},s}^{(0)} \big\|^2
	\end{equation}
	for all $s \in c(I)$, hence $\|tu \cdot Y \| \geq \| t\widetilde{Y} \|$. The latter inequality is strict if and only if there is strict inequality in \eqref{eq:twoStepRDAG} for at least one $s$. By $ |t_{ss}|^2 > 0$ and uniqueness of $\widetilde{Y}$, this is the case if and only if $uY \neq \widetilde{Y}$.
	
	For~(c), write $a = tu$ with $t \in T$ and $u \in U$. Since $aY$ is of minimal norm in $\Aset \cdot Y$, we deduce $uY = \widetilde{Y}$ using~(b). Thus, $aY \in T \cdot \widetilde{Y}$ and similarly $\widetilde{a}Y \in T \cdot \widetilde{Y}$. As $T \cdot \widetilde{Y} \subseteq \Aset \cdot Y$ the matrices $aY$ and $\widetilde{a}Y$ are also of minimal norm in $T \cdot \widetilde{Y}$. Recall that $T \cong \{ (t_{ss})_{s \in c(I)} \in (\KK^{\times})^{|c(I)|} \mid \prod_s t_{ss}^{\alpha_s} = 1\}$ is a diagonalizable group. In particular, $T$ is reductive. Hence, Kempf-Ness, Theorem~\ref{thm:KempfNessAKRS}(b), for the action of $T$ implies that there is some $t \in T$ with $t\HT t = \Id_m$ and $\; taY = \widetilde{a}Y$.
\end{proof}

We finish the section with an alternative description of the set of MLEs.

\begin{prop}[{\cite[Proposition~A.3]{RDAG}}] \label{prop:RDAGsecond-bijection}
	Fix the RDAG model on $\Gc$ with compatible colouring $c$ and set $\Aset := \AGc_{\SL}$. If $\lambda a\HT a$ is an MLE given $Y$, where $a \in \Aset$ and $\lambda > 0$ as in Theorem~\ref{thm:WeakCorrespondence}, then the set of MLEs given $Y$ is in bijection with $\Aset_Y$ under mapping $b \in \Aset_Y$ to $\lambda (a + b - \Id_m)\HT (a + b - \Id_m)$.
\end{prop}

%todo $b$ has two meanings; once in A_Y and once in N_Y

\begin{proof}
	As usual, let $T$ and $U$ be the set of diagonal respectively unipotent matrices in $\Aset$.
	If $aY = Y$ for some $a \in \Aset$, then Equation~\eqref{eq:MYsLeftMultiplication} becomes
		\[ M_{Y,s}^{(0)} = a_{s,s} M_{Y,s}^{(0)} + \sum_{t \in [\beta_s]} a_{s,t} M_{Y,s}^{(t)}. \]
	We have $M_{Y,s}^{(0)} \notin \mathrm{span} \big\lbrace M_{Y,s}^{(t)} \colon t \in [\beta_s] \big\rbrace$ for all $s \in c(I)$, since $Y$ is polystable. Thus, $a_{s,s} = 1$ for all $s$, i.e., $a \in U$ and therefore $\Aset_Y = U_Y$. We set $N_Y := U_Y -\Id_m$, which consists of strictly upper triangular matrices. It suffices to show that for fixed MLE $\lambda a\HT a$ the map
	\begin{align*}
		\varphi \colon N_Y &\to \{ \text{MLEs given } Y \} \\
		b &\mapsto \lambda (a + b)\HT (a + b)
	\end{align*}
	is well-defined and bijective. For the latter, note that $bY = 0$ for any $b \in N_Y$. Therefore, $(a + b)Y = aY$ is of minimal norm in $\Aset \cdot Y$ and thus $\varphi(b)$ is also an MLE by the weak correspondence, Theorem~\ref{thm:WeakCorrespondence}.
	
	For surjectivity, let $\lambda \widetilde{a}\HT \widetilde{a}$ be another MLE given $Y$. Then $aY$ and $\widetilde{a} Y$ are of minimal norm in $\Aset \cdot Y$, hence there is some $t \in T$ with $t\HT t = \Id_m$ and $aY = t\widetilde{a}Y$ by Lemma~\ref{lem:twoStepRDAG}(c). We set $b := t \widetilde{a} - a$ so that $b \cdot Y = 0$ and $(\Id_m + b)Y = Y$. By Lemma~\ref{lem:PropertiesCompatibleColouring}(iii), we have $t \widetilde{a} \in \Aset$ and thus all entries of $b = t \widetilde{a} - a$ obey the colouring $c$. Thus, we can also use Equation~\eqref{eq:MYsLeftMultiplication} for $bY = 0$:
		\[ 0 = M_{bY,s}^{(0)} = b_{s,s} M_{Y,s}^{(0)} + \sum_{t \in [\beta_s]} b_{s,t} M_{Y,s}^{(t)}. \]
	The latter implies $b_{s,s} = 0$ for all $s \in c(I)$ by polystability of $Y$, hence $b \in N_Y$. We compute $\varphi(b) = \lambda (t\widetilde{a})\HT (t \widetilde{a}) = \lambda \widetilde{a}\HT \widetilde{a}$ using $t\HT t = \Id_m$.
	
	To show injectivity, let $b, b' \in N_Y$ be such that $\varphi(b) = \varphi(b')$. Let $t \in T$ be defined by $t_{s,s} = \overline{a_{s,s}} / |a_{s,s}|$. Then $t\HT t = \Id_m$ and thus
	\[ (ta + tb)\HT (ta + tb) = (a+b)\HT t\HT t(a+b) = (a+b)\HT (a+b).\]
	Similarly, $(ta + tb')\HT (ta + tb') = (a+b')\HT (a+b')$. Therefore, $\varphi(b) = \varphi(b')$ implies
	\begin{equation}\label{eq:bijectionCholesky}
		(ta + tb)\HT (ta + tb) = (ta + tb')\HT (ta + tb').
	\end{equation}
	Moreover, $tb$ and $tb'$ are strictly upper triangular and $ta \in \Aset$ has positive diagonal entries $|a_{s,s}|$, by construction of $t$. Hence, applying uniqueness of the Cholesky decomposition to \eqref{eq:bijectionCholesky} gives $ta + tb = ta + tb'$, and we deduce $b = b'$.
\end{proof}






\section{Connections to Gaussian group models}\label{sec:RDAGsGaussianGroupModels}


Although many presented results on RDAGs do not need a group structure on $\AGc$, we have more tools available if $\AGc$ is a group.\footnote{In fact, Visu Makam, Anna Seigal and myself first studied RDAG models where $\AGc$ was assumed to be a group. The results on TDAG models as Gaussian group models served as a guideline and this perspective fostered our understanding to obtain many results of \cite{RDAG}.}
In this section we illustrate this as follows. We use Popov's Criterion from Section~\ref{sec:Popov} to study polystability of a sample matrix $Y$. Moreover, we give a description of the set of MLEs in an RDAG model via the action of the stabilizer from Proposition~\ref{prop:MLEsStabilizer}. We start with the \emph{butterfly criterion}, which characterizes when $\AGc$ is a subgroup of $\GL_m(\KK)$.

\subsubsection{The Butterfly Criterion}

Recall that for a DAG $\Gcal$ the DAG model is $\Mg_{\AG}$, see Lemma~\ref{lem:DAGmodelEqualsMgAG}, and in view of Gaussian group models it was natural to ask when $\AG$ is a group. By Proposition~\ref{prop:TDAGgroup}, the latter is the case if and only if $\Gcal$ is transitive.

Similarly, we have seen that the RDAG model of a coloured DAG $\Gc$ with compatible colouring $c$ equals $\Mg_{\AGc}$, compare Proposition~\ref{prop:RDAGmodelEqualsMgAGc}. Thus one may ask for an analogous characterization when $\AGc$ is a group. For this, we define the concept of a butterfly graph.

\begin{defn}[Butterfly graph] \label{defn:ButterflyGraph}
	Let $\Gc$ be a coloured DAG.
	For a pair of vertices $i,j \in [m]$, define the \emph{butterfly body} as
		\[ b(ij) := \big\{ k \in [m] \mid i \leftarrow k, k \leftarrow j \text{ in } \Gcal \big\}  . \]
	The \emph{butterfly graph}\index{butterfly graph} $\Gcal_{b(ij)}$ is defined as the coloured subgraph on $\{i\} \cup \{j\} \cup b(ij)$, with edges $i \leftarrow k, k \leftarrow j$ for each $k \in b(ij)$, and colours inherited from $c$.
	\hfill\defnSymbol
\end{defn}

\begin{example} \label{exa:ButterflyGraph}
	Consider the coloured DAG
		\begin{center}
			\begin{tikzcd}[decoration={snake,amplitude=1pt}]
				 & \squared{2} \ar[ld, orange, dashed] & \squared{3} \ar[l, Maroon, dotted] \ar[lld, red] &  \\
				\squared{1} & & & \squared{6} \ar[lll, Maroon, dotted] \ar[ld, orange, dashed] \ar[lu, OliveGreen, decorate] \ar[llu, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] \ar[lld, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] \\
				 & \squared{4} \ar[lu, OliveGreen, decorate] & \squared{5} \ar[l, Maroon, dotted] \ar[llu, red] & 
			\end{tikzcd}
		\hspace{1em} which has \hspace{1em}
			\begin{tikzcd}[row sep = small, column sep = large, decoration={snake,amplitude=1pt}]
				& \squared{2} \ar[ldd, orange, dashed] & \\
				& \squared{3} \ar[ld, red]  & \\
				\squared{1} &  & \squared{6} \ar[ldd, orange, dashed] \ar[luu, OliveGreen, decorate] \ar[lu, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] \ar[ld, Fuchsia, decorate, decoration={zigzag,amplitude=3pt}] \\
				& \squared{4} \ar[lu, OliveGreen, decorate] & \\
				& \squared{5} \ar[luu, red] & \\
			\end{tikzcd}
		\end{center}
	as butterfly graph $\Gcal_{b(1, 6)}$.
	We point out that the brown (dotted) edges do not appear in the butterfly graph.
	\hfill\exSymbol
\end{example}

We can characterize when $\AGc$ is a group via the butterfly graphs.

\begin{prop}[Butterfly Criterion {\cite[Proposition~B.2]{RDAG}}]
	\label{prop:butterfly} 
	\ \\
	Consider the RDAG model on $\Gc$ where colouring $c$ is compatible. The set $\AGc$ is a group if and only if
	\begin{itemize}
		\item[(a)] $\Gcal$ is transitive; and
		\item[(b)] if $c(ij) = c(kl)$ for edges $j \to i, \; l \to k$ in $\Gcal$, then $\Gcal_{b(ij)} \simeq \Gcal_{b(kl)}$.
	\end{itemize}
\end{prop}

\begin{remark}
	Given a DAG $\Gcal$, we know from Remark~\ref{rem:DAGmodelViaCompatible} that there is a compatible colouring $c$ on $\Gcal$ such that $\MGar = \MGcar$ and $\AG = \AGc$. Since this colouring $c$ assigns to each edge its own \emph{distinct} colour, item~(b) of Proposition~\ref{prop:butterfly} is trivially satisfied.
	Thus, the Butterfly Criterion contains Proposition~\ref{prop:TDAGgroup} as a special case.
	\hfill\remSymbol
\end{remark}

\begin{proof}[Proof of Proposition~\ref{prop:butterfly}]
	By definition in Equation~\eqref{eq:defnAGc}, $\Id_m \in \AGc$ and there is a $\KK$-linear subspace $L \subseteq \KK^{m \times m}$ such that $\AGc = L \cap \GL_m(\KK)$.
	Hence, by Lemma~\ref{lem:GroupOnlyMultiplication} $\AGc$ is a subgroup of $\GL_m(\KK)$ if and only if $\AGc$ is closed under multiplication. We have $gh \in \AGc$ for $g, h \in \AGc$ if and only if
	\begin{itemize}
		\item[(1)] $(gh)_{ii} = (gh)_{jj}$ whenever $c(i) = c(j)$;
		\item[(2)] $(gh)_{ij} = (gh)_{kl}$ whenever $j \to i$, $l \to k$ in $\Gcal$ have $c(ij) = c(kl)$; and
		\item[(3)] $(gh)_{ij} = 0$ whenever $j \not \to i$ in $\Gcal$.
	\end{itemize} 
	
	For (1), observe that $(gh)_{ii} = g_{ii} h_{ii}$. Thus, if $c(i) = c(j)$ then $(gh)_{ii} = (gh)_{jj}$. 
	For (2), take $j \to i$, $l \to k$ in $\Gcal$ with $c(ij) = c(kl)$. Then
	\[ (gh)_{ij} = g_{ii} h_{ij} + g_{ij} h_{jj} + \sum_{p \in b(ij)} g_{ip} h_{pj}
	\quad \text{and} \quad
	(gh)_{kl} = g_{kk} h_{kl} + g_{kl} h_{ll} + \sum_{q \in b(kl)} g_{kq} h_{ql}, \]
	hence $(gh)_{ij} = (gh)_{kl}$ if $\Gcal_{b(ij)} \simeq \Gcal_{b(kl)}$. Conversely, assume $(gh)_{ij} = (gh)_{kl}$ as a polynomial identity in the unknown entries of matrices $g$ and $h$.
	By compatibility, $c(i) = c(k)$, so $g_{ii} h_{ij} = g_{kk} h_{kl}$.
	Vertex and edge colours are disjoint and the sums over $b(ij)$ and $b(kl)$ only involve edge colours. Thus, $(gh)_{ij} = (gh)_{kl}$ implies $g_{ij} h_{jj} = g_{kl} h_{ll}$, so $h_{jj} = h_{ll}$, and the sum over $b(ij)$ must equal the sum over $b(kl)$. This means $c(j) = c(l)$, and the two collections $(c(ip),c(pj)), p \in b(ij)$ and $(c(kq),c(ql)), q \in b(kl)$ of \emph{ordered} pairs\footnote{The order matters, since the variables in the entries of $g$ are distinct from those in $h$. Also compare Example~\ref{exa:OrderButterfly} for an illustration.}
	counted with multiplicity agree. Compatibility ensures the correct colours on the vertices in $b(ij)$ and $b(kl)$ as well, hence $\Gcal_{b(ij)} \simeq \Gcal_{b(kl)}$.
	
	For (3), we observe that if $j \not \to i$ in $\Gcal$ then $g_{ij} = 0 = h_{ij}$ and therefore $(gh)_{ij} = \sum_{p \in b(ij)} g_{ip} h_{pj}$. The latter is zero for all $g, h \in A(\Gcal,c)$ if and only if $b(ij) = \emptyset$. Thus, condition~$(3)$ is equivalent to the following: if $j \not \to i$ in $\Gcal$, then there does not exist $p \in I$ with $j \to p$ and $p \to i$ in $\Gcal$, i.e., $\Gcal$ must be transitive by contraposition.
	We have shown that (1), (2) and (3) are satisfied if and only if conditions~(a) and (b) hold.
\end{proof}

The following example illustrates that the \emph{order} of the colours $c(i \leftarrow k)$ and $c(k \leftarrow j)$ for $k \in b(ij)$ in the butterfly graph $\Gcal_{b(ij)}$ indeed matters.

\begin{example} \label{exa:OrderButterfly}
	 Consider the coloured TDAG $\Gc$ given by
	 	\begin{center}
	 		\begin{tikzcd}[decoration={snake, amplitude=1pt}]
	 			\squared{1} & \squared{2} \ar[l, orange, dashed] & \squared{4} & \squared{5} \ar[l, red] \\
	 			& \squared{3} \ar[u, red] \ar[lu, OliveGreen, decorate] & & \squared{6} \ar[u, orange, dashed] \ar[lu, OliveGreen, decorate]
	 		\end{tikzcd}
	 	\end{center}
	 The colouring is compatible as all vertices are black (squared). The butterfly graphs $\Gcal_{b(1,3)}$ and $\Gcal_{b(4,6)}$ for the green (squiggly) edges are
	 	\begin{center}
	 		\begin{tikzcd}
	 			\squared{1} & \squared{2} \ar[l, orange, dashed] & \squared{3} \ar[l, red] & \text{ and }  &
	 			\squared{4} & \squared{5} \ar[l, red] & \squared{6} \ar[l, orange, dashed]
	 		\end{tikzcd}
	 	\end{center}
	respectively. Due to the different order of red (solid) and orange (dashed) arrows the butterfly graphs $\Gcal_{b(1,3)}$ and $\Gcal_{b(4,6)}$ are not isomorphic. Thus, $\AGc$ is not a group by the Butterfly Criterion. This can also be checked by hand. Consider the block-diagonal matrices $a := \diag(M_1, M_2) , \, 
	b := \diag(M_2, M_1) \in \AGc$, where
	 	\begin{align*}
	 		M_1 := \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix} \qquad \text{and} \qquad
	 		M_2 := \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} .
	 	\end{align*}
	Thus, $a$ has entry one for red (solid) and entry zero for orange (dashed) and green (squiggly), while $b$ has entry one for orange (dashed) and entry zero for red (solid) and green (squiggly). We compute $(ab)_{1,3} = (M_1 M_2)_{1,3} = 0$ and $(ab)_{4,6} = (M_2 M_1)_{1,3} = 1$. Therefore, the matrix $ab$ violates the green (squiggly) colour condition. Hence, $ab \notin \AGc$ and so $\AGc$ is not a group.
	\hfill\exSymbol
\end{example}

\begin{example}[{\cite[Example~B.3]{RDAG}}]
	Interestingly, two graphs can have all the same butterfly graphs without being isomorphic. We present an example. 
	Consider the following coloured graph with 10 black (square) vertices, and edges that are red (solid), green (squiggly), orange (dashed) or brown (dotted).
	\begin{center}
		\begin{tikzcd}[row sep = small, decoration={snake,amplitude=1pt}]
			% 1st row, 1st component
			& \squared{$c_1$} \ar[lddd, Maroon, dotted, bend right = 30] & & \squared{$b_1$} \ar[ll, red] \ar[lldddddd, orange, dashed] \ar[lldddd, OliveGreen, decorate] & \\
			
			%buffer
			& & & & \\
			
			% 2nd row, 1st component
			& \squared{$c_2$} \ar[ld, Maroon, dotted] & & \squared{$b_2$} \ar[ll, red] \ar[lluu, orange, dashed] \ar[lldddd, OliveGreen, decorate] & \\
			
			% middle row, 1st component
			\squared{$d_1$} & & & & \squared{$a_1$} \ar[luuu, Maroon, dotted, bend right = 30] \ar[lu, Maroon, dotted] \ar[ld, Maroon, dotted] \ar[lddd, Maroon, dotted, bend left = 30]  \\
			
			
			% 3rd row, 1st component
			& \squared{$c_3$} \ar[lu, Maroon, dotted] & & \squared{$b_3$} \ar[ll, red] \ar[lluu, orange, dashed] \ar[lluuuu, OliveGreen, decorate] & \\
			
			
			%buffer
			& & & &  \\
			
			% 4th row, 1st component
			& \squared{$c_4$} \ar[luuu, Maroon, dotted, bend left = 30] & & \squared{$b_4$} \ar[ll, red] \ar[lluu, orange, dashed] \ar[lluuuu, OliveGreen, decorate] & 
		\end{tikzcd}
	\end{center}
	We add some further edges:  four purple edges $a_1 \to c_i$, four blue edges $b_i \to d_1$, and a yellow edge $a_1 \to d_1$.
	Now, additionally consider the graph obtained by exchanging the green (squiggly) and orange (dashed) edges.
	
	The butterfly graphs for the two graphs are the same, as follows. On the yellow edge, the butterfly graphs both have four paths consisting of a brown edge followed by a blue edge, and four that are a purple edge followed by a brown edge. 
	Similarly, we can check the butterfly graphs at the other edge colours.
	
	However, the two coloured graphs are not isomorphic. Indeed, the only way to get an isomorphism is to permute the b-layer and the c-layer. The red (solid) edges give the identity permutation, the orange (dashed) edges give the cycle $\sigma = (1 \; 4 \; 3 \; 2)$, and the green (squiggly) edges give $\sigma^2$. Hence an isomorphism would need to consist of permutations $\tau_1$ and $\tau_2$ of $\{1,2,3,4\}$ with $\tau_1 {\rm id} \tau_2 = {\rm id}$, $\tau_1 \sigma \tau_2 = \sigma^2$, $\tau_1 \sigma^2 \tau_2 = \sigma$. The first condition implies $\tau_2 = \tau_1^{-1}$, hence $\sigma$ and $\sigma^2$ need to be simultaneously conjugate to $\sigma^2$ and $\sigma$ respectively. This implies $(\sigma^2)^2 = \sigma$, a contradiction because $\sigma^4 = {\rm id}$.
	\hfill\exSymbol
\end{example}


\subsubsection{Popov's Criterion for RDAGs} \label{subsec:RDAGpopov} 

If $\AGc$ is a group we can prove the important Lemma~\ref{lem:RDAGSemistablePolystable} on polystability differently. Namely, we generalize the proof of Theorem~\ref{thm:StabilityLinearIndepTDAG}(b) for TDAG models, where we used Popov's Criterion, Theorem~\ref{thm:PopovCriterion}. We stress that during the work on \cite{RDAG} this generalization process led to the concept of augmented sample matrices $M_{Y,s}$, which are crucial for several main results on RDAG models. It illustrates once more how the invariant theory perspective can foster the statistical understanding.

\medskip

Let $\Gc$ be a coloured DAG with compatible colouring. Recall that it suffices to work over $\CC$ when using Popov's Criterion, compare Lemma~\ref{lem:PopovForReal}. Assume that $\AGc \subseteq \GL_m(\CC)$ is a group. Hence, $G := \AGc_{\SL}$ is a group as well and we denote the subgroup of diagonal matrices in $G$ by $T$. Then $T$ is isomorphic to the diagonalizable group $\big\{ (\lambda_{s,s})_s \in (\CC^\times)^{|c(I)|} \mid \prod_s \lambda_{s,s}^{\alpha_s} = 1 \big\}$.

We briefly recall the setting of Section~\ref{sec:Popov} for the special case of RDAGs. The group $G$ acts on $\CC^{m \times n}$ by left-multiplication and $x_{i,j} \in \CC[G]$, $i,j \in [m]$ are the coordinate functions on $G$. By compatibility and similar to Lemma~\ref{lem:PropertiesCompatibleColouring}, we can consider the coordinate functions for the colour entries $z_{s,s}$ and $z_{s,t}$, where $s \in c(I)$ and $t \in \prc(s)$. They capture the equalities among the $x_{i,j}$, i.e.,  $z_{s,s} =x_{i,i}$ whenever $c(i) = s$ and $z_{s,t} = x_{i,j}$ whenever $c(ij) = (s,t)$.
For $Y \in \CC^{m \times n}$, we recall from Equation~\eqref{eq:PopovRY} the $\CC$-algebra
	\[ R_Y=  \CC \Big[ \sum_{j=1}^m Y_{j,l} \, x_{i,j} \, \Big\vert \, i \in [m], l \in [n] \Big] \subseteq \CC[G]. \]
Using the equalities among the $x_{i,j}$ and that $x_{i,j} = 0$ if $j \notin \{i\} \cup \pa(i)$, we can rewrite the algebra generators of $R_Y$ as follows:
	\begin{equation}\label{eq:PopovGeneratorsAGc}
		\sum_{j=1}^m x_{i,j} Y_{j,l} = z_{c(ii)} Y_{i,l} + \sum_{j \in \pa(i)} z_{c(ij)} Y_{j,l}
		= z_{s,s} Y_{i,l} + \sum_{t=1}^{\beta_s} z_{s,t} \Big( \sum_{\substack{i \leftarrow j\\c(i j) = t}} Y_{j,l} \Big) \, ,
	\end{equation}
where $s := c(i)$.
The character group of $T$ is $\Xfrak(T) \cong \ZZ^{|c(I)|} / \big( \ZZ \cdot (\alpha_s)_{s \in c(I)} \big)$, so that the semigroup $\Xfrak_{G \cdot Y}$ can be written as
\begin{align*}
	\Xfrak_{G \cdot Y} = \bigg\lbrace (d_s)_{s \in c(I)} \in \Xfrak(T) \, \Big\vert \, \prod_{s \in c(I)} z_{s,s}^{d_s} \in R_Y \bigg\rbrace .
\end{align*}


\begin{remark}[{\cite[Remark~B.5]{RDAG}}]
	The group $G = \AGc_{\SL} \subseteq \GL_m(\CC)$ may not be connected as required in Popov's Criterion,  Theorem~\ref{thm:PopovCriterion}. However, the orbit $G \cdot Y$ is Zariski-closed if $G^{\circ} \cdot Y$ is Zariski-closed, where $G^\circ$ is the identity component of $G$.\footnote{Recall that Zariski and Euclidean identity component agree over $\CC$, compare Section~\ref{sec:LinearAlgebraicGroups}}
	Thus, after restricting to $G^\circ = T^\circ U$ we may assume that $G$ is connected. Restricting to $T^\circ$ amounts to restricting to the torsion-free part of $\Xfrak(T)$, compare Proposition~\ref{prop:Characters}(e).
	If $\alpha$ is the greatest common divisor of all $\alpha_s, s \in c(I)$, then $T^\circ \cong \big\lbrace (g_s)_{s \in c(I)} \mid \prod_s g_s^{\alpha_s / \alpha} = 1 \big\rbrace$ and $\, \Xfrak(T^{\circ}) = \ZZ^{|c(I)|} / \big( \ZZ \cdot (\alpha_s/\alpha)_{s \in c(I)} \big)$.
	\hfill\remSymbol
\end{remark}

We are ready to generalize the proof of Theorem~\ref{thm:StabilityLinearIndepTDAG}(b) to the RDAG situation. This reproves the part on polystability in Lemma~\ref{lem:RDAGSemistablePolystable}.\footnote{For $\KK = \RR$ the argument only ensures that the orbit is Euclidean closed.}

\begin{lemma}\label{lem:PopovRDAG}
	Consider the RDAG model on $\Gc$ where colouring $c$ is compatible. Assume $\AGc \subseteq \GL_m(\KK)$ is a group and set $G := \AGc_{\SL}$. 
	Let $Y \in \KK^{m \times n}$ be such that $M_{Y,s}^{(0)} \notin \Span \big\lbrace M_{Y,s}^{(1)},\ldots, M_{Y,s}^{(\beta_s)} \big\rbrace$ for all $s \in c(I)$. Then $Y$ is polystable under $G$.
\end{lemma}

\begin{proof}
	The assumption ensures that $Y \neq 0$, so we need to show that $G \cdot Y$ is Euclidean closed in $\KK^{m \times n}$. By Lemma~\ref{lem:PopovForReal}, it is enough to prove that $G \cdot Y$ is Zariski closed for $\KK = \CC$. We will use Popov's Criterion for this.
	
	Fix a vertex colour $s \in c(I)$. As usual, set $\alpha_s := | c^{-1}(s) |$ and $\beta_s := |\prc(s)|$. Let $i_1 < i_2 < \cdots < i_{\alpha_s}$ be the vertices of colour $s$ and let $(e_0,e_1,\ldots,e_{\beta_s})$ be the ordered standard basis of $\CC^{\beta_s + 1}$. Since $M_{Y,s}^{(0)} \notin \mathrm{span} \big\lbrace M_{Y,s}^{(1)},\ldots,M_{Y,s}^{(\beta_s)} \big\rbrace$ we have $\ker(M_{Y,s}\HT) \subseteq \mathrm{span}\{e_1, \ldots, e_{\beta_s}\} \subseteq \CC^{\beta_s + 1}$. Therefore, $e_0$ is in the orthogonal complement of $\ker(M_{Y,s}\HT)$, i.e., in the image of $M_{Y,s}$. Hence, there is some $w \in \CC^{\alpha_s n}$ with $M_{Y,s}w = e_0$.
	For convenience, we write the entries of $w$ as $w_{k,l}$, where $k \in [\alpha_s]$ and $l \in [n]$; the ordering is as follows:
		\[ w = (w_{1,1}, \ldots, w_{1,n}, w_{2,1}, \ldots, w_{2,n}, \ldots \ldots, w_{\alpha_s,1}, \ldots, w_{\alpha_s n})\T . \]
	Similarly, we index the entries of the row vectors $M_{Y,s}^{(t)} \in \CC^{1 \times \alpha_s n}$.
	Using the construction of $M_{Y,s}$ in Definition~\ref{defn:MYs}, we compute
		\begin{align*}
			z_{s,s} &= \begin{pmatrix} z_{s,s} & z_{s,1} & z_{s,2} & \cdots & z_{s, \beta_s} \end{pmatrix} \big( M_{Y,s} w \big) \\
			&=  \sum_{k=1}^{\alpha_s} \sum_{l =1}^{n} z_{s,s} \big( M^{(0)}_{Y,s} \big)_{k,l} w_{k,l}  + \sum_{t=1}^{\beta_s} \sum_{k=1}^{\alpha_s} \sum_{l =1}^{n} z_{s,t} \big( M^{(t)}_{Y,s} \big)_{k,l} w_{k,l} \\
			&=  \sum_{k=1}^{\alpha_s} \sum_{l =1}^{n} w_{k,l} \bigg( z_{s,s} Y_{i_k,l} + \sum_{t=1}^{\beta_s} z_{s,t} \Big( \sum_{\substack{i_k \leftarrow j\\c(i_{k} j) = t}} Y_{j,l} \Big) \bigg)
			= \sum_{k=1}^{\alpha_s} \sum_{l =1}^{n} w_{k,l} \bigg( \sum_{j=1}^m x_{i_k,j} Y_{j,l} \bigg) ,
		\end{align*}
	where we used Equation~\eqref{eq:PopovGeneratorsAGc} in the last equality.
	This shows that $z_{s,s}$ is a $\CC$-linear combination of the $\sum_{j=1}^m x_{i,j} Y_{j,l}$, where $i \in c^{-1}(s)$ and $l \in [n]$. In particular, $z_{s,s} \in R_Y$ for all $s \in c(I)$ and hence we have
		\[ \forall (d_s)_s \in \ZZ_{\geq 0}^{|c(I)|} \colon \qquad \prod_{s \in c(I)} z_{s,s}^{d_s} \in R_Y .\]
	Any character of $T$ is of the latter form, since $\prod_{s} z_{ss}^{\alpha_s}$ is the trivial character.\footnote{In other words, any element of $\Xfrak(T) \cong \ZZ^{|c(I)|} / \big( \ZZ \cdot (\alpha_s)_{s \in c(I)} \big)$ admits a representative with non-negative entries.}
	We conclude $\Xfrak_{G \cdot Y} = \Xfrak(T)$ and hence $\Xfrak_{G \cdot Y}$ is a group. Therefore, $G \cdot Y$ is Zariski closed by Popov's Criterion, Theorem~\ref{thm:PopovCriterion}.
\end{proof}




\subsubsection{Bijection between the Stabilizer and the Set of MLEs}

So far we have given two descriptions of the MLEs given $Y$ in an RDAG model. Corollary~\ref{cor:RDAG-MLEs} gives a linear space of possible $\Lambda$, while Proposition~\ref{prop:RDAGsecond-bijection} gives an additive bijection between the set of MLEs and the $\AGc_{\SL}$-stabilizing set.

Here we give an alternative (multiplicative) bijection. Namely, for a Gaussian group model $\Mg_G$ we have a natural action of the $G_{\SL}$-stabilizer of $Y$ on the set of MLEs given $Y$, compare Proposition~\ref{prop:MLEsStabilizer}. For Zariski closed self-adjoint groups we have seen in Proposition~\ref{prop:MLEsTransitiveStabilizerAction} that this action is transitive. In the RDAG case the action is even transitive \emph{and} free. The following statement contains Proposition~\ref{prop:StabilizerMLEsTDAG} as a special case, since any TDAG $\Gcal$ arises as a coloured DAG $\Gc$ with compatible colouring such that the group $\AG$ equals $\AGc$, see Remark~\ref{rem:DAGmodelViaCompatible}.

\begin{prop}[{\cite[Proposition~B.6]{RDAG}}] \label{prop:StabilizerMLEsGroupRDAG}
	Consider the RDAG model on $\Gc$ where colouring $c$ is compatible and assume $\AGc$ is a group. Set $\Aset := \AGc_{\SL}$ and let $Y \in \KK^{m \times n}$ be polystable under $\Aset$. Let $\lambda a\HT a$ be an MLE given $Y$, where $a \in \Aset$ and $\lambda \in \RR_{>0}$ are as in Theorem~\ref{thm:WeakCorrespondence}. We have a bijection
	\begin{align*}
		\varphi \colon \Aset_Y &\to \{ \text{MLEs given } Y \} \\
		g &\mapsto \lambda g\HT  a\HT  a g .
	\end{align*}
	In other words, $\Aset_Y$ acts freely and transitively on the set of MLEs given $Y$.
\end{prop}

\begin{proof}
	For $g \in \Aset_Y$ we have $a g Y = aY$, which is of minimal norm in $\Aset \cdot Y$ as $\lambda a\HT a$ is an MLE. Hence, $\varphi(g) = \lambda (ag)\HT(ag)$ is another MLE given $Y$, by Theorem~\ref{thm:WeakCorrespondence}, and we see that $\varphi$ is well-defined.
	
	For surjectivity, let $\lambda \tilde{a}\HT \tilde{a}$ be another MLE given $Y$. Then $aY$ and $\tilde{a}Y$ are of minimal norm in $\Aset \cdot Y$, hence there is some $t \in T$ with $t\HT t = \Id_m$ such that $taY = \tilde{a} Y$, by Lemma~\ref{lem:twoStepRDAG}(c). Thus, for $g := a^{-1} t^{-1} \tilde{a} $ we have $gY = Y$ and also $g \in \Aset$, since $\AGc$ (and hence $\Aset$) is a group. Hence, $g \in \Aset_Y$ and the property $t\HT t = \Id_m$ gives $\varphi(g) =  \lambda g\HT  a\HT  a g = \lambda \tilde{a}\HT \tilde{a}$.
	
	To prove injectivity, let $g, g' \in \Aset_Y$ be such that $\varphi(g) = \varphi(g')$. The latter implies $g\HT a\HT ag = {g'}\HT a\HT a g'$, which is equivalent to $h\HT h = \Id_m$ where $h := a g' g^{-1} a^{-1}$.  In the following we show that $h = \Id_m$ which implies $g = g'$ as desired.
	
	First, as $\Aset$ is a group we have $h \in \Aset$. In particular, $h$ is upper triangular. Together with $h\HT h = \Id_m$, $h$ is a diagonal matrix by Lemma~\ref{lem:UpperTriangularUnitaryIsDiagonal} below.
	Moreover, using $g,g' \in \Aset_Y$ we deduce $haY = aY$, i.e., $h \in \Aset_{aY}$. Note that $Y$ and $aY$ have the same orbit (closure), where we again use that $\Aset$ is a group. Thus, $aY$ is polystable as $Y$ is polystable. In particular, for all vertex colours~$s$ we must have $M_{aY,s}^{(0)} \neq 0$ by Theorem~\ref{thm:RDAGStabilityVsLinDependence}(b). Finally, combining the latter with Eq.~\eqref{eq:MYsLeftMultiplication} for $M_{h \cdot (aY)}^{(0)}$, $h(aY) = aY$ and $h$ being diagonal implies $h = \Id_m$.
\end{proof}

We are left to show the following lemma.

\begin{lemma}\label{lem:UpperTriangularUnitaryIsDiagonal}
	Let $h \in \GL_{m}(\KK)$ be upper triangular with $h\HT h = \Id_m$. Then $h$ is a diagonal matrix.
\end{lemma}

\begin{proof}
	We prove the statement by induction on $m \geq 1$. For $m=1$, there is nothing to show as any $1 \times 1$ matrix is diagonal. Now, assume the statement holds for a fixed $m \geq 1$ and let $h \in \GL_{m+1}(\KK)$ be upper triangular with $h\HT h = \Id_{m+1}$. Then we have for all $j \in [m+1]$ that
		\[ \big( h\HT h \big)_{1,j} = \sum_{k =1}^{m+1} \overline{h_{k,i}} \, h_{k,j} =  \overline{h_{1,1}} \, h_{1,j}
		= \begin{cases} 1 & \text{, if } j=1  \\ 0 & \text{, if } j \neq 1  \end{cases}\]
	where we used in the middle equality that $h$ is upper triangular. We deduce that $h_{1,1} \neq 0$ and consequently for $j \geq 2$ we must have $h_{1,j} = 0$. Hence, $h$ is a block-diagonal matrix of the form $\diag(1,g)$ with $g \in \GL_m(\KK)$. The properties of $h$ yield that $g$ must be upper triangular with $g\HT g = \Id_{m}$. By induction hypothesis, $g$ is diagonal and therefore $h$ as well.
\end{proof}




















